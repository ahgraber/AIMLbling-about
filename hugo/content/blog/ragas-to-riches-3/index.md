---
title: RAGAS to Riches (part 3)
date: 2024-11-11
authors:
  - name: ahgraber
    link: https://github.com/ahgraber
    image: https://github.com/ahgraber.png
tags:
  # meta
  - "blogumentation"
  - "experiment"
  # ai/ml
  - "evals"
  - "LLMs"
  - "RAG"
  # homelab
  - "homelab"
series:
  - "ragas"
layout: single
toc: true
math: false
draft: false
---

This is part three of a three-part series ([one]({{< ref "/blog/ragas-to-riches-1" >}}), [two]({{< ref "/blog/ragas-to-riches-2" >}}))
where I explore best practices for evaluating RAG architecture via Ragas' recent v0.2 release (specifically, `Ragas v0.2.3`).
Code from these experiments is available [here](https://github.com/ahgraber/AIMLbling-about/tree/main/experiments/ragas-experiment).

In this post, I'll use Ragas to investigate my hypothesis - _LLMs prefer answers generated by themselves_ (as opposed to being objective evaluators).
This is, admittedly, a toy experiment, since I'm running this on a limited set of source information and with cost limitations.
However, the experience has been extremely instructive on configuring valid experiments

## Experiment

As mentioned above, I want to examine whether _LLMs prefer answers generated by themselves_ when used for RAG evaluation, though this design could be easily modified to test chunking strategies or embedding models.

I evaluated 4 different models - OpenAI's `GPT-4o-mini` [^4o-mini], Anthropic's `Claude-3-Haiku` [^3-haiku], `Llama-3.1-Instruct-70B-Turbo` [^llama31], quantized and hosted by TogetherAI,
and a `Q4_K_M` quantized version of `Mistral-Nemo-Instruct-2407` [^mistral] that I self-host with [LMStudio](https://lmstudio.ai/).
I used an embedding model from the same provider as the model where possible -
OpenAI's `text-embedding-3-small`, Anthropic's recommended embedding provider VoyageAI's `voyage-3-lite` [^voyage], TogetherAI's `m2-bert-80M-8k-retrieval` [^m2], and Nomic's `nomic-embed-text-v1.5` [^nomic].

The source data comprises the articles from this blog (posted between April and September, 2024), and the documentation markdown files from [my homelab gitops repo](https://github.com/ahgraber/homelab-gitops-k3s).

The procedure to build a knowledge graph and synthetic test set is better described in [part two of the series]({{< ref "/blog/ragas-to-riches-2" >}}).
The knowledge graph was built with `Mistral-Nemo-Instruct-2407` and `nomic-embed-text-v1.5`.
Based on the knowledge graph, I generated ~100 (it ended up being 112) test questions each per model, for a total of 448 questions in the synthetic test set.

I created a different RAG index per model/embedding pair. Documents from the knowledge base were chunked based on markdown sections, and embedded into vector space.
Note that this chunking and embedding process is different from the one that build the knowledge graph used for testset generation; this ensures that the testset is independent of the RAG architecture we might be evaluating!

I generated baseline responses, retrievals, and RAG responses for each of the 448 questions with each model/retriever.
The baseline response is simply what the model responded to the test question _without_ providing it any retrieved context, meaning the answer came from internal knowledge - a "closed-book exam".  
Baseline response quality was evaluated by analyzing `semantic similarity` and `answer relevance`.  
Retrieved context is the result of querying each RAG index (OpenAI, Anthropic/Voyager, Together, local) for each question in the test set.
The retrievals were evaluated by analyzing `context recall` and `context precision`.  
RAG responses took the retrievals from each RAG index, and provided both the retrievals as additional context and the test question to the LLM, which generated the answer in "open-book exam" fashion.
RAG response quality was evaluated by analyzing `semantic similarity`, `answer relevance`, and `faithfulness`.

> [!NOTE]
>
> - **SemanticSimilarity** - How similar is the _response_ to the ground truth reference?
> - **Answer/Response Relevance** - Is response relevant to the original input?<br>
> - **Context Precision** - Was the retrieved context "useful at arriving at the" in the ground truth _reference_?
> - **Context Recall** - Can sentences in ground truth _reference_ be attributed to the _retrieved context_?
> - **Faithfulness** - Can the claims made in the response can be inferred from the _retrieved context_?

Each eval uses an LLM and/or embedding model to evaluate the retrieval/response.
Given my hypothesis - that models will prefer their own responses, every evaluation described above was actually run 4x - once for each LLM/embedding model pair.
For example, I generate baseline response, retrievals, and RAG response with `GPT-4o-mini` for the first question in the test set.
To evaluate the quality baseline response from `GPT-4o-mini` (as a generator/responder), I measure `answer relevance` to the test question with each of 4 models as evaluators
(`GPT-4o-mini`,`Claude-3-Haiku`, `Llama-3.1-Instruct-70B-Turbo`, and `Mistral-Nemo-Instruct-2407`).
To evaluate the context retrievals, I measure `context precision` and `context recall` again using all 4 models to evaluate each metric.
Finally, I follow the same procedure for RAG response, evaluating `answer relevance` and `faithfulness` both using all 4 models as evaluators.

The diagrams below visually describe the evaluation process _(click the tabs labels to swap images)_:

{{< tabs items="Baseline Response,Context Retrieval,RAG Response" >}}

{{< tab >}}
{{< figure
  src="images/baseline.png"
  alt="ragas baseline"
  caption="Baseline response and analysis" >}}
{{< /tab >}}
{{< tab >}}
{{< figure
  src="images/retrieval.png"
  alt="context retrieval"
  caption="Context retrieval and analysis" >}}
{{< /tab >}}
{{< tab >}}
{{< figure
  src="images/rag.png"
  alt="RAG generation and analysis"
  caption="RAG generation and analysis" >}}
{{< /tab >}}
{{< /tabs >}}

## Analysis

I've aggregated model performance on some standard benchmarks in the below table, with the hope that they may provide some context
to the results that we see, both in terms of model performance in the RAG pipeline and LLM-as-a-judge performance.

| Provider   | Model                                 |  MMLU |  GPQA | HumanEval | MATH |
| :--------- | :------------------------------------ | ----: | ----: | --------: | ---: |
| OpenAI     | GPT-4o-mini [^4o-mini]                | 82.0% | 43.0% |     87.2% |  75% |
| Anthropic  | Claude-3-Haiku [^3-haiku]             | 75.2% | 33.3% |     75.9% |  41% |
| TogetherAI | Llama-3.1-Instruct-70B [^llama31]     | 83.6% | 46.7% |     80.5% |  60% |
| LMStudio   | Mistral-Nemo-Instruct-2407 [^mistral] | 68.0% | 33.0% |     68.0% |  40% |

> [!NOTE]
>
> Table data from provider's model cards + Artificial Analysis; higher value if conflicts

### Baseline Responses Evaluation

{{< tabs items="Semantic Similarity,Answer Relevance" >}}
{{< tab >}}

#### Semantic Similarity

When using `nomic-embed-text-v1.5` to assess how relevant (similar) baseline responses are to ground-truth reference,
models perform as we might expect given their benchmark scores: `GPT-4o-mini`, `Claude-3-Haiku`, `Llama-3.1-70B-Instruct-Turbo` (quantized), `Mistral-Nemo-Instruct-2407` (12B, quantized).

| response_by | semantic_similarity |
| :---------- | ------------------: |
| openai      |               0.903 |
| anthropic   |               0.897 |
| together    |               0.880 |
| local       |               0.875 |

> [!NOTE]
>
> Semantic similarity only examines how well each model performed as a generator/responder;
> since it does not use an LLM to evaluate, it has no influence on our analysis of LLM's as evaluators.

{{< /tab >}}
{{< tab >}}

#### Answer Relevance

In the table below, the rows represent the model generating the response, and the columns label the LLM-as-a-Judge Answer Relevance evaluators.
Interestingly, `Claude-3-Haiku`'s answers are scored the worst by all evaluators, and `Mistral-Nemo-Instruct-2407` outperforms both `Claude-3-Haiku` and `Llama-3.1-70B-Instruct-Turbo` (which are much larger models).
There does not seem to be a self-preference.

| response_by | ar_openai | ar_anthropic | ar_together | ar_local |
| :---------- | --------: | -----------: | ----------: | -------: |
| openai      |     0.901 |        0.917 |       0.913 |    0.905 |
| anthropic   |     0.690 |        0.732 |       0.734 |    0.723 |
| together    |     0.851 |        0.879 |       0.880 |    0.871 |
| local       |     0.884 |        0.884 |       0.882 |    0.872 |

Model performance as generator/responder seems more related to model release recency than size,
with the more recent `GPT-4o-mini` and `Mistral-Nemo-Instruct-2407` outperforming older `Claude-3-Haiku` and `Llama-3.1-70B-Instruct-Turbo`

If we look at the descriptive statistics behind these scores, we see that `Claude-3-Haiku` and `Llama-3.1-70B-Instruct-Turbo` provide higher/more optimistic mean scores across the board, though `GPT-4o-mini` has a more optimistic median.

| evaluation_by | count |  mean |   std | min |   25% |   50% |   75% | max |
| :------------ | ----: | ----: | ----: | --: | ----: | ----: | ----: | --: |
| openai        |  1792 | 0.831 | 0.275 |   0 | 0.854 | 0.926 | 0.969 |   1 |
| anthropic     |  1790 | 0.853 | 0.231 |   0 | 0.850 | 0.918 | 0.969 |   1 |
| together      |  1655 | 0.852 | 0.231 |   0 | 0.850 | 0.913 | 0.967 |   1 |
| local         |  1792 | 0.843 | 0.233 |   0 | 0.837 | 0.910 | 0.962 |   1 |

All LLMs show negative skew (mean < median), indicating more scores clustered at higher values.
`GPT-4o-mini`, with the highest variance and greater deviation between mean and median, may demonstrate greater discrimination between "good" and "bad" responses.
{{< /tab >}}
{{< /tabs >}}

### Retrieval Evaluation

As before, the rows represent the model retrieving the response, and the columns label the LLM-as-a-Judge evaluators.
In terms of retrieval performance, VoyageAI's `voyage-3-lite` outperforms other models, though `text-embedding-3-small` and `nomic-embed-text-v1.5` follow closely.
TogetherAI's `m2-bert-80M-8k-retrieval` model vastly underperforms all other models.

When looking at evaluator performance, `Claude-3-Haiku` is again the most optimistic model. There does not seem to be a self-preference.

{{< tabs items="Context Precision,Context Recall" >}}
{{< tab >}}

#### Context Precision

Retriever vs. Evaluator

| retriever                | cp_openai | cp_anthropic | cp_together | cp_local |
| :----------------------- | --------: | -----------: | ----------: | -------: |
| text-embedding-3-small   |     0.698 |        0.826 |       0.629 |    0.540 |
| voyage-3-lite            |     0.709 |        0.845 |       0.684 |    0.559 |
| m2-bert-80M-8k-retrieval |     0.234 |        0.375 |       0.183 |    0.157 |
| nomic-embed-text-v1.5    |     0.674 |        0.793 |       0.604 |    0.523 |

Descriptive stats by Evaluator

| evaluation_by | count |  mean |   std | min |   25% |   50% | 75% | max |
| :------------ | ----: | ----: | ----: | --: | ----: | ----: | --: | --: |
| openai        |  1792 | 0.579 | 0.432 |   0 |     0 | 0.756 |   1 |   1 |
| anthropic     |  1759 | 0.711 | 0.392 |   0 | 0.478 | 0.950 |   1 |   1 |
| together      |   637 | 0.518 | 0.442 |   0 |     0 | 0.500 |   1 |   1 |
| local         |  1792 | 0.445 | 0.447 |   0 |     0 | 0.333 |   1 |   1 |

{{< /tab >}}
{{< tab >}}

#### Context Recall

Retriever vs. Evaluator

| retriever                | cr_openai | cr_anthropic | cr_together | cr_local |
| :----------------------- | --------: | -----------: | ----------: | -------: |
| text-embedding-3-small   |     0.809 |        0.922 |       0.640 |    0.729 |
| voyage-3-lite            |     0.816 |        0.926 |       0.704 |    0.791 |
| m2-bert-80M-8k-retrieval |     0.354 |        0.523 |       0.268 |    0.380 |
| nomic-embed-text-v1.5    |     0.752 |        0.867 |       0.610 |    0.703 |

Descriptive stats by Evaluator

| evaluation_by | count |  mean |   std | min |   25% |   50% | 75% | max |
| :------------ | ----: | ----: | ----: | --: | ----: | ----: | --: | --: |
| openai        |  1792 | 0.683 | 0.424 |   0 |  0.25 |     1 |   1 |   1 |
| anthropic     |  1701 | 0.808 | 0.372 |   0 |     1 |     1 |   1 |   1 |
| together      |  1328 | 0.532 | 0.473 |   0 |     0 | 0.667 |   1 |   1 |
| local         |  1747 | 0.650 | 0.410 |   0 | 0.333 |     1 |   1 |   1 |

{{< /tab >}}
{{< /tabs >}}

### RAG Evaluation

As before, the rows represent the model retrieving/generating the response, and the columns label the LLM-as-a-Judge evaluators.
We see both the improvement that RAG provides we see the damage that a poor retriever can do in both Semantic Similarity and Answer Relevance evaluations.
Across both metrics, all models improved over the baseline; however, TogetherAI's poor retrieval model held it back and scores for the `Llama-3.1-70B-Instruct-Turbo`/`m2-bert-80M-8k-retrieval` combination lag behind other providers.
There does not seem to be a self-preference.

{{< tabs items="Semantic Similarity,Answer Relevance, Faithfulness" >}}
{{< tab >}}

#### Semantic Similarity

When using `nomic-embed-text-v1.5` to assess how relevant (similar) RAG responses are to ground-truth reference, we see both the improvement that RAG provides over baseline, as well as the importance of having a strong retrieval model.
In all cases, models improved over their baseline similarity scores; however, TogetherAI's poor retrieval model held it back.

| response_by | semantic_similarity (RAG) | semantic_similarity (base) |
| :---------- | ------------------------: | -------------------------: |
| openai      |                     0.928 |                      0.903 |
| anthropic   |                     0.917 |                      0.897 |
| together    |                     0.884 |                      0.880 |
| local       |                     0.906 |                      0.875 |

> [!NOTE]
>
> Note that semantic similarity only examines how well each model performed as a generator/responder;
> since it does not use an LLM to evaluate, it has no influence on our analysis of LLM's as evaluators.

{{< /tab >}}
{{< tab >}}

#### Answer Relevance

`Claude-3-Haiku` is again the most optimistic evaluator, giving the highest scores.

RAG Responder vs. Evaluator

| response_by | ar_openai | ar_anthropic | ar_together | ar_local |
| :---------- | --------: | -----------: | ----------: | -------: |
| openai      |     0.917 |        0.925 |       0.917 |    0.906 |
| anthropic   |     0.864 |        0.909 |       0.897 |    0.868 |
| together    |     0.748 |        0.784 |       0.783 |    0.778 |
| local       |     0.853 |        0.869 |       0.871 |    0.858 |

Descriptive stats by Evaluator

| evaluation_by | count |  mean |   std | min |   25% |   50% |   75% | max |
| :------------ | ----: | ----: | ----: | --: | ----: | ----: | ----: | --: |
| openai        |  1792 | 0.846 | 0.249 |   0 | 0.852 | 0.922 | 0.975 |   1 |
| anthropic     |  1792 | 0.872 | 0.186 |   0 | 0.848 | 0.919 | 0.974 |   1 |
| together      |  1527 | 0.871 | 0.186 |   0 | 0.850 | 0.913 | 0.969 |   1 |
| local         |  1779 | 0.852 | 0.208 |   0 | 0.826 | 0.906 | 0.965 |   1 |

{{< /tab >}}
{{< tab >}}

#### Faithfulness

> [!CAUTION]
>
> Parse failures were so frequent with `Claude-3-Haiku` and `Llama-3.1-Instruct-70B-Turbo` that I stopped using them to evaluate faithfulness
> because it was becoming incredibly expensive to retry with the "parse-fixer" agent.

RAG Responder vs. Evaluator

| response_by | ff_openai | ff_local |
| :---------- | --------: | -------: |
| openai      |     0.754 |    0.702 |
| anthropic   |     0.767 |    0.679 |
| together    |     0.436 |    0.421 |
| local       |     0.627 |    0.598 |

Descriptive stats by Evaluator

| evaluation_by | count |  mean |   std | min |   25% |   50% | 75% | max |
| :------------ | ----: | ----: | ----: | --: | ----: | ----: | --: | --: |
| openai        |  1685 | 0.647 | 0.359 |   0 | 0.359 |  0.75 |   1 |   1 |
| local         |  1668 | 0.603 | 0.360 |   0 | 0.315 | 0.659 |   1 |   1 |

{{< /tab >}}
{{< /tabs >}}

## Results

LLMs appear to be more objective judges than I anticipated. **In no experiment was there an obvious evaluator bias toward answers generated by the same model.**
I ran several regression models to examine the interaction effects that the retriever, response model and evaluator model selections had on metrics.
The regression reports are too much to post in this already-long article, but in sum, the regression models defined like `answer_relevance ~ C(response_by, levels=providers) * C(eval_by, levels=providers)` have poor fit
and do not find statistically significant interactions between response and evaluator models.

## Lessons learned

### Alignment as Evaluators

I also ran correlations across models' evaluation scores to determine model agreement/alignment in their role as evaluators.
Models show strong alignment, with simpler evaluations (i.e., baseline vs retrieval/RAG) having closer agreement.
More complex evaluation instructions (context recall, faithfulness) reduce alignment as the particular models handle the complexity differently.

> [!TIP]
>
> It would be best practice to have some human-labeled test sets and responses to run a similar analysis and ensure that the LLM-as-a-judge evaluators align with the human experts' judgements.

{{< tabs items="Baseline Response, Context Retrieval,RAG Response" >}}

{{< tab >}}

{{< figure
  src="images/baseline_answer_relevance_corr.png"
  alt="models are aligned on answer relevance"
  caption="Models are closely aligned on baseline Answer Relevance evaluations" >}}

{{< /tab >}}
{{< tab >}}

<table>
<tr>
  <td style="width:50%">
{{< figure
  src="images/retrieval_context_precision_corr.png"
  alt="models agree on context precision"
  caption="Models show fairly strong agreement on Context Precision" >}}
  </td>
  <td style="width:50%">
{{< figure
  src="images/retrieval_context_recall_corr.png"
  alt="alignment is lower on context recall"
  caption="Context Recall is a 'harder' evaluation based on increased missingness, which reduces model alignment" >}}
  </td>
</tr>
</table>

{{< /tab >}}
{{< tab >}}

<table>
<tr>
  <td style="width:50%">
{{< figure
  src="images/rag_answer_relevance_corr.png"
  alt="models are aligned on answer relevance"
  caption="Models are aligned on RAG Answer Relevance evaluations, though less so than in the baseline case" >}}
  </td>
  <td style="width:50%">
{{< figure
  src="images/rag_faithfulness_corr.png"
  alt="alignment is lower on faithfulness"
  caption="Faithfulness is a 'harder' evaluation based on increased missingness, which reduces model alignment" >}}
  </td>
</tr>
</table>

{{< /tab >}}
{{< /tabs >}}

### SOTA is not necessary(?)

I had great success with `Mistral-Nemo-Instruct-2407`, running either on my Macbook Pro (M2 Pro) or PC with an RTX 3090.
Evaluations indicate it performs as well as or better than Anthropic's `Claude-3-Haiku` or `Llama-3.1-70B-Instruct`.
A 3090 graphics card with 24GB VRAM is not necessary; with quantized models, you can run models on cards with as little as 8 GB VRAM, though more helps with longer contexts.
Cursory testing indicated that `gemma-2-9b-it-function-calling` would also be a strong evaluator, with good instruction-following and response-formatting performance.

> [!TIP]
>
> Small (8-14B parameter) language models are sophisticated enough to function in the LLM-as-a-judge role.
> It is worth considering whether you need full-fat state-of-the-art models to run evaluations or if smaller, cheaper models would suffice.

### Costs

Creating the knowledge graph and synthetic test set are one-time costs that are easily updated with minimal incremental cost.
Running the benchmark is expensive, and many runs may be required to represent the full experimental search space.
For a more in-depth look at cost expectations, see [part two of the series]({{< ref "/blog/ragas-to-riches-2" >}}).

All told, running this experiment (including debugging, user errors like failure-to-save, etc.) had the following API use and costs:

| Provider  | Model                        | Input Tokens | Output Tokens | Cost ($) | $/1M in/out tokens |
| :-------- | :--------------------------- | -----------: | ------------: | -------: | -----------------: |
| OpenAI    | GPT-4o-mini                  |   69,105,772 |    10,828,362 |  $ 16.46 |      $0.15 / $0.60 |
| Anthropic | Claude-3-Haiku               |   87,160,706 |     6,431,004 |  $ 29.81 |      $0.25 / $1.25 |
| Together  | Llama-3.1-70B-Instruct-Turbo | 81,706,133\* |  40,487,049\* |  $107.53 |      $0.88 / $0.88 |

> [!NOTE]
>
> I am not including embedding calls or costs here because they tend to be minimal

> [!WARNING]
>
> \* Estimated token utilization; TogetherAI has poor user-facing reporting and I'm not sure why or how I have 4-5x _output_ use with approximately the same input.
> As such, I cannot (currently) recommend TogetherAI; however, the customer service rep addressing my complaint to this effect has indicated they are aware of this problem and working toward improving reporting.

### Limits

All providers have a variety of limits to prevent abusing the API;
typically these limit the number of requests that can be sent in a given time period (per minute, per day), and/or limit the token utilization (per minute, per day).
Providers "tier" users based on how much the user spends.
I found Requests Per Minute (RPM) and Tokens Per Minute (TPM) limits to be generally manageable; Ragas provides retries with backoff to handle these limits.
However, [Anthropic](https://docs.anthropic.com/en/api/rate-limits#setting-lower-limits-for-workspaces) implements a Tokens Per Day (TPD) limit,
which can severely hamper the ability to use LLM-as-a-judge frameworks like Ragas, especially with state of the art models like `Claude-3.5-Sonnet`.
For context, compare Anthropic's limits to [OpenAI's](https://platform.openai.com/docs/guides/rate-limits?context=tier-two).

> [!TIP]
>
> Review your expected token throughput and make sure the various stages of the Ragas pipeline will complete within the limit.
> If, for instance, you get 80% through evaluating your test set and run out of tokens, there is not a good way to preserve work up to that point -- you lose that work, and have to split the test set to run in smaller batches in the future.

### Parse Failures and Eval Missingness

In my experience, missingness in evaluations is caused by timeouts or parse failures.
Parse failures are worse, because it means the evaluator model is not following the prompt instructions,
and the parse failure "agent" that Ragas uses to attempt to encourage the model to respond
with the proper schema ends up acting as a API use multiplier, sending roughly the same prompt multiple times and increasing costs.
I found that `Llama-3.1-Instruct-70B-Turbo` had significantly more parse failures than other models.

> [!TIP]
>
> Test with your desired evaluator LLM at a small scale to determine whether it demonstrates a propensity to return invalid response structures.
> Ragas has provisions to provide custom prompts, so you can prompt engineer into more reliable performance.

{{< tabs items="Baseline Missingess,Retrieval Missingness,RAG Missingness" >}}
{{< tab >}}

#### Baseline Response Missingness

##### Answer Relevance

| response_by | ar_openai | ar_anthropic | ar_together | ar_local |
| :---------- | --------: | -----------: | ----------: | -------: |
| openai      |     0.00% |        0.00% |       6.70% |    0.00% |
| anthropic   |     0.00% |        0.00% |       6.25% |    0.00% |
| together    |     0.00% |        0.00% |      11.83% |    0.00% |
| local       |     0.00% |        0.00% |       5.80% |    0.00% |

{{< /tab >}}
{{< tab >}}

#### Retrieval Missingness

##### Context Precision

| retriever                | cp_openai | cp_anthropic | cp_together | cp_local |
| :----------------------- | --------: | -----------: | ----------: | -------: |
| text-embedding-3-small   |     0.00% |        2.01% |      95.76% |    0.00% |
| voyage-3-lite            |     0.00% |        0.89% |      54.46% |    0.00% |
| m2-bert-80M-8k-retrieval |     0.00% |        3.35% |      62.05% |    0.00% |
| nomic-embed-text-v1.5    |     0.00% |        1.12% |      45.54% |    0.00% |

##### Context Recall

| retriever                | cr_openai | cr_anthropic | cr_together | cr_local |
| :----------------------- | --------: | -----------: | ----------: | -------: |
| text-embedding-3-small   |     0.00% |        6.47% |      91.96% |    2.68% |
| voyage-3-lite            |     0.00% |        4.91% |       4.46% |    2.90% |
| m2-bert-80M-8k-retrieval |     0.00% |        2.90% |       5.13% |    1.79% |
| nomic-embed-text-v1.5    |     0.00% |        6.03% |       2.01% |    2.68% |

{{< /tab >}}
{{< tab >}}

#### RAG Response Missingness

##### Answer Relevance

| response_by | ar_openai | ar_anthropic | ar_together | ar_local |
| :---------- | --------: | -----------: | ----------: | -------: |
| openai      |     0.00% |        0.00% |      10.26% |    2.67% |
| anthropic   |     0.00% |        0.00% |      10.93% |    0.00% |
| together    |     0.00% |        0.00% |      24.55% |    0.00% |
| local       |     0.00% |        0.00% |      13.39% |    0.22% |

##### Faithfulness

> [!CAUTION]
>
> Parse failures were so frequent with `Claude-3-Haiku` and `Llama-3.1-Instruct-70B-Turbo` that I stopped using them to evaluate faithfulness
> because it was becoming incredibly expensive to retry with the "parse-fixer" agent.

| response_by | ff_openai | ff_local |
| :---------- | --------: | -------: |
| openai      |     8.48% |    8.26% |
| anthropic   |     2.90% |    2.01% |
| together    |     8.03% |   11.16% |
| local       |     4.46% |    6.25% |

{{< /tab >}}
{{< /tabs >}}

## References

[^4o-mini]: [GPT-4o mini](https://openai.com/index/GPT-4o-mini-advancing-cost-efficient-intelligence/)

[^3-haiku]: [Claude 3 Haiku](https://www.anthropic.com/news/Claude-3-Haiku)

[^llama31]: [Meta Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct)

[^mistral]: [MistralAI Mistral-Nemo-Instruct-2407](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407)

[^voyage]: [VoyageAI Embeddings](https://docs.voyageai.com/docs/embeddings)

[^m2]: [Serverless Embedding models](https://docs.together.ai/docs/serverless-models#embedding-models)

[^nomic]: [NomicAI nomic-embed-text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5)
