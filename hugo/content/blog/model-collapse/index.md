---
title: Model Collapse
date: 2024-08-25
authors:
  - name: ahgraber
    link: https://github.com/ahgraber
    image: https://github.com/ahgraber.png
tags:
  # meta
  - meta
  - opinion
  # ai/ml
  - arxiv
  - generative AI
  - prompts
  - LLMs
series: []
layout: single
toc: true
math: false
draft: false
---

Over the past month, I've seen several articles reporting on the dangers of "model collapse" popping in news aggregators (HackerNews, Reddit, etc.). [^nature] [^bloomberg] [^conversation]
As someone working in the space, I find the timing of these reports interesting because research papers naming the phenomenon ("model collapse"[^recursion] or "model autophogy disorder" [^mad]) came out over a year ago.

## Model Collapse

"Model collapse" is "a degenerative process whereby, over time, models forget the true underlying data distribution, even in the absence of a shift in the distribution over time" when learning from data produced by other models.[^recursion]
Similarly, other researchers coin the phrase "Model Autophagy Disorder (MAD)", which they define as
"the process in which models exhibit a progressive degradation in quality (precision) or diversity (recall) over autophagous (self-consuming) generations."[^mad]
Regardless of name, the phenomenon is based on the idea that recursively training a new model on the outputs of the prior generation ultimately reduces performance, and that performance degradation worsens generationally.

Intuitively, as models sample based on probability, the most-probable tokens will show up more frequently than the least-probable tokens.
When a limited set of generations is produced, the most probable tokens become overrepresented and the least-probable disappear.
As this process is repeated, the distribution reduces to the mean.

<table>
<tr>
  <td style="width:50%">{{< figure
    src="images/model-collapse1.png"
    alt="model collapse intuition"
    caption="Model Collapse intuition via [[2305.17493] The Curse of Recursion: Training on Generated Data Makes Models Forget](https://arxiv.org/abs/2305.17493)" >}}
  </td>
    <td style="width:50%">{{< figure
    src="images/model-collapse2.png"
    alt="model collapse to the mean"
    caption="Model Collapse reduction to the mean via [[2305.17493] The Curse of Recursion: Training on Generated Data Makes Models Forget](https://arxiv.org/abs/2305.17493)" >}}
  </td>
</tr>
</table>

## Why Now?

Why is "model collapse" entering the zeitgeist now? I think there are several reasons:
The public is tiring of AI, and the loss of the rose colored glasses has people looking for proof that generative AI is bad or to justify their belief that it is due to fail.
Additionally, there are concerns that the outputs of generative AI are difficult (if not impossible) to distinguish from human-made content;
the concern being that AI generated content is pouring onto the internet and poisoning future model generations.
Mainstream publications are picking up on older papers;
Nature published an article _AI models collapse when trained on recursively generated data_ in July 2024[^nature] that is an abridged version of the original "model collapse" paper published over a year prior.[^recursion]
Model providers are relying on synthetic data as part of their data pipelines for training state-of-the-art foundation models as demonstrated in the latest Llama 3[^llama3] technical paper.
Taken together, we have well-regarded SOTA models relying on the use of synthetic data, mainstream / well-regarded publications reporting on the risks associated with using synthetic data,
and a public who is looking to preemptively scapegoat AI in preparation for an AI crash.

## Model collapse is unlikely to occur

Model collapse is a real phenomenon that presents real risk to predictive models, but the training configuration that results in model collapse is unlikely to occur.

Models are trained on internet-based corpuses of data. As the proportion of AI-generated information on the web is expected to increase, it might lead to a situation in which AI-generated content dominates.
This situation seems to align with the model collapse scenario in which a model is recursively trained on AI-generated data; however,
research on model collapse typically posits a naive approach during sampling that amplifies the central tendency[^recursion] [^mad],
which is neither aligned with how data filtering and preparation work nor with how synthetic data is used in modern training pipelines.

### Model collapse requires recurrence without altering the data mix

"The process of _model collapse_ is universal among generative models that recursively train on data generated by previous generations."[^recursion]
The assumption is that the training pipelines are naively automated and train next-generation models solely on the outputs of the prior generation of the same model.
Definitionally, this scenario is unlikely to occur.

It is unlikely for a single model to dominate the training data, even if AI-generated data ends up overrepresented in the internet corpus.
Already, this is a break from the assumptions behind model collapse -- there are a multitude of different genAI models, and each will have different output distributions.

Additionally, model collapse posits that the source corpus is held static (i.e., a snapshot of the internet from 2023) and that no new information is added to the system when training subsequent generations;
thus the original data becomes a smaller and smaller proportion of the training data.
This is also improbable; models are constantly updated with new data.

{{< figure
src="images/model-collapse3.png"
alt="model collapse static"
caption="Model Collapse assumes static data0 via [[2305.17493] The Curse of Recursion: Training on Generated Data Makes Models Forget](https://arxiv.org/abs/2305.17493)" >}}

Other research makes the importance of data mixing more explicit --
"If the generative model initially trained on real data is good enough, and the iterative retraining is made on a mixture of synthetic and real data, then the retraining procedure is stable."[^stability]
"Replacing the original real data by each generation's synthetic data does indeed tend towards model collapse ... [however], accumulating the successive generations of synthetic data alongside the original real data avoids model collapse;
these results hold across a range of model sizes, architectures, and hyperparameters."[^inevitable]

{{< figure
src="images/inevitable.png"
alt="inevitable"
caption="Accumulating data avoids model collapse via [[2404.01413] Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data](https://arxiv.org/abs/2404.01413)" >}}

### Data pipelines and training procedures for modern LLMs reduce risk of model collapse

Meta shared how critical data curation is in the process of training large language models.
They employ multiple techniques that leverage rules, heuristics, and multiple different generative and non-generative ML models to ensure the training corpus contains high-quality examples.[^llama3]
Generative models are used for _filtering_ or _classifying_ the pre-training dataset, not for creating data for training.
In this case, the risk of model collapse comes from the "poisoning of the well" as AI-generated information is added to the internet.
This risk is mitigated as mentioned above, and the data quality procedures further reduce the risk of having only centrally-distributed training data.

Synthetic data _is_ employed during the "post-training" phase where the model is instruction-tuned and aligned with human preferences.
However, this data is created with human-in-the-loop oversight and direction, which mitigate the effects of naive popularity sampling.
Additionally, novel human instructions and preferences are included in these data, so the data mix is "safe" per the aforementioned research.
Finally, "preserving the ability of LLMs to model low-probability events is essential to the fairness of their predictions: such events are often relevant to marginalized groups. Low-probability events are also vital to understand complex systems."[^recursion]
Managing model outputs vis-a-vis these low-probability events is the point of instruction-tuning and alignment, and in performing these post-training steps we substantially mitigate the scenarios leading to model collapse.
"Training from feedback-augmented synthesized data, either by pruning incorrect predictions or by selecting the best of several guesses, can prevent model collapse, validating popular approaches like RLHF."[^beyond]

## References

[^nature]: [AI models collapse when trained on recursively generated data | Nature](https://www.nature.com/articles/s41586-024-07566-y)

[^bloomberg]: [AI 'Model Collapse': Why Researchers Are Raising Alarms - Bloomberg](https://www.bloomberg.com/news/articles/2024-08-05/ai-model-collapse-why-researchers-are-raising-alarms)

[^conversation]: [What is 'model collapse'? An expert explains the rumours about an impending AI doom](https://theconversation.com/what-is-model-collapse-an-expert-explains-the-rumours-about-an-impending-ai-doom-236415) 18 Aug

[^recursion]: [[2305.17493] The Curse of Recursion: Training on Generated Data Makes Models Forget](https://arxiv.org/abs/2305.17493)

[^mad]: [[2307.01850] Self-Consuming Generative Models Go MAD](https://arxiv.org/abs/2307.01850)

[^llama3]: [[2407.21783] The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)

[^stability]: [[2310.00429] On the Stability of Iterative Retraining of Generative Models on their own Data](https://arxiv.org/abs/2310.00429)

[^inevitable]: [[2404.01413] Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data](https://arxiv.org/abs/2404.01413)

[^beyond]: [[2406.07515] Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement](https://arxiv.org/abs/2406.07515)
