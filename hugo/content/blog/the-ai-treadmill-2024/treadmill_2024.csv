title,url
Retro on Viberary | ‚òÖ‚ù§‚ú∞ Vicki Boykis ‚òÖ‚ù§‚ú∞,https://vickiboykis.com/2024/01/05/retro-on-viberary/
Microsoft Phi-2 model changes licence to MIT | Hacker News,https://news.ycombinator.com/item?id=38889539
[2309.05463] Textbooks Are All You Need II: phi-1.5 technical report,https://arxiv.org/abs/2309.05463
Emerging Architectures for LLM Applications | Andreessen Horowitz,https://a16z.com/emerging-architectures-for-llm-applications/
[1511.04707] Deep Linear Discriminant Analysis,https://arxiv.org/abs/1511.04707
Feature-Extraction-With-Deep-Neural-Networks-by-a-Generalized-Discriminant-Analysis.pdf,https://www.researchgate.net/profile/Thomas-Zielke/publication/262146231_Feature_Extraction_With_Deep_Neural_Networks_by_a_Generalized_Discriminant_Analysis/links/571e264d08aeaced7889dadb/Feature-Extraction-With-Deep-Neural-Networks-by-a-Generalized-Discriminant-Analysis.pdf?origin=publication_detail
LLM Visualization,https://bbycroft.net/llm
10 Noteworthy AI Research Papers of 2023,https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023
[2304.01373] Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,https://arxiv.org/abs/2304.01373
[2307.09288] Llama 2: Open Foundation and Fine-Tuned Chat Models,https://arxiv.org/abs/2307.09288
[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs,https://arxiv.org/abs/2305.14314
[2303.17564] BloombergGPT: A Large Language Model for Finance,https://arxiv.org/abs/2303.17564
[2305.18290] Direct Preference Optimization: Your Language Model is Secretly a Reward Model,https://arxiv.org/abs/2305.18290
[2310.06825] Mistral 7B,https://arxiv.org/abs/2310.06825
[2311.11045] Orca 2: Teaching Small Language Models How to Reason,https://arxiv.org/abs/2311.11045
[2310.16764] ConvNets Match Vision Transformers at Scale,https://arxiv.org/abs/2310.16764
[2304.02643] Segment Anything,https://arxiv.org/abs/2304.02643
[2311.10709] Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning,https://arxiv.org/abs/2311.10709
AI Canon | Andreessen Horowitz,https://a16z.com/ai-canon/
Navigating the High Cost of AI Compute | Andreessen Horowitz,https://a16z.com/navigating-the-high-cost-of-ai-compute/
How Are Consumers Using Generative AI? | Andreessen Horowitz,https://a16z.com/how-are-consumers-using-generative-ai/
The Next Token of Progress: 4 Unlocks on the Generative AI Horizon | Andreessen Horowitz,https://a16z.com/the-next-token-of-progress-4-unlocks-on-the-generative-ai-horizon/
"For B2B Generative AI Apps, Is Less More? | Andreessen Horowitz",https://a16z.com/for-b2b-generative-ai-apps-is-less-more/
Emerging Architectures for Modern Data Infrastructure | Andreessen Horowitz,https://a16z.com/emerging-architectures-for-modern-data-infrastructure/
Large Language Model Architecture Explained [Updated],https://www.labellerr.com/blog/exploring-architectures-and-configurations-for-large-language-models-llms/
[2401.02385] TinyLlama: An Open-Source Small Language Model,https://arxiv.org/abs/2401.02385
TinyLlama: An Open-Source Small Language Model,https://news.ycombinator.com/item?id=38885054
Generative AI with Large Language Models: Hands-On Training feat. Hugging Face and PyTorch Lightning - YouTube,https://www.youtube.com/watch?v=Ku9PM26Cc2c
Effortless AI: No-Code Automation Using n8n Cloud and OpenAI Vision API ‚Äì Automation with n8n,https://n8n-automation.com/2024/01/11/open-ai-vision-api/
Evaluation Metrics for LLM Applications In Production - Parea AI,https://docs.parea.ai/blog/eval-metrics-for-llm-apps-in-prod
Vector DB Comparison,https://superlinked.com/vector-db-comparison
Say Goodbye to Irrelevant Search Results: Cohere Rerank Is Here,https://cohere.com/blog/rerank
[2401.05856] Seven Failure Points When Engineering a Retrieval Augmented Generation System,https://arxiv.org/abs/2401.05856
predibase/llm_distillation_playbook: Practical best practices for distilling large language models.,https://github.com/predibase/llm_distillation_playbook
Fast Llama 2 on CPUs With Sparse Fine-Tuning and DeepSparse - Neural Magic,https://neuralmagic.com/blog/fast-llama-2-on-cpus-with-sparse-fine-tuning-and-deepsparse/
How many GPU resources do I need for full-fine tuning of the 7b model? - ü§óTransformers - Hugging Face Forums,https://discuss.huggingface.co/t/how-many-gpu-resources-do-i-need-for-full-fine-tuning-of-the-7b-model/66286
What is the Difference Between LlamaIndex and LangChain,https://www.gettingstarted.ai/langchain-vs-llamaindex-difference-and-which-one-to-choose/
"Hidden Changes in GPT-4, Uncovered | Dennis Miczek",https://dmicz.github.io/machine-learning/openai-changes/
OpenAI Quietly Updates ChatGPT for Election Content Moderation | Dennis Miczek,https://dmicz.github.io/machine-learning/chatgpt-election-update/
"SDS 747: Technical Intro to Transformers and LLMs, with Kirill Eremenko - Podcasts - SuperDataScience | Machine Learning | AI | Data Science Career | Analytics | Success",https://www.superdatascience.com/podcast/technical-intro-to-transformers-and-llms-with-kirill-eremenko
"Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs",https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention
rl-for-llms.md,https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81
Arxiv Dives - Training Language Models to Follow Instructions (InstructGPT),https://blog.oxen.ai/training-language-models-to-follow-instructions-instructgpt/
[2309.15217] RAGAS: Automated Evaluation of Retrieval Augmented Generation,https://arxiv.org/abs/2309.15217
Arxiv Dives - How Mistral 7B works,https://blog.oxen.ai/arxiv-dive-how-to-mistral-7b-works/
Arxiv Dives - How Mixture of Experts works with Mixtral 8x7B,https://blog.oxen.ai/arxiv-dives-mixture-of-experts-moe-with-mixtral-8x7b/
Arxiv Dives - Vision Transformers (ViT),https://blog.oxen.ai/arxiv-dives-vision-transformers-vit/
Arxiv Dives - Zero-shot Image Classification with CLIP,https://blog.oxen.ai/arxiv-dives-zero-shot-image-classification-with-clip/
Arxiv Dives - LLaVA üåã an open source Large Multimodal Model (LMM),https://blog.oxen.ai/arxiv-dive-how-to-llava-works/
What's new with ML in production | ‚òÖ‚ù§‚ú∞ Vicki Boykis ‚òÖ‚ù§‚ú∞,https://vickiboykis.com/2024/01/15/whats-new-with-ml-in-production/
Broadcast,https://www.withbroadcast.com/
Cost per Request | Garrit's Notes,https://garrit.xyz/posts/2024-01-18-cost-per-request
@philschmid/clipper - npm,https://www.npmjs.com/package/@philschmid/clipper
Home | ArtificialAnalysis.ai,https://artificialanalysis.ai/
decrease inference costs with open-source LLMs,https://twitter.com/wenquai/status/1748016021808595242
HN discussion,https://news.ycombinator.com/item?id=39048948
LeftoverLocals,https://leftoverlocals.com/
Ask HN: What is the state of art approximate k-NN search algorithm today?,https://news.ycombinator.com/item?id=39029979
[2308.08155] AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation,https://arxiv.org/abs/2308.08155
OpenAI Assistants: the future of Semantic Kernel | Semantic Kernel,https://devblogs.microsoft.com/semantic-kernel/assistants-the-future-of-semantic-kernel/
Prompt flow ‚Äî Prompt flow documentation,https://microsoft.github.io/promptflow/
promptflow/examples/flows/evaluation at main ¬∑ microsoft/promptflow,https://github.com/microsoft/promptflow/tree/main/examples/flows/evaluation
Efficient Guided Generation for Large Language Models,https://arxiv.org/abs/2307.09702
outlines-dev/outlines: Guided Text Generation,https://github.com/outlines-dev/outlines
guidance-ai/guidance: A guidance language for controlling large language models.,https://github.com/guidance-ai/guidance
Guiding Text Generation with Constrained Beam Search in ü§ó Transformers,https://huggingface.co/blog/constrained-beam-search
"[2401.08406] RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture",https://arxiv.org/abs/2401.08406
[2401.06532] INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning,https://arxiv.org/abs/2401.06532
Stable Code 3B: Coding on the Edge ‚Äî Stability AI,https://stability.ai/news/stable-code-2024-llm-code-completion-release
HN discussion,https://news.ycombinator.com/item?id=39019532
EvalPlus Leaderboard,https://evalplus.github.io/leaderboard.html
[2401.02731] Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks,https://arxiv.org/abs/2401.02731
[2401.02415] LLaMA Pro: Progressive LLaMA with Block Expansion,https://arxiv.org/abs/2401.02415
[2401.06104] Transformers are Multi-State RNNs,https://arxiv.org/abs/2401.06104
[2307.01850] Self-Consuming Generative Models Go MAD,https://arxiv.org/abs/2307.01850
[2312.14135] V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs,https://arxiv.org/abs/2312.14135
[2305.17216] Generating Images with Multimodal Language Models,https://arxiv.org/abs/2305.17216
ChatGPT language support - alpha (web) | OpenAI Help Center,https://help.openai.com/en/articles/8357869-chatgpt-language-support-alpha-web
[2304.05613] ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning,https://arxiv.org/abs/2304.05613
"[2302.04023] A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",https://arxiv.org/abs/2302.04023
mlabonne/llm-course: Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks.,https://github.com/mlabonne/llm-course#4-supervised-fine-tuning
stas00/ml-engineering: Machine Learning Engineering Online Book,https://github.com/stas00/ml-engineering
Top Papers,https://www.emergentmind.com/top
Evolution of AI and Amara's Law ¬∑ N9O,https://n9o.xyz/posts/202401-evolution-ai/
2309.17453 - Efficient Streaming Language Models with Attention Sinks,https://arxiv.org/abs/2309.17453
Attention Is Off By One ‚Äì Evan Miller,https://www.evanmiller.org/attention-is-off-by-one.html
2401.06855 - Fine-grained Hallucination Detection and Editing for Language Models,https://arxiv.org/abs/2401.06855
2401.10225 - ChatQA: Building GPT-4 Level Conversational QA Models,https://arxiv.org/abs/2401.10225
2401.04925 - The Impact of Reasoning Step Length on Large Language Models,https://arxiv.org/abs/2401.04925
New Theory Suggests Chatbots Can Understand Text | Quanta Magazine,https://www.quantamagazine.org/new-theory-suggests-chatbots-can-understand-text-20240122/
[2307.15936] A Theory for Emergence of Complex Skills in Language Models,https://arxiv.org/abs/2307.15936
[2401.05391] Efficient LLM inference solution on Intel GPU,https://arxiv.org/abs/2401.05391
Using Document Layout Structure for Efficient RAG,https://ambikasukla.substack.com/p/efficient-rag-with-document-layout?r=ft8uc
7 methods to secure LLM apps from prompt injections and jailbreaks [Guest],https://artificialintelligencemadesimple.substack.com/p/mitigate-prompt-attacks
Exploring Token Probabilities as a Means to Filter GPT-3's Answers | by LucianoSphere | Towards Data Science,https://archive.is/02BDO
Open-source LLMs as LangChain Agents,https://huggingface.co/blog/open-source-llms-as-agents
InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding with Instructions,https://arxiv.org/abs/2401.13313
Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding,https://arxiv.org/abs/2401.12954
[2401.08417] Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation,https://arxiv.org/abs/2401.08417
haoranxu/ALMA-13B-R ¬∑ Hugging Face,https://huggingface.co/haoranxu/ALMA-13B-R
A Guide to Large Language Model Abstractions - Two Sigma,https://www.twosigma.com/articles/a-guide-to-large-language-model-abstractions/
A Survey of Resource-efficient LLM and Multimodal Foundation Models,https://arxiv.org/abs/2401.08092
MM-LLMs: Recent Advances in MultiModal Large Language Models,https://arxiv.org/abs/2401.13601
"rasbt/LLMs-from-scratch: Implement a ChatGPT-like LLM in PyTorch from scratch, step by step",https://github.com/rasbt/LLMs-from-scratch
ChatGPT clone in 30 minutes on AWS Kubernetes | by Cluster.dev | Medium,https://medium.com/@cluster.dev/kubernetes-infrastructure-for-hugging-face-models-and-chat-with-cluster-dev-580050c29db5
github.com/nlmatics/llmsherpa,https://github.com/nlmatics/llmsherpa
nlmatics/nlm-ingestor: This repo provides the server side code for llmsherpa API to connect. It includes parsers for various file formats.,https://github.com/nlmatics/nlm-ingestor?tab=readme-ov-file
Releases | MLflow,https://mlflow.org/releases/
Zayd's Blog ‚Äì Why is machine learning 'hard'?,https://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html
HN Discussion Why is machine learning 'hard'? (2016),https://news.ycombinator.com/item?id=39109481
"Stop Overusing Scikit-Learn and Try OR-Tools Instead | by Matt Chapman | Jan, 2024 | Towards Data Science",https://archive.is/LoU15
intro ‚Äì great_tables,https://posit-dev.github.io/great-tables/articles/intro.html
Introduction ‚Äî Spinning Up documentation,https://spinningup.openai.com/en/latest/user/introduction.html#what-this-is
Gymnasium Documentation,https://gymnasium.farama.org/
microscope.openai.com/models,https://microscope.openai.com/models
ü¶Ö Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages (RWKV-v5),https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers
Mistral CEO confirms 'leak' of new open source AI model nearing GPT-4 performance | VentureBeat,https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/
Introducing the Enterprise Scenarios Leaderboard: a Leaderboard for Real World Use Cases,https://huggingface.co/blog/leaderboard-patronus
This Paper from MIT and Microsoft Introduces 'LASER': A Novel Machine Learning Approach that can Simultaneously Enhance an LLM's Task Performance and Reduce its Size with no Additional Training - MarkTechPost,https://www.marktechpost.com/2024/01/02/this-paper-from-mit-and-microsoft-introduces-laser-a-novel-machine-learning-approach-that-can-simultaneously-enhance-an-llms-task-performance-and-reduce-its-size-with-no-additional-training/
[2312.13558] The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction,https://arxiv.org/abs/2312.13558
Prompt Template Designer App,https://mitenmit.github.io/gpt/
OpenAI's GPT-4 safety systems broken by Scots Gaelic - The Register,https://www.theregister.com/2024/01/31/gpt4_gaelic_safety/
SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity,https://arxiv.org/abs/2401.17072
[2401.03038] SPADE: Synthesizing Data Quality Assertions for Large Language Model Pipelines,https://arxiv.org/abs/2401.03038
"AymenKallala/RAG_Maestro: Building a chatbot powered with a RAG pipeline to read,summarize and quote the most relevant papers related to the user query.",https://github.com/AymenKallala/RAG_Maestro
Scaling RAG for Production | VectorHub by Superlinked,https://superlinked.com/vectorhub/articles/scaling-rag-production
GitHub - gabrielchua/RAGxplorer: Open-source tool to visualise your RAG üîÆ,https://github.com/gabrielchua/RAGxplorer
How to Find the Best Multilingual Embedding Model for Your RAG | by Iulia Brezeanu | Towards Data Science,https://towardsdatascience.com/how-to-find-the-best-multilingual-embedding-model-for-your-rag-40325c308ebb
How do transformers work?+Design a Multi-class Sentiment Analysis for Customer Reviews,https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi
Create a Discord Chatbot Using LlamaIndex for Your Server | ClusteredBytes,https://clusteredbytes.pages.dev/posts/2024/create-a-discord-chatbot-using-llamaindex-for-your-server/
Ask HN: What have you built with LLMs?,https://news.ycombinator.com/item?id=39263664
The pain points of building a copilot - Austin Z. Henley,https://austinhenley.com/blog/copilotpainpoints.html
"2312.14231 - Building Your Own Product Copilot: Challenges, Opportunities, and Needs",https://arxiv.org/abs/2312.14231
2402.00838 - OLMo: Accelerating the Science of Language Models,https://arxiv.org/abs/2402.00838
2402.00159 - Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research,https://arxiv.org/abs/2402.00159
[2401.12178] In-Context Learning for Extreme Multi-Label Classification,https://arxiv.org/abs/2401.12178
AI Tools Like GitHub Copilot Are Rewiring Coders' Brains. Yours May Be Next | WIRED,https://www.wired.com/story/fast-forward-ai-rewiring-coders-brains-github-copilot/
2401.18059 - RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval,https://arxiv.org/abs/2401.18059
2402.05862 - Let Your Graph Do the Talking: Encoding Structured Data for LLMs,https://arxiv.org/abs/2402.05862
2402.01035 - Getting the most out of your tokenizer for pre-training and domain adaptation,https://arxiv.org/abs/2402.01035
2402.03620 - Self-Discover: Large Language Models Self-Compose Reasoning Structures,https://arxiv.org/abs/2402.03620
2402.05929 - An Interactive Agent Foundation Model,https://arxiv.org/abs/2402.05929
2402.05120 - More Agents Is All You Need,https://arxiv.org/abs/2402.05120
2402.00396 - Efficient Exploration for LLMs,https://arxiv.org/abs/2402.00396
2402.04249 - HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,https://arxiv.org/abs/2402.04249
2402.04177 - Scaling Laws for Downstream Task Performance of Large Language Models,https://arxiv.org/abs/2402.04177
2402.05935 - SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models,https://arxiv.org/abs/2402.05935
"Multimodal LM roundup: Unified IO 2, inputs and outputs, Gemini, LLaVA-RLHF, and RLHF questions",https://www.interconnects.ai/p/multimodal-rlhf
How I think about LLM prompt engineering,https://fchollet.substack.com/p/how-i-think-about-llm-prompt-engineering
Language Language Models (in 2023),https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/mobilepresent?pli=1&slide=id.g2885e521b53_0_0
LLM Evaluation Primer - by David Hershey,https://generallyintelligent.substack.com/p/llm-evaluation-primer
llmware (llmware),https://huggingface.co/llmware
"llmware-ai/llmware: Providing enterprise-grade LLM-based development framework, tools, and fine-tuned models.",https://github.com/llmware-ai/llmware
Aligning a LLM with Human Preferences - DataDreamer,https://datadreamer.dev/docs/latest/pages/get_started/quick_tour/aligning.html
justinchiu/openlogprobs: Extract full next-token probabilities via language model APIs,https://github.com/justinchiu/openlogprobs
lucidrains/x-transformers: A simple but complete full-attention transformer with a set of promising experimental features from various papers,https://github.com/lucidrains/x-transformers
OpenAccess-AI-Collective/axolotl: Go ahead and axolotl questions,https://github.com/OpenAccess-AI-Collective/axolotl
How to Optimize Retrieval-Augmented Generation,https://generatingconversation.substack.com/p/how-to-optimize-retrieval-augmented
LLMs are the variable interest credit card of tech debt,https://generatingconversation.substack.com/p/llms-are-the-variable-interest-credit
2312.05934 - Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs,https://arxiv.org/abs/2312.05934
"How we got fine-tuning Mistral-7B to not suck: Helix Project Report, Feb 2024",https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b
Fine-tuning to Memorize Knowledge - LlamaIndex ü¶ô 0.9.46,https://docs.llamaindex.ai/en/latest/examples/finetuning/knowledge/finetune_knowledge.html
2402.03902 - A phase transition between positional and semantic learning in a solvable model of dot-product attention,https://arxiv.org/abs/2402.03902
2401.15391 - MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries,https://arxiv.org/abs/2401.15391
Thinking about High-Quality Human Data | Lil'Log,https://lilianweng.github.io/posts/2024-02-05-human-data-quality/
Detecting Issues in LLM Outputs | Cleanlab Studio Docs,https://help.cleanlab.ai/tutorials/llm/#unrealistic-llm-responses
2023.trustnlp-1.28.pdf,https://aclanthology.org/2023.trustnlp-1.28.pdf
2302.09664 - Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation,https://arxiv.org/abs/2302.09664
2308.16175 - Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness,https://arxiv.org/abs/2308.16175
[2401.14423] Prompt Design and Engineering: Introduction and Advanced Methods,https://arxiv.org/abs/2401.14423
An (incomplete and opinionated) survey of LLM tooling - Eric J. Ma's Personal Site,https://ericmjl.github.io/blog/2024/2/1/an-incomplete-and-opinionated-survey-of-llm-tooling/
[2312.10997] Retrieval-Augmented Generation for Large Language Models: A Survey,https://arxiv.org/abs/2312.10997
Memory and new controls for ChatGPT,https://openai.com/blog/memory-and-new-controls-for-chatgpt
Build a Custom LLM with Chat With RTX | NVIDIA,https://www.nvidia.com/en-us/ai-on-rtx/chat-with-rtx-generative-ai/
Vector search in old and modern databases,https://manticoresearch.com/blog/vector-search-in-databases/
Aya | Cohere For AI,https://cohere.com/research/aya
GitHub - rentruewang/bocoel: Bayesian Optimization as a Coverage Tool for Evaluating LLMs. 10 times faster and accurate evaluation (benchmarking) with just a few lines of modular code.,https://github.com/rentruewang/bocoel
Ask HN: What are some actual use cases of AI Agents right now? | Hacker News,https://news.ycombinator.com/item?id=39373662
2402.06196 - Large Language Models: A Survey,https://arxiv.org/abs/2402.06196
2402.05964 - A Survey on Transformer Compression,https://arxiv.org/abs/2402.05964
2104.09864 - RoFormer: Enhanced Transformer with Rotary Position Embedding,https://arxiv.org/abs/2104.09864
LLaMA: Concepts Explained (Summary) | by Anshu Kumar | Medium,https://akgeni.medium.com/llama-concepts-explained-summary-a87f0bd61964
2311.08377 - Learning to Filter Context for Retrieval-Augmented Generation,https://arxiv.org/abs/2311.08377
2308.12284 - D4: Improving LLM Pretraining via Document De-Duplication and Diversification,https://arxiv.org/abs/2308.12284
2402.05880 - Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking,https://arxiv.org/abs/2402.05880
"When You Hear "Filter Bubble", "Echo Chamber", or "Rabbit Hole" ‚Äî Think "Feedback Loop" | by Luke Thorburn | Understanding Recommenders | Medium",https://medium.com/understanding-recommenders/when-you-hear-filter-bubble-echo-chamber-or-rabbit-hole-think-feedback-loop-7d1c8733d5c
The Annotated S4,https://srush.github.io/annotated-s4/
2402.07871 - Scaling Laws for Fine-Grained Mixture of Experts,https://arxiv.org/abs/2402.07871
2402.07043 - A Tale of Tails: Model Collapse as a Change of Scaling Laws,https://arxiv.org/abs/2402.07043
2402.08640 - Forecasting high-impact research topics via machine learning on evolving knowledge graphs,https://arxiv.org/abs/2402.08640
2402.08164 - On Limitations of the Transformer Architecture,https://arxiv.org/abs/2402.08164
Chatbot Arena (formerly LMSYS): Free AI Chat to Compare & Test Best AI Chatbots,https://lmarena.ai/
Mistral AI launches Mixtral-Next,https://news.ycombinator.com/item?id=39406168
Nomic Embedding - LlamaIndex,https://docs.llamaindex.ai/en/stable/examples/embeddings/nomic/
LlamaIndex Sessions: Practical Tips and Tricks for Productionizing RAG (feat. Sisil @ Jasper),https://www.youtube.com/watch?v=ZP1F9z-S7T0
The Berkeley Artificial Intelligence Research Blog,https://bair.berkeley.edu/blog/
System Design for Recommendations and Search,https://eugeneyan.com/writing/system-design-for-discovery/
2311.16863 - Power Hungry Processing: Watts Driving the Cost of AI Deployment?,https://arxiv.org/abs/2311.16863
2402.04315 - Training Language Models to Generate Text with Citations via Fine-grained Rewards,https://arxiv.org/abs/2402.04315
2402.09668 - How to Train Data-Efficient LLMs,https://arxiv.org/abs/2402.09668
2402.09353 - DoRA: Weight-Decomposed Low-Rank Adaptation,https://arxiv.org/abs/2402.09353
2402.10200 - Chain-of-Thought Reasoning Without Prompting,https://arxiv.org/abs/2402.10200
2402.10555 - SPAR: Personalized Content-Based Recommendation via Long Engagement Attention,https://arxiv.org/abs/2402.10555
"Introducing LlamaCloud and LlamaParse | by Jerry Liu | Feb, 2024 | LlamaIndex Blog",https://blog.llamaindex.ai/introducing-llamacloud-and-llamaparse-af8cedf9006b?gi=cb33ac1bd950
LlamaCloud and LlamaParse,https://news.ycombinator.com/item?id=39443972
ChunkViz,https://chunkviz.up.railway.app/
Web Scraping in Python - The Complete Guide | ProxiesAPI,https://archive.is/32hgm
Web Scraping in Python ‚Äì The Complete Guide,https://news.ycombinator.com/item?id=39442273
The Journey of RAG Development: From Notebook to Microservices | by Wenqi Glantz | Towards Data Science,https://towardsdatascience.com/the-journey-of-rag-development-from-notebook-to-microservices-cc065d0210ef
2402.14020 - Coercing LLMs to do and reveal (almost) anything,https://arxiv.org/abs/2402.14020
2402.13753 - LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens,https://arxiv.org/abs/2402.13753
"what dimensions are The weights (W^Q, W^K, W^V)",https://www.phind.com/search?cache=b4q3bgef8octbrukzkjfgwvd
A Deep Dive Into the Transformer Architecture | transformer_deep_dive ‚Äì Weights & Biases,https://wandb.ai/carlolepelaars/transformer_deep_dive/reports/A-Deep-Dive-Into-the-Transformer-Architecture--VmlldzozODQ4NDQ
Transformers Demystified (Part 1): Into TheTransformer | by Sekhar M | Towards Data Science,https://towardsdatascience.com/into-the-transformer-5ad892e0cee
deep learning - Dimensions of Transformer - dmodel and depth - Data Science Stack Exchange,https://datascience.stackexchange.com/questions/93768/dimensions-of-transformer-dmodel-and-depth
Au Large | Mistral AI | Frontier AI in your hands,https://mistral.ai/news/mistral-large/
SciPhi-AI/R2R: A framework for rapid development and deployment of production-ready RAG systems,https://github.com/SciPhi-AI/R2R
2402.14158 - TOOLVERIFIER: Generalization to New Tools via Self-Verification,https://arxiv.org/abs/2402.14158
2310.01405 - Representation Engineering: A Top-Down Approach to AI Transparency,https://arxiv.org/abs/2310.01405
Representation Engineering Mistral-7B an Acid Trip,https://vgel.me/posts/representation-engineering/
2402.14433 - A Language Model's Guide Through Latent Space,https://arxiv.org/abs/2402.14433
2402.14207 - Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models,https://arxiv.org/abs/2402.14207
2402.14904 - Watermarking Makes Language Models Radioactive,https://arxiv.org/abs/2402.14904
AllHands,https://www.linkedin.com/posts/pascal-hetzscholdt_microsoft-researchers-introduce-allhands-activity-7178306788847435776-VPrg
Follow-up. Agents,https://www.deeplearning.ai/the-batch/issue-242/
Embeddings,https://www.linkedin.com/posts/langchain_rag-from-scratch-multi-representation-activity-7179205407217766400-Wm0w
Building WOPR: A 7x4090 AI Server,https://www.mov-axbx.com/wopr/wopr_concept.html
Advance Retrieval Techniques - Miro,https://miro.com/app/board/uXjVN1Hs-PI=/
Embedding model costs,https://www.linkedin.com/posts/llamaindex_save-memory-and-money-in-the-rag-activity-7179169269031587840-nVgs
Utah passws AI law,https://www.jdsupra.com/legalnews/utah-passes-artificial-intelligence-1386840/
Laion foundational data investigation,https://knowingmachines.org/models-all-the-way
MSFT AI safety tools,https://www.theregister.com/2024/03/29/microsoft_azure_safety_tools/
"2403.16977 - A comparison of Human, GPT-3.5, and GPT-4 Performance in a University-Level Coding Course",https://arxiv.org/abs/2403.16977
Jamba,https://www.maginative.com/article/ai21-labs-unveils-jamba-the-first-production-grade-mamba-based-ai-model/
[2308.09124] Linearity of Relation Decoding in Transformer Language Models,https://arxiv.org/abs/2308.09124
2403.15452 - What Are Tools Anyway? A Survey from the Language Model Perspective,https://arxiv.org/abs/2403.15452
2403.16971 - LLM Agent Operating System,https://arxiv.org/abs/2403.16971
Binary and Scalar Embedding Quantization for Significantly Faster & Cheaper Retrieval,https://huggingface.co/blog/embedding-quantization
The First Truly Global Generative AI Landscape 2024,https://www.blog.aiport.tech/p/the-first-truly-global-generative
https://www.threads.net/@ethan_mollick/post/C46AfItO8RS,https://www.threads.net/@ethan_mollick/post/C46AfItO8RS
HN Discussion,https://news.ycombinator.com/item?id=39809177
Which AI should I use? Superpowers and the State of Play,https://www.oneusefulthing.org/p/which-ai-should-i-use-superpowers
LLM API Pricing,https://www.botgenuity.com/tools/llm-pricing
Y2Z/monolith: ‚¨õÔ∏è CLI tool for saving complete web pages as a single HTML file,https://github.com/Y2Z/monolith
https://www.linkedin.com/posts/a-roucher_%3F%3F%3F%3F%3F%3F-%3F%3F-%3F%3F%3F%3F%3F%3F%3F%3F%3F%3F%3F%3F-activity-7178079605495353347-Yoq9?utm_source=share&utm_medium=member_ios,https://www.linkedin.com/posts/a-roucher_%3F%3F%3F%3F%3F%3F-%3F%3F-%3F%3F%3F%3F%3F%3F%3F%3F%3F%3F%3F%3F-activity-7178079605495353347-Yoq9?utm_source=share&utm_medium=member_ios
Wired News,https://www.wired.com/story/eight-google-employees-invented-modern-ai-transformers-paper/
2402.02622 - DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging,https://arxiv.org/abs/2402.02622
How LLMs boost AI progress: Devin's demo | Andrew Ng posted on the topic,https://www.linkedin.com/feed/update/urn:li:activity:7176663393708224512
"Robots Talk Back, AI Security Risks, Political Deepfakes, and more",https://www.deeplearning.ai/the-batch/issue-241/
Parakeet: A Tiny LLM,https://news.ycombinator.com/item?id=39745700
"Nvidia announces GB200 Blackwell AI chip, launching later this year",https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html
"Inside Suno AI, the Start-up Creating a ChatGPT for Music",https://www.rollingstone.com/music/music-features/suno-ai-chatgpt-for-music-1234982307/
Ask HN: What are you building LLM/RAG chatbots with,https://news.ycombinator.com/item?id=39759461
World,https://arstechnica.com/information-technology/2024/03/worlds-first-global-ai-resolution-unanimously-adopted-by-united-nations/
2403.10949 - SelfIE: Self-Interpretation of Large Language Model Embeddings,https://arxiv.org/abs/2403.10949
2403.13793 - Evaluating Frontier Models for Dangerous Capabilities,https://arxiv.org/abs/2403.13793
HN discussion,https://app.fastmail.com/mail/Inbox/%E2%80%A2https:/news.ycombinator.com/item?id=39443972&u=6b394014
Unstructured.io,https://unstructured.io/
"Arize-ai/phoenix: AI Observability & Evaluation - Evaluate, troubleshoot, and fine tune your LLM, CV, and NLP models in a notebook.",https://github.com/Arize-ai/phoenix/tree/main
"How to perform, evaluate and track Advanced RAG using LlamaIndex, Ragas and AzureML",https://www.linkedin.com/pulse/performing-evaluating-tracking-advanced-rag-ft-azureml-prabhat-i1rkc/
"The Journey of RAG Development: From Notebook to Microservices | by Wenqi Glantz | Feb, 2024 | Towards Data Science",https://archive.is/jUaOr
2402.14083 - Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping,https://arxiv.org/abs/2402.14083
2402.14652 - Cleaner Pretraining Corpus Curation with Neural Web Scraping,https://arxiv.org/abs/2402.14652
here,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf
https://blog.google/technology/developers/gemma-open-models/,https://blog.google/technology/developers/gemma-open-models/
https://www.independent.co.uk/tech/chatgpt-status-reddit-down-gibberish-messages-latest-b2499816.html,https://www.independent.co.uk/tech/chatgpt-status-reddit-down-gibberish-messages-latest-b2499816.html
https://github.com/enjalot/latent-scope,https://github.com/enjalot/latent-scope
Compare LLMs in LLM Arena,https://lmsys.org/blog/2023-05-03-arena/
"GitHub - traceloop/openllmetry-js: Sister project to OpenLLMetry, but in Typescript. Open-source observability for your LLM application, based on OpenTelemetry",https://github.com/traceloop/openllmetry-js
HN Discussion,https://news.ycombinator.com/item?id=37843907
Nomic.AI,https://blog.nomic.ai/
https://static.nomic.ai/reports/2024_Nomic_Embed_Text_Technical_Report.pdf,https://static.nomic.ai/reports/2024_Nomic_Embed_Text_Technical_Report.pdf
LiteLLM,https://github.com/BerriAI/litellm
https://codingwithintelligence.com/,https://codingwithintelligence.com/
https://generatingconversation.substack.com/,https://generatingconversation.substack.com/
Machine Learning: The High Interest Credit Card of Technical Debt,https://research.google/pubs/machine-learning-the-high-interest-credit-card-of-technical-debt/
OpenAccess-AI-Collective/axolotl: Go ahead and axolotl questions,https://app.fastmail.com/mail/Inbox/%5baxolotl/README.md%20at%20main%20%C2%B7%20OpenAccess-AI-Collective/axolotl%20%C2%B7%20GitHub%5d(https:/github.com/OpenAccess-AI-Collective/axolotl/blob/main/README.md?u=6b394014#axolotl-supports)
NVIDIA/NeMo-Guardrails,https://github.com/NVIDIA/NeMo-Guardrails
https://pastebin.com/vnxJ7kQk,https://pastebin.com/vnxJ7kQk
LangChain,https://www.youtube.com/watch?v=wd7TZ4w1mSw&list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x&pp=iAQB
2402.04978 - An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration,https://arxiv.org/abs/2402.04978
MiniCPM: Unveiling the Potential of End-side Large Language Models,https://shengdinghu.notion.site/MiniCPM-Unveiling-the-Potential-of-End-side-Large-Language-Models-d4d3a8c426424654a4e80e42a711cb20
2402.01781 - When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards,https://arxiv.org/abs/2402.01781
Mastering RAG: How To Architect An Enterprise RAG System - Galileo,https://www.rungalileo.io/blog/mastering-rag-how-to-architect-an-enterprise-rag-system#multi-tenancy
Scaling RAG for Production - VectorHub,https://hub.superlinked.com/scaling-rag-for-production
RAG + Agents w/ Llamaindex,https://www.linkedin.com/posts/llamaindex_4-levels-of-agents-for-rag-theres-activity-7155596883472596992-WZGA?utm_source=share&utm_medium=member_ios
RAKE,https://www.analyticsvidhya.com/blog/2021/10/rapid-keyword-extraction-rake-algorithm-in-natural-language-processing/
Introducing the Enterprise Scenarios Leaderboard: a Leaderboard for Real World Use Cases,https://huggingface.co/blog/leaderboards-on-the-hub-patronus
Seminar,https://www.ctouniverse.com/frs/25388599/llmops-for-your-data--best-practices-to-ensure-safety--quality--and-cost/download
vimeo,https://vimeo.com/904202968
2401.16960 - Two Heads Are Better Than One: Integrating Knowledge from Knowledge Graphs and Large Language Models for Entity Alignment,https://arxiv.org/abs/2401.16960
Project Plan Generator - 100% FREE for entrepreneurs and small businesses - Done by AI,https://app.bodegalaabs.com/project-plan-generator
HuggingFace's Alignment Handbook,https://github.com/huggingface/alignment-handbook
OpenAI releases new models and pricing,https://openai.com/blog/new-embedding-models-and-api-updates
Andreessen Horowitz,https://github.com/a16z-infra/llm-app-stack
Hugging Face and Google partner for open AI collaboration,https://huggingface.co/blog/gcp-partnership
2305.14106 - Better Zero-Shot Reasoning with Self-Adaptive Prompting,https://arxiv.org/abs/2305.14106
2305.14926 - Universal Self-Adaptive Prompting,https://arxiv.org/abs/2305.14926
2310.02238 - Who's Harry Potter? Approximate Unlearning in LLMs,https://arxiv.org/abs/2310.02238
@Alan Akil,https://arxiv.org/abs/2306.11644
Phi2,https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/
mlflow 2.10 released,https://mlflow.org/releases/2024/01/26/2.10.0-release
huggingface summary,https://huggingface.co/blog/tomaarsen/attention-sinks
arxiv-dive blog,https://blog.oxen.ai/arxiv-dives-efficient-streaming-language-models-with-attention-sinks/
WaveNet architecture,https://paperswithcode.com/method/wavenet
How to make LLMs go fast,https://vgel.me/posts/faster-inference/#KV_caching
stanfordnlp/dspy: DSPy: The framework for programming,https://github.com/stanfordnlp/dspy
Huggingface compared alignment techniques,https://huggingface.co/blog/pref-tuning
"Better, Cheaper, Faster LLM Alignment with KTO - Contextual AI",https://contextual.ai/better-cheaper-faster-llm-alignment-with-kto/
[PDF] Human-Centered Loss Functions,https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf
[PDF] Vector Databases: A Technical Primer,https://tge-data-web.nyc3.digitaloceanspaces.com/docs/Vector%20Databases%20-%20A%20Technical%20Primer.pdf
Vector DB Comparison,https://vdbs.superlinked.com/
Say Goodbye to Irrelevant Search Results: Cohere Rerank Is Here,https://txt.cohere.com/rerank/
"We removed advertising cookies, here",https://blog.sentry.io/we-removed-advertising-cookies-heres-what-happened/
Options to mitigate the effect of browser cookie limitations | Adobe Analytics,https://experienceleague.adobe.com/docs/analytics/technotes/cookies/cookieless.html?lang=en
DeepEval,https://github.com/confident-ai/deepeval
Blog on LLM Evals,https://levelup.gitconnected.com/decoding-llm-performance-a-guide-to-evaluating-llm-applications-e8d7939cafce
VectorDB comparison,https://benchmark.vectorview.ai/vectordbs.html
VectorDB overview,https://thedataquarry.com/posts/vector-db-1/
https://www.together.ai/,https://www.together.ai/
Almost an Agent: What GPTs can do - by Ethan Mollick,https://www.oneusefulthing.org/p/almost-an-agent-what-gpts-can-do
State of AI & predictions for 2024,https://axflow.dev/blog/ai-predictions-2024/
HN Discussion,https://news.ycombinator.com/item?id=38915863
https://www.forbes.com/sites/robtoews/2023/12/21/10-ai-predictions-for-2024/?sh=60437cd24898,https://www.forbes.com/sites/robtoews/2023/12/21/10-ai-predictions-for-2024/?sh=60437cd24898
https://fortune.com/2024/01/04/generative-ai-predictions-come-back-down-to-earth-hype/,https://fortune.com/2024/01/04/generative-ai-predictions-come-back-down-to-earth-hype/
New NYT lawsuit vs LLMS,https://katedowninglaw.com/2024/01/06/the-new-york-times-launches-a-very-strong-case-against-microsoft-and-openai/
HN Discussion,https://news.ycombinator.com/item?id=38900197
code,https://github.com/jzhang38/TinyLlama
MSFT Phi2,https://huggingface.co/microsoft/phi-2
A New Approach to Synthetic Text Generation,https://www.watchful.io/blog/navigating-the-geometry-of-language-a-new-approach-to-synthetic-text-generation
SVD of Transformer Weight Matrices are Highly Interpretable,https://www.alignmentforum.org/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight
Barriers to GenAI Success,https://venturebeat.com/ai/forrester-identifies-biggest-barriers-to-generative-ai-success/
Efficient LLMs - distillation vs quantization,https://www.artfintel.com/p/efficient-llm-inference
"Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies",https://arxiv.org/abs/2309.13063
Intent Routing using LLMs,https://blog.getzep.com/building-an-intent-router-with-langchain-and-zep/
Exploring the Potential of LLMs for Intent Recognition,https://thecaicompany.com/2023/04/11/exploring-the-potential-of-llms-for-intent-recognition/
BerTopic (bert-based topic extraction),https://maartengr.github.io/BERTopic/index.html
The architecture of today,https://github.blog/2023-10-30-the-architecture-of-todays-llm-applications/
",https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e
Actions,https://platform.openai.com/docs/actions
Plugins,https://platform.openai.com/docs/plugins/introduction
Assistants,https://platform.openai.com/docs/assistants/overview
Tools,https://platform.openai.com/docs/assistants/tools
How to implement LLM guardrails,https://cookbook.openai.com/examples/how_to_use_guardrails
Summary,https://lifearchitect.ai/chinchilla/
Summary/Explainer,https://medium.com/@raniahossam/chinchilla-scaling-laws-for-large-language-models-llms-40c434e4e1c1
Analysis,https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models
Analysis/Implications,https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications
"Scaling laws for neural language models (OpenAI, 2020)",https://arxiv.org/abs/2001.08361
Training Compute-Optimal Large Language Models,https://arxiv.org/abs/2203.15556
Transformer Math,https://blog.eleuther.ai/transformer-math/
Stanford CS324 scaling law lecture notes,https://stanford-cs324.github.io/winter2022/assets/pdfs/Scaling%20laws%20pdf.pdf
Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws,https://arxiv.org/abs/2401.00448
GPT-4 follows scaling expectations,https://arxiv.org/abs/2303.08774
Scaling laws for Transfer,https://arxiv.org/abs/2102.01293
Instacart tech blog on prompt engineering,https://tech.instacart.com/monte-carlo-puppetry-and-laughter-the-unexpected-joys-of-prompt-engineering-4b9272e0c4eb
"Hackernews discussion about prompt engineering, especially enforcing structured data formatting (JSON)",https://news.ycombinator.com/item?id=38782678
"KTO, the direct-feedback method for model alignment (in the vein of RLHF / PPO / DPO)",https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_what-if-you-can-improve-llms-using-direct-activity-7142558802863689729-Du62
sign up,https://blog.oxen.ai/arxiv-dive-manifesto/
Practical ML on RAG (Retrieval-Augmented Generation) (Youtube recording yet to be posted),https://www.youtube.com/watch?v=dlJGCjCOkdM
associated repo,https://github.com/Oxen-AI/rag-dive
Machine learning podcast,https://www.themachinelearningpodcast.com/
https://changelog.com/practicalai,https://changelog.com/practicalai
Newsletter / Medium Blogstream,https://magazine.sebastianraschka.com/
https://eugeneyan.com/writing/llm-patterns/,https://eugeneyan.com/writing/llm-patterns/
https://eugeneyan.com/writing/llm-problems/,https://eugeneyan.com/writing/llm-problems/
https://eugeneyan.com/writing/finetuning/,https://eugeneyan.com/writing/finetuning/
https://huyenchip.com/2023/04/11/llm-engineering.html,https://huyenchip.com/2023/04/11/llm-engineering.html
https://huyenchip.com/2023/06/07/generative-ai-strategy.html,https://huyenchip.com/2023/06/07/generative-ai-strategy.html
"Fine tuning is for form, not facts",https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts
"You need RAG, Not Finetuning",https://zzbbyy.substack.com/p/why-you-need-rag-not-finetuning
You probably don,https://www.tidepool.so/2023/08/17/why-you-probably-dont-need-to-fine-tune-an-llm/
Hackernews discussion,https://news.ycombinator.com/item?id=37174850
Small But Mighty,https://www.unite.ai/small-but-mighty-small-language-models-breakthroughs-in-the-era-of-dominant-large-language-models/
SLMs vs LLMs: How Microsoft is Redefining AI,https://www.techopedia.com/slms-vs-llms
Apple,https://www.macrumors.com/2023/12/21/apple-ai-researchers-run-llms-iphones/
arxiv,https://arxiv.org/abs/2312.11514
hackernews discussion,https://news.ycombinator.com/item?id=38704982
https://medium.com/@yulemoon/a-complete-guide-to-llms-based-autonomous-agents-part-i-69515c016792,https://medium.com/@yulemoon/a-complete-guide-to-llms-based-autonomous-agents-part-i-69515c016792
https://levelup.gitconnected.com/actionable-chatgpt-let-chatgpt-become-personal-butler-6eeb7b69dd2f,https://levelup.gitconnected.com/actionable-chatgpt-let-chatgpt-become-personal-butler-6eeb7b69dd2f
ReAct,https://arxiv.org/abs/2210.03629
InterAct,https://arxiv.org/abs/2308.01552
AutoGen,https://microsoft.github.io/autogen/docs/Getting-Started
https://lilianweng.github.io/posts/2023-06-23-agent/,https://lilianweng.github.io/posts/2023-06-23-agent/
https://www.popularaitools.ai/blog/ai-agents-chatgpt-plugin-unleashing-ai-power,https://www.popularaitools.ai/blog/ai-agents-chatgpt-plugin-unleashing-ai-power
Warning,https://jina.ai/news/auto-gpt-unmasked-hype-hard-truths-production-pitfalls/
GPTs,https://openai.com/blog/introducing-gpts
press release,https://www.marktechpost.com/2023/12/18/microsoft-launches-gpt-rag-a-machine-learning-library-that-provides-an-enterprise-grade-reference-architecture-for-the-production-deployment-of-llms-using-the-rag-pattern-on-azure-openai/
repo,https://github.com/Azure/GPT-RAG
Azure ML Studio,https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in
MLFlow,https://mlflow.org/docs/latest/llms/openai/index.html
LangSmith,https://www.langchain.com/langsmith
Giskard,https://github.com/Giskard-AI/giskard
Confident-AI,https://www.confident-ai.com/
Pykoi,https://github.com/CambioML/pykoi
https://prodi.gy/,https://prodi.gy/
https://github.com/doccano/doccano,https://github.com/doccano/doccano
https://labelbox.com/product/annotate/,https://labelbox.com/product/annotate/
"Intro to DSPy: Goodbye Prompting, Hello Programming! | by Leonie Monigatti | Feb, 2024 | Towards Data Science",https://archive.is/N48cn
"GGUF, the long way around | ‚òÖ‚ù§‚ú∞ Vicki Boykis ‚òÖ‚ù§‚ú∞",https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/
The Transformer Family Version 2.0 | Lil'Log,https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/
"2402.16844 - Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding",https://arxiv.org/abs/2402.16844
"2402.15627 - MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",https://arxiv.org/abs/2402.15627
Researchers at Stanford Unveil C3PO: A Novel Machine Learning Approach for Context-Sensitive Customization of Large Language Models - MarkTechPost,https://www.marktechpost.com/2024/02/27/researchers-at-stanford-unveil-c3po-a-novel-machine-learning-approach-for-context-sensitive-customization-of-large-language-models/
2402.17764 - The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits,https://arxiv.org/abs/2402.17764
discussion,https://news.ycombinator.com/item?id=39535800
2310.11453 - BitNet: Scaling 1-bit Transformers for Large Language Models,https://arxiv.org/abs/2310.11453
2210.06313 - The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers,https://arxiv.org/abs/2210.06313
2402.17759 - Towards Optimal Learning of Language Models,https://arxiv.org/abs/2402.17759
Gemma | Kaggle,https://www.kaggle.com/models/google/gemma/frameworks/pyTorch
Introducing Pathways: A next-generation AI architecture,https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/
gemma_pytorch/gemma/model.py at main ¬∑ google/gemma_pytorch,https://github.com/google/gemma_pytorch/blob/main/gemma/model.py
Showcasing Agile Safety Classifiers with Gemma,https://codelabs.developers.google.com/codelabs/responsible-ai/agile-classifiers#0
Towards Long Context RAG ‚Äî LlamaIndex - Build Knowledge Assistants over your Enterprise Data,https://www.llamaindex.ai/blog/towards-long-context-rag
big-ann-benchmarks/neurips23/ongoing_leaderboard/leaderboard.md at main ¬∑ harsha-simhadri/big-ann-benchmarks,https://github.com/harsha-simhadri/big-ann-benchmarks/blob/main/neurips23/ongoing_leaderboard/leaderboard.md
What we learned in 6 months of working on a CodeGen dev tool GPT Pilot ‚Äì Pythagora Blog,https://blog.pythagora.ai/2024/02/19/gpt-pilot-what-did-we-learn-in-6-months-of-working-on-a-codegen-pair-programmer/
LLM Token based pricing: Embeddings and LLMs - Google Drive,https://docs.google.com/spreadsheets/d/1NX8ZW9Jnfpy88PC2d6Bwla87JRiv3GTeqwXoB4mKU_s/htmlview
Smarter Chatbots: How Semantic Routing Prevents the Unwanted,https://marvelousmlops.substack.com/p/smarter-chatbots-how-semantic-routing
,https://www.linkedin.com/posts/dhuynh95_lavague-selenium-transparent-activity-7170432809281409024-1u4Y
2402.18376 - Tokenization Is More Than Compression,https://arxiv.org/abs/2402.18376
"The $25,000,000,000 Eigenvector: The Linear Algebra behind Google",https://www.rose-hulman.edu/~bryan/googleFinalVersionFixed.pdf
2402.19194 - High Expectations: An Observational Study of Programming and Cannabis Intoxication,https://arxiv.org/abs/2402.19194
Dotfiles digest: git,https://adrg.se/blog/dotfiles-digest-git
Cloudflare announces Firewall for AI,https://blog.cloudflare.com/firewall-for-ai
OWASP Top 10 for Large Language Model Applications | OWASP Foundation,https://owasp.org/www-project-top-10-for-large-language-model-applications/
Launch HN: Greptile (YC W24) - RAG on codebases that actually works,https://news.ycombinator.com/item?id=39604961
Generative AI and the widening software developer knowledge gap | LeadDev,https://leaddev.com/team/generative-ai-and-widening-software-developer-knowledge-gap
discussion,https://news.ycombinator.com/item?id=39603163
Prompt injection and jailbreaking are not the same thing,https://simonwillison.net/2024/Mar/5/prompt-injection-jailbreaking/
LLM Leaderboard 2024,https://www.vellum.ai/llm-leaderboard
How Anthropic uses Claude for AI chatbot | Clem Delangue ü§ó posted on the topic,https://www.linkedin.com/posts/clementdelangue_this-is-required-reading-if-you-are-interested-activity-7171143202731765760-1HU0
bananaml/fructose,https://github.com/bananaml/fructose
llamagym,https://github.com/KhoomeiK/LlamaGym
instructor,https://jxnl.github.io/instructor/
transformer-aligner,https://github.com/openai/transformer-debugger
Philipp Schmid on LinkedIn: New evaluation benchmark & leaderboard by Allen Institute for AI (AI2). üî•‚Ä¶,https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_new-evaluation-benchmark-leaderboard-by-activity-7171853629325316096-67sr
"Matrix multiplication advancement could lead to faster, more efficient AI models - Ars Technica",https://arstechnica.com/information-technology/2024/03/matrix-multiplication-breakthrough-could-lead-to-faster-more-efficient-ai-models/
Paul Iusztin on LinkedIn: #machinelearning #mlops #datascience | 25 comments,https://www.linkedin.com/posts/pauliusztin_machinelearning-mlops-datascience-activity-7171792117088960513-UZaI
Gemma doesn't suck anymore - bug fixes.ipynb - Colab,https://colab.research.google.com/drive/1fxDWAfPIbC-bHwDSVj5SBmEJ6KG3bUu5?usp=sharing
Fixing Gemma Bugs,https://news.ycombinator.com/item?id=39671146
An Introduction to Knowledge Graphs - TextMine,https://textmine.com/post/an-introduction-to-knowledge-graphs
The job market for software engineers in 2024: data from 20M job postings - bloomberry,https://bloomberry.com/how-ai-is-disrupting-the-tech-job-market-data-from-20m-job-postings/
MEPs approve world's first comprehensive AI law,https://www.bbc.com/news/technology-68546450
"2403.01222 - Emotion Analysis in NLP: Trends, Gaps and Roadmap for Future Directions",https://arxiv.org/abs/2403.01222
2403.01590 - The Hidden Attention of Mamba Models,https://arxiv.org/abs/2403.01590
2402.19379 - Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy,https://arxiv.org/abs/2402.19379
2403.01851 - Rethinking LLM Language Adaptation: A Case Study on Chinese Mixtral,https://arxiv.org/abs/2403.01851
"2402.17177 - Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models",https://arxiv.org/abs/2402.17177
2403.03163 - Design2Code: How Far Are We From Automating Front-End Engineering?,https://arxiv.org/abs/2403.03163
[2402.14808] RelayAttention for Efficient Large Language Model Serving with Long System Prompts,https://arxiv.org/abs/2402.14808
The Pile,https://pile.eleuther.ai/
Paper page - ShortGPT: Layers in Large Language Models are More Redundant Than You Expect,https://arxiv.org/abs/2403.03853
Yi: Open Foundation Models by 01.AI,https://arxiv.org/abs/2403.04652
LLMGuard: Guarding Against Unsafe LLM Behavior,https://arxiv.org/abs/2403.00826
Is Cosine-Similarity of Embeddings Really About Similarity?,https://arxiv.org/abs/2403.05440
Mathematics of Neural Networks (Lecture Notes Graduate Course),https://arxiv.org/abs/2403.04807
Paper page - tinyBenchmarks: evaluating LLMs with fewer examples,https://arxiv.org/abs/2402.14992
Niels Rogge on LinkedIn: #documentai #microsoft #huggingface #artificialintelligence | 53 comments,https://www.linkedin.com/posts/niels-rogge-a3b7a3127_documentai-microsoft-huggingface-activity-7170801559675613184-0ACg
stealing part of a production LLM,https://arxiv.org/abs/2403.06634
New paper on search-in-the-Chain by Shicheng Xu | LlamaIndex posted on the topic,https://www.linkedin.com/posts/llamaindex_search-in-the-chain-this-paper-by-shicheng-activity-7174801183054729216-K3MC
"Likelihood functions, p-values, and the replication crisis - Arbital",https://arbital.com/p/likelihoods_not_pvalues/?l=4xx
How LLMs boost AI progress: Devin's demo | Andrew Ng posted on the topic,"https://www.linkedin.com/feed/update/urn:li:activity:7176663393708224512?commentUrn=urn:li:comment:(activity:7176663393708224512,7176742541713707008)&dashCommentUrn=urn:li:fsd_comment:(7176742541713707008,urn:li:activity:7176663393708224512)"
mshumer/gpt-prompt-engineer,https://github.com/mshumer/gpt-prompt-engineer
Aymeric Roucher on LinkedIn: ùêÅùêûùê∞ùêöùê´ùêû ùê®ùêü ùêãùêûùêöùêùùêûùê´ùêõùê®ùêöùê´ùêùùê¨ - ùêÅùêÆùê¢ùê•ùêù ùê≤ùê®ùêÆùê´ ùê®ùê∞ùêß‚Ä¶ | 14 comments,https://www.linkedin.com/posts/a-roucher_%3F%3F%3F%3F%3F%3F-%3F%3F-%3F%3F%3F%3F%3F%3F%3F%3F%3F%3F%3F%3F-activity-7178079605495353347-Yoq9
Eduardo Ordax on LinkedIn: #rag #genai #llmops | 13 comments,https://www.linkedin.com/posts/eordax_rag-genai-llmops-activity-7178771071121170432-Cs86
Microsoft Project Sophia,https://adoption.microsoft.com/en-us/project-sophia/
Claude 3 beats GPT-4 on Aider,https://aider.chat/2024/03/08/claude-3.html
Introducing DBRX: A New State-of-the-Art Open LLM | Databricks,https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm
2403.18421 - BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text,https://arxiv.org/abs/2403.18421
2403.17887 - The Unreasonable Ineffectiveness of the Deeper Layers,https://arxiv.org/abs/2403.17887
2403.18802 - Long-form factuality in large language models,https://arxiv.org/abs/2403.18802
2403.17919 - LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning,https://arxiv.org/abs/2403.17919
Introducing Jamba: AI21's Groundbreaking SSM-Transformer Model,https://www.ai21.com/blog/announcing-jamba
16 Changes to the Way Enterprises Are Building and Buying Generative AI | Andreessen Horowitz,https://a16z.com/generative-ai-enterprise-2024/
RAG Arena,https://www.ragarena.com/leaderboard
2403.15371 - Can large language models explore in-context?,https://arxiv.org/abs/2403.15371
2305.15038 - Is GPT-4 a Good Data Analyst?,https://arxiv.org/abs/2305.15038
2403.15042 - LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement,https://arxiv.org/abs/2403.15042
2403.15362 - CoLLEGe: Concept Embedding Generation for Large Language Models,https://arxiv.org/abs/2403.15362
Nvidia Blackwell announced,https://nvidianews.nvidia.com/news/nvidia-blackwell-dgx-generative-ai-supercomputing
Nvidia,https://www.semianalysis.com/p/nvidias-plans-to-crush-competition
What I learned from looking at 900 most popular open source AI tools,https://huyenchip.com/2024/03/14/ai-oss.html
"Groq Inference Tokenomics: Speed, But At What Cost?",https://www.semianalysis.com/p/groq-inference-tokenomics-speed-but
HN Discussion: Groq runs Mixtral 8x7B-32k with 500 T/s,https://news.ycombinator.com/item?id=39428880
A framework for exploring AI as a tech savvy org (Interconnected),https://interconnected.org/home/2023/12/08/ai-pathfinding
The surprising ease and effectiveness of AI in a loop (Interconnected),https://interconnected.org/home/2023/03/16/singularity
Who will build new search engines for new personal AI agents? (Interconnected),https://interconnected.org/home/2024/03/20/agents
2403.11901 - Larimar: Large Language Models with Episodic Memory Control,https://arxiv.org/abs/2403.11901
2403.12373 - RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners,https://arxiv.org/abs/2403.12373
2403.13372 - LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models,https://arxiv.org/abs/2403.13372
2403.12895 - mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding,https://arxiv.org/abs/2403.12895
Using Pydantic to manage config settings,https://www.linkedin.com/posts/pauliusztin_machinelearning-mlops-datascience-activity-7171429403837583361-bMbW
EU AI Act: first regulation on artificial intelligence,https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence
2312.11805 - Gemini: A Family of Highly Capable Multimodal Models,https://arxiv.org/abs/2312.11805
[2403.05530] Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,https://arxiv.org/abs/2403.05530
2403.08299 - AutoDev: Automated AI-Driven Development,https://arxiv.org/abs/2403.08299
2403.07008 - AutoEval Done Right: Using Synthetic Data for Model Evaluation,https://arxiv.org/abs/2403.07008
2403.06963 - The pitfalls of next-token prediction,https://arxiv.org/abs/2403.06963
"2304.14732 - Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks",https://arxiv.org/abs/2304.14732
2403.08950 - Exploring Prompt Engineering Practices in the Enterprise,https://arxiv.org/abs/2403.08950
2403.08319 - Knowledge Conflicts for LLMs: A Survey,https://arxiv.org/abs/2403.08319
2403.05812 - Algorithmic progress in language models,https://arxiv.org/abs/2403.05812
2403.07816 - Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM,https://arxiv.org/abs/2403.07816
2403.08763 - Simple and Scalable Strategies to Continually Pre-train Large Language Models,https://arxiv.org/abs/2403.08763
2403.05063 - Aligning Large Language Models for Controllable Recommendations,https://arxiv.org/abs/2403.05063
2403.09629 - Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking,https://arxiv.org/abs/2403.09629
"2403.09611 - MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",https://arxiv.org/abs/2403.09611
GPT in 60 Lines of NumPy | Jay Mody,https://jaykmody.com/blog/gpt-from-scratch/
GraphRAG: Unlocking LLM discovery on narrative private data - Microsoft Research,https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/
Anthropic announces Claude 3,https://www.anthropic.com/news/claude-3-family
aurelio-labs/semantic-router,https://github.com/aurelio-labs/semantic-router/tree/main
2402.19173 - StarCoder 2 and The Stack v2: The Next Generation,https://arxiv.org/abs/2402.19173
2403.00071 - Resonance RoPE: Improving Context Length Generalization of Large Language Models,https://arxiv.org/abs/2403.00071
2402.18734 - Priority Sampling of Large Language Models for Compilers,https://arxiv.org/abs/2402.18734
Beyond Language Models: Byte Models are Digital World Simulators,https://arxiv.org/abs/2402.19155
Ask HN: People who switched from GPT to their own models. How was it?,https://news.ycombinator.com/item?id=39519692
Anthony Accomazzo @ acco.io | URLs make ChatGPT stupid,https://www.acco.io/urls-make-chatgpt-stupid
Generative Artificial Intelligence and Data Privacy: A Primer,https://crsreports.congress.gov/product/pdf/R/R47569
2402.16459 - Defending LLMs against Jailbreaking Attacks via Backtranslation,https://arxiv.org/abs/2402.16459
2401.10774 - Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads,https://arxiv.org/abs/2401.10774
2402.10893 - RLVF: Learning from Verbal Feedback without Overgeneralization,https://arxiv.org/abs/2402.10893
Amazon Gives Anthropic $2.75 Billion So It Can Spend It On AWS XPUs,https://www.nextplatform.com/2024/03/27/amazon-gives-anthropic-2-75-billion-so-it-can-spend-it-on-aws-gpus/
Generative AI is already transforming SEO marketing efforts,https://www.marketingbrew.com/stories/2024/03/20/generative-ai-is-transforming-seo-marketing-efforts
The Secret Sauce behind 100K context window in LLMs: all tricks in one place | by Galina Alperovich | GoPenAI,https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c?gi=08ee7ba3996d
GitHub - AnswerDotAI/rerankers,https://github.com/AnswerDotAI/rerankers/
Your AI Product Needs Evals ‚Äì Hamel's Blog,https://hamel.dev/blog/posts/evals/
Designing RAGs. A guide to Retrieval-Augmented‚Ä¶ | by Micha≈Ç Oleszak | Towards Data Science,https://towardsdatascience.com/designing-rags-dbb9a7c1d729
Deploying a Production-Ready RAG Server: A Comprehensive Guide with LlamaIndex | by Marco Bertelli | Level Up Coding,https://levelup.gitconnected.com/deploying-a-production-ready-rag-server-a-comprehensive-guide-with-llamaindex-dbe57cc960df
"developersdigest/llm-answer-engine: Build a Perplexity-Inspired Answer Engine Using Next.js, Groq, Llama-3, Langchain, OpenAI, Upstash, Brave & Serper",https://github.com/developersdigest/llm-answer-engine
A Builder's Guide to Evals for LLM-based Applications | Eugene Yan,https://eugeneyan.com/writing/evals/
Stanford CS25: V3 I Retrieval Augmented Language Models,https://www.youtube.com/watch?v=mE7IDf2SmJg&t=653s
infiniflow/ragflow: RAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding.,https://github.com/infiniflow/ragflow
DSPy,https://dspy.ai/
Running OCR against PDFs and images directly in the browser,https://news.ycombinator.com/item?id=39877391
Ask HN: Is anybody getting value from AI Agents? How so?,https://news.ycombinator.com/item?id=39886178
3Blue1Brown: But what is a GPT? [video],https://news.ycombinator.com/item?id=39898221
Common Settings | docs.ST.app,https://docs.sillytavern.app/usage/common-settings/#sampler-parameters
What is Beam Search? Explaining The Beam Search Algorithm | Width.ai,https://www.width.ai/post/what-is-beam-search
How to generate text: using different decoding methods for language generation with Transformers,https://huggingface.co/blog/how-to-generate
Trying simple tree-search techniques for LLM token sampling - ‚å®Ô∏èü§∑üèª‚Äç‚ôÇÔ∏èüì∑,https://andys.page/posts/llm_sampling_strategies/
First Impressions of Early-Access GPT-4 Fine-Tuning | Supersimple,https://www.supersimple.io/blog/gpt-4-fine-tuning-early-access
2402.02244 - Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models,https://arxiv.org/abs/2402.02244
2401.01325 - LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning,https://arxiv.org/abs/2401.01325
2403.18684 - Scaling Laws For Dense Retrieval,https://arxiv.org/abs/2403.18684
[2403.10131] RAFT: Adapting Language Model to Domain Specific RAG,https://arxiv.org/abs/2403.10131
The New Language Model Stack | Sequoia Capital,https://www.sequoiacap.com/article/llm-stack-perspective/
"Yingjun Wu @RisingWave on X: ""The landscape of the vector database market https://t.co/g8Jjrf7aye"" / X",https://twitter.com/YingjunWu/status/1667232357953466369/
Banning Open-Weight Models Would be a Disaster,https://rbren.substack.com/p/banning-open-weight-models-would
OpenAI customer story: Harvey,https://openai.com/customer-stories/harvey
Joanna Stoffregen on LinkedIn: RAG #5 - Costs of a RAG app,https://www.linkedin.com/posts/joannastoffregen_rag-5-costs-of-a-rag-app-ugcPost-7181036773441982465-GWZ1
"nilsherzig/LLocalSearch: ""Perplexity-style"" local meta-search engine using LLM Agents. The user can ask a question and the system will use a chain of LLMs to find the answer. The user can see the progress of the agents and the final answer. No OpenAI or Google API keys are needed.",https://github.com/nilsherzig/LLocalSearch
"ü™Ñ Lumos: Language Agents with Unified Data Formats, Modular Design, and Open-Source LLMs",https://allenai.github.io/lumos/
2404.00450 - Planning and Editing What You Retrieve for Enhanced Tool Learning,https://arxiv.org/abs/2404.00450
2404.02249 - RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction,https://arxiv.org/abs/2404.02249
2403.18365 - BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models,https://arxiv.org/abs/2403.18365
2312.17543 - Building Efficient Universal Classifiers with Natural Language Inference,https://arxiv.org/abs/2312.17543
Moritz Laurer on LinkedIn: BERT-based zero-shot classifiers,https://www.linkedin.com/posts/moritz-laurer_releasing-a-new-series-of-8-zeroshot-classifiers-activity-7181324010826121216-3U-S
Zeroshot Classifiers - a MoritzLaurer Collection,https://huggingface.co/collections/MoritzLaurer/zeroshot-classifiers-6548b4ff407bb19ff5c3ad6f
2404.01261 - FABLES: Evaluating faithfulness and content selection in book-length summarization,https://arxiv.org/abs/2404.01261
Marker-Inc-Korea/AutoRAG: AutoRAG: An Open-Source Framework for Retrieval-Augmented Generation (RAG) Evaluation & Optimization with AutoML-Style Automation,https://github.com/Marker-Inc-Korea/AutoRAG
Antonio Montano ü™Ñ on LinkedIn: #machinelearning,https://www.linkedin.com/posts/montano_machinelearning-activity-7180890274838024193-CnK5
Li Yin on LinkedIn: #ai #ml #rag #llms,https://www.linkedin.com/posts/li-yin-ai_ai-ml-rag-activity-7182625663755894784-Hu4n
LlamaIndex on LinkedIn: This is the best tutorial we've seen on building a full-stack RAG app that‚Ä¶ | 11 comments,https://www.linkedin.com/posts/llamaindex_this-is-the-best-tutorial-weve-seen-on-building-activity-7182744191129563136-hW-w
LangChain on LinkedIn: üíôBuilding a RAG chat bot with Azure OpenAI and LangChain.js Another‚Ä¶ | 33 comments,https://www.linkedin.com/posts/langchain_building-a-rag-chat-bot-with-azure-openai-activity-7182784194077470721-R1fP
Eugene Yan - Home,https://eugeneyan.com/writing/evals/#summarization-consistency-relevance-length
BeeTrove - OpenAI GPTs Dataset,https://beetrove.com/
Inside Big Tech's underground race to buy AI training data | Reuters,https://www.reuters.com/technology/inside-big-techs-underground-race-buy-ai-training-data-2024-04-05/
"Will Search Be Generative AI or Blue Links? Actually, It's Both.",https://www.bigtechnology.com/p/will-search-be-generative-ai-or-blue
Hamza Tahir on LinkedIn: Yesterday morning Hamel H. kicked off a lively discussion on the role of‚Ä¶,https://www.linkedin.com/posts/hamzatahirofficial_yesterday-morning-hamel-h-kicked-off-a-lively-activity-7178696693339611137-iCel
The Price of Intelligence ‚Äî Understanding the Cost of Using LLMs | MongoDB,https://medium.com/mongodb/the-cost-of-using-llms-f5e30b26c342
2404.03502 - AI and the Problem of Knowledge Collapse,https://arxiv.org/abs/2404.03502
2404.01413 - Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data,https://arxiv.org/abs/2404.01413
2404.03602 - Evaluating LLMs at Detecting Errors in LLM Responses,https://arxiv.org/abs/2404.03602
Building reliable systems out of unreliable agents | Rainforest QA,https://www.rainforestqa.com/blog/building-reliable-systems-out-of-unreliable-agents
Building Reliable Systems Out of Unreliable Agents,https://news.ycombinator.com/item?id=39984209
You can't build a moat with AI,https://generatingconversation.substack.com/p/you-cant-build-a-moat-with-ai
Preprocessing Unstructured Data for LLM Applications - DeepLearning.AI,https://www.deeplearning.ai/short-courses/preprocessing-unstructured-data-for-llm-applications/
"Practical AI: Machine Learning, Data Science: RAG continues to rise on Apple Podcasts",https://podcasts.apple.com/us/podcast/practical-ai-machine-learning-data-science/id1406537385?i=1000652030784
Lessons after a half-billion GPT tokens - Ken Kantzer's Blog,https://kenkantzer.com/lessons-after-a-half-billion-gpt-tokens/
The Machine Learning Podcast: Strategies For Building A Product Using LLMs At DataChat on Apple Podcasts,https://podcasts.apple.com/us/podcast/strategies-for-building-a-product-using-llms-at-datachat/id1626358243?i=1000647830981
How SetFit works with LLMs | Marcel Marais posted on the topic,https://www.linkedin.com/posts/marcelmarais1_setfit-overview-ugcPost-7184882879707938817-6Wjp
"Mastering RAG Systems: From Fundamentals to Advanced, with Strategic Component Evaluation | by Hamza Gharbi | Apr, 2024 | Towards Data Science",https://towardsdatascience.com/mastering-rag-systems-from-fundamentals-to-advanced-with-strategic-component-evaluation-3551be31858f
[Video] LlamaIndex: an introduction‚Ä¶ | 12 comments,https://www.linkedin.com/posts/llamaindex_were-excited-to-release-a-brand-new-tutorial-activity-7185664762754596866-uOdb
WizardLM 2,https://wizardlm.github.io/WizardLM2/
Pile-T5 | EleutherAI Blog,https://blog.eleuther.ai/pile-t5/
2404.07647 - Why do small language models underperform? Studying Language Model Saturation via the Softmax Bottleneck,https://arxiv.org/abs/2404.07647
2404.08634 - Pre-training Small Base LMs with Fewer Tokens,https://arxiv.org/abs/2404.08634
"Agentic Design Patterns Part 5, Multi-Agent Collaboration",https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/
Introducing Meta Llama 3: The most capable openly available LLM to date,https://ai.meta.com/blog/meta-llama-3/
Snowflake Launches Practical Text-Embedding Model for Retrieval use Cases,https://www.snowflake.com/blog/introducing-snowflake-arctic-embed-snowflakes-state-of-the-art-text-embedding-family-of-models/
Build apps with advanced RAG using Azure AI Search and LlamaIndex,https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/advanced-rag-with-azure-ai-search-and-llamaindex/ba-p/4115007
"Inside The New AI Index: Expensive New Models, Targeted Investments, and More",https://hai.stanford.edu/news/inside-new-ai-index-expensive-new-models-targeted-investments-and-more
Announcing MLCommons AI Safety v0.5 Proof of Concept - MLCommons,https://mlcommons.org/2024/04/mlc-aisafety-v0-5-poc/
2404.10198 - How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs' internal prior,https://arxiv.org/abs/2404.10198
2404.10102 - Chinchilla Scaling: A replication attempt,https://arxiv.org/abs/2404.10102
2404.10719 - Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study,https://arxiv.org/abs/2404.10719
2404.11018 - Many-Shot In-Context Learning,https://arxiv.org/abs/2404.11018
"2404.11584 - The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",https://arxiv.org/abs/2404.11584
2404.10981 - A Survey on Retrieval-Augmented Text Generation for Large Language Models,https://arxiv.org/abs/2404.10981
2404.06283 - LLMs' Reading Comprehension Is Affected by Parametric Knowledge and Struggles with Hypothetical Statements,https://arxiv.org/abs/2404.06283
LoRA fine-tuning of embedding models using LlamaIndex | by Mariboo | Medium,https://medium.com/@diagnosta/lora-fine-tuning-of-embedding-models-using-llamaindex-a60b823a2c94
Notes on how to use LLMs in your product. | Irrational Exuberance,https://lethain.com/mental-model-for-how-to-use-llms-in-products/
A simple Python implementation of the ReAct pattern for LLMs | Simon Willison's TILs,https://til.simonwillison.net/llms/python-react-pattern
lm-hackers/lm-hackers.ipynb at main ¬∑ fastai/lm-hackers,https://github.com/fastai/lm-hackers/blob/main/lm-hackers.ipynb
Understanding What Matters for LLM Ingestion and Preprocessing ‚Äì Unstructured,https://unstructured.io/blog/understanding-what-matters-for-llm-ingestion-and-preprocessing
PsyArXiv Preprints | Skill but not Effort Drive GPT Overperformance over Humans in Cognitive Reframing of Negative Scenarios,https://osf.io/preprints/psyarxiv/fzvd8
"2404.12253 - Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing",https://arxiv.org/abs/2404.12253
HN discussion on previously shared [2404.11584] Survey Study on AI Agent Architectures (2024),https://news.ycombinator.com/item?id=40115482
2404.12753 - AutoCrawler: A Progressive Understanding Web Agent for Web Crawler Generation,https://arxiv.org/abs/2404.12753
2404.12272 - Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences,https://arxiv.org/abs/2404.12272
2404.12457 - RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation,https://arxiv.org/abs/2404.12457
2404.12872 - LLM-R2: A Large Language Model Enhanced Rule-based Rewrite System for Boosting Query Efficiency,https://arxiv.org/abs/2404.12872
TheFastest.ai,https://thefastest.ai/?r=iad
Introducing more enterprise-grade features for API customers,https://openai.com/blog/more-enterprise-grade-features-for-api-customers
LLMs and the Harry Potter problem,https://www.pyqai.com/blog/llms-and-the-harry-potter-problem
"Wing Lian (caseus) on X: ""Llama 3 achieves pretty good recall to 65k context w/ rope_theta set to 16M https://t.co/YZuK7ym7fU"" / X",https://twitter.com/winglian/status/1783122644579090600
2404.14619 - OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework,https://arxiv.org/abs/2404.14619
2404.15574 - Retrieval Head Mechanistically Explains Long-Context Factuality,https://arxiv.org/abs/2404.15574
2404.14219 - Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,https://arxiv.org/abs/2404.14219
"The Art of Efficient Search: Scalar Quantization and Vectors | by Rithvik Panchapakesan | Snowflake | Apr, 2024 | Medium",https://medium.com/snowflake/the-art-of-efficient-search-scalar-quantization-and-vectors-f912fd2e9bb7
Andy McMahon on LinkedIn: GitHub - stanfordnlp/dspy: DSPy: The framework for programming‚Äînot‚Ä¶ | 16 comments,https://www.linkedin.com/posts/andrew-p-mcmahon_github-stanfordnlpdspy-dspy-the-framework-activity-7188076600255524869-rSXt
microsoft/sammo: A library for prompt engineering and optimization (SAMMO = Structure-aware Multi-Objective Metaprompt Optimization),https://github.com/microsoft/sammo
Marvin Documentation - Marvin,https://www.askmarvin.ai/welcome/overview/
Haystack. The proverbial needle in a stack of LLM orchestrators. | by Jldevtech | Medium,https://medium.com/@jldevtech/haystack-the-proverbial-needle-in-a-stack-of-llm-orchestrators-94f65fca13df
RAG Evaluation Series: Validating the RAG Performance of LangChain vs Haystack | Blog | Tonic.a,https://www.tonic.ai/blog/rag-evaluation-series-validating-the-rag-performance-of-langchain-vs-haystack
The Problem With LangChain | Max Woolf's Blog,https://minimaxir.com/2023/07/langchain-problem/
Are AI agents the next big thing in the generative race?,https://www.emergingtechbrew.com/stories/2024/04/26/are-ai-agents-the-next-big-thing?mbcid=35172458.107154&mblid=0578e6ac4582&mid=f3319b3034940a0a197931d5aae5b037
Build a Complete OpenSource LLM RAG QA Chatbot ‚Äî An In-depth Journey (Introduction) | by Marco Bertelli | Medium,https://medium.com/@marco.bertelli/build-a-complete-opensource-llm-rag-qa-chatbot-an-in-depth-journey-introduction-c630b16c330c
carsonpo/haystackdb,https://github.com/carsonpo/haystackdb
discussion,https://news.ycombinator.com/item?id=40192552
GitHub Copilot Workspace: Welcome to the Copilot-native developer environment - The GitHub Blog,https://github.blog/2024-04-29-github-copilot-workspace/
ArXiv Machine Learning Landscape,https://lmcinnes.github.io/datamapplot_examples/ArXiv_data_map_example.html
2404.14723 - Insights into Alignment: Evaluating DPO and its Variants Across Multiple Tasks,https://arxiv.org/abs/2404.14723
2404.15758 - Let's Think Dot by Dot: Hidden Computation in Transformer Language Models,https://arxiv.org/abs/2404.15758
OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments,https://os-world.github.io/
discussion,https://news.ycombinator.com/item?id=40191047
2402.06925 - A Thorough Examination of Decoding Methods in the Era of LLMs,https://arxiv.org/abs/2402.06925
2404.17347 - InspectorRAGet: An Introspection Platform for RAG Evaluation,https://arxiv.org/abs/2404.17347
IBM/InspectorRAGet: The repository contains generative AI analytics platform application code.,https://github.com/IBM/InspectorRAGet
2404.18231 - From Persona to Personalization: A Survey on Role-Playing Language Agents,https://arxiv.org/abs/2404.18231
2404.18796 - Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models,https://arxiv.org/abs/2404.18796
"Wing Lian (caseus) on X: ""Llama 3 achieves pretty good recall to 65k context w/ rope_theta set to 16M [https://t.co/YZuK7ym7fU",https://t.co/YZuK7ym7fU
2404.16789 - Continual Learning of Large Language Models: A Comprehensive Survey,https://www.databricks.com/resources/ebook/big-book-generative-ai?scid=701Vp000003wPHFIA2
2404.13208 - The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions,https://arxiv.org/abs/2404.13208
2404.12096 - LongEmbed: Extending Embedding Models for Long Context Retrieval,https://arxiv.org/abs/2404.12096
2404.14047 - How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study,https://arxiv.org/abs/2404.14047
2404.10830 - Fewer Truncations Improve Language Modeling,https://arxiv.org/abs/2404.10830
LlamaIndex: an Introduction to Agents,https://www.youtube.com/playlist?list=PLTZkGHtR085YI9BL1tk6Qti5JaRA1NnkU
then taken down for toxicity testing,https://twitter.com/WizardLM_AI/status/1780101465950105775
"Simon Willison on X: ""50% discount on OpenAI pricing if you submit a batch and give them up to 24 hours to run the prompts for you",https://twitter.com/simonw/status/1779932174592167965
2404.08801 - Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length,https://arxiv.org/abs/2404.08801
2404.09173 - TransformerFAM: Feedback attention is working memory,https://arxiv.org/abs/2404.09173
www.pmi.org,http://www.pmi.org/
Three major LLM releases in 24 hours (plus weeknotes),https://simonwillison.net/2024/Apr/10/weeknotes-llm-releases/
RAG continues to rise | Practical AI Podcast #264,https://changelog.com/practicalai/264
2404.05961 - LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders,https://arxiv.org/abs/2404.05961
2310.01208 - Label Supervised LLaMA Finetuning,https://arxiv.org/abs/2310.01208
2404.04689 - Multicalibration for Confidence Scoring in LLMs,https://arxiv.org/abs/2404.04689
2404.02255 -,https://arxiv.org/abs/2404.02255
2404.07503 - Best Practices and Lessons Learned on Synthetic Data for Language Models,https://arxiv.org/abs/2404.07503
2404.07413 - JetMoE: Reaching Llama2 Performance with 0.1M Dollars,https://arxiv.org/abs/2404.07413
2404.04392 - Increased LLM Vulnerabilities from Fine-tuning and Quantization,https://arxiv.org/abs/2404.04392
2404.06654 - RULER: What's the Real Context Size of Your Long-Context Language Models?,https://arxiv.org/abs/2404.06654
2404.07143 - Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention,https://arxiv.org/abs/2404.07143
2404.07979 - LLoCO: Learning Long Contexts Offline,https://arxiv.org/abs/2404.07979
2404.02258 - Mixture-of-Depths: Dynamically allocating compute in transformer-based language models,https://arxiv.org/abs/2404.02258
2404.07965 - Rho-1: Not All Tokens Are What You Need,https://arxiv.org/abs/2404.07965
2404.07839 - RecurrentGemma: Moving Past Transformers for Efficient Open Language Models,https://arxiv.org/abs/2404.07839
Griffin,https://arxiv.org/abs/2402.19427
"Full Steam Ahead: The 2024 MAD (Machine Learning, AI & Data) Landscape",https://mattturck.com/mad2024/
Mastering RAG Pattern Chatbots: Azure OpenAI and LangChain.js Integration | Azure Devs JS Day 2024,https://techcommunity.microsoft.com/t5/microsoft-developer-community/building-a-rag-pattern-chat-bot-with-azure-openai-and-langchain/ba-p/4105727
code,https://github.com/Azure-Samples/azure-search-openai-javascript/
code,https://github.com/rsrohan99/rag-stream-intermediate-events-tutorial?tab=readme-ov-file
2404.04204 - Social Skill Training with Large Language Models,https://arxiv.org/abs/2404.04204
2305.05176 - FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance,https://arxiv.org/abs/2305.05176
2404.01037 - ARAGOG: Advanced RAG Output Grading,https://arxiv.org/abs/2404.01037
"2404.04125 - No ""Zero-Shot"" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance",https://arxiv.org/abs/2404.04125
2402.09727 - A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts,https://arxiv.org/abs/2402.09727
2404.03163 - Uncertainty in Language Models: Assessment through Rank-Calibration,https://arxiv.org/abs/2404.03163
2403.15157 - AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large Language Models,https://arxiv.org/abs/2403.15157
2403.12031 - ROUTERBENCH: A Benchmark for Multi-LLM Routing System,https://arxiv.org/abs/2403.12031
2403.14403 - Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity,https://arxiv.org/abs/2403.14403
2403.19889 - Towards a Robust Retrieval-Based Summarization System,https://arxiv.org/abs/2403.19889
2403.20327 - Gecko: Versatile Text Embeddings Distilled from Large Language Models,https://arxiv.org/abs/2403.20327
2403.20041 - Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs,https://arxiv.org/abs/2403.20041
www.pmi.org,https://www.pmi.org/
A Survey of Causal Inference Applications at Netflix | by Netflix Technology Blog | Netflix TechBlog,https://netflixtechblog.com/a-survey-of-causal-inference-applications-at-netflix-b62d25175e6f?gi=0c61dde7007f
Build Your Second Brain One Piece At A Time,https://www.dataengineeringpodcast.com/episodepage/build-your-second-brain-one-piece-at-a-time
SuperpoweredAI/spRAG: High-performance RAG framework for unstructured data,https://github.com/SuperpoweredAI/spRAG
Show HN: SpRAG ‚Äì Open-source RAG implementation for challenging real-world tasks,https://news.ycombinator.com/item?id=40237546
2404.19737 - Better & Faster Large Language Models via Multi-token Prediction,https://arxiv.org/abs/2404.19737
quite good discussion,https://news.ycombinator.com/item?id=40220851
2404.19553 - Extending Llama-3's Context Ten-Fold Overnight,https://arxiv.org/abs/2404.19553
2404.19705 - When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively,https://arxiv.org/abs/2404.19705
Musings on Building a Generative AI Product,https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product
Chip Huyen on LinkedIn: #aiengineering #llms #aiapplication | 77 comments,https://www.linkedin.com/posts/chiphuyen_aiengineering-llms-aiapplication-activity-7193302509845692416--yPc
New Microsoft AI model may challenge GPT-4 and Google Gemini | Ars Technica,https://arstechnica.com/information-technology/2024/05/microsoft-developing-mai-1-language-model-that-may-compete-with-openai-report/
Humam Kourani on LinkedIn: #ijcai2024 #promoai #bpmn #petrinet #openai #businessprocessmanagement‚Ä¶ | 28 comments,https://www.linkedin.com/posts/humam-kourani-98b342232_ijcai2024-promoai-bpmn-activity-7190623978518904833-dIzf
"2405.00732 - LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report",https://arxiv.org/abs/2405.00732
2405.01470 - WildChat: 1M ChatGPT Interaction Logs in the Wild,https://arxiv.org/abs/2405.01470
knn-router/deploy/pulze-intent-v0.1 at main ¬∑ pulzeai-oss/knn-router,https://github.com/pulzeai-oss/knn-router/tree/main/deploy/pulze-intent-v0.1
grafychat,https://www.grafychat.com/
Model Spec (2024/05/08),https://cdn.openai.com/spec/model-spec-2024-05-08.html
Hallucination-Free RAG: Making LLMs Safe for Healthcare | Matt Yeung,https://mattyyeung.github.io/deterministic-quoting
2405.04324 - IBM Granite Code Models: A Family of Open Foundation Models for Code Intelligence,https://arxiv.org/abs/2405.04324
2405.04520 - NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts,https://arxiv.org/abs/2405.04520
2405.03133 - Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training,https://arxiv.org/abs/2405.03133
"2405.04434 - DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",https://arxiv.org/abs/2405.04434
2310.14424 - Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation,https://arxiv.org/abs/2310.14424
2311.17295 - Elo Uncovered: Robustness and Best Practices in Language Model Evaluation,https://arxiv.org/abs/2311.17295
"2405.05374 - Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models",https://arxiv.org/abs/2405.05374
Chip Huyen on LinkedIn,https://www.linkedin.com/posts/chiphuyen_aiengineering-llms-aievaluation-activity-7194734998376050688-uP2s
Streaming Pipelines for LLMs and RAG | Decoding ML,https://medium.com/decodingml/sota-python-streaming-pipelines-for-fine-tuning-llms-and-rag-in-real-time-82eb07795b87#fefe
4 Advanced RAG Algorithms You Must Know - by Paul Iusztin,https://decodingml.substack.com/p/the-4-advanced-rag-algorithms-you
"Subbarao Kambhampati on X: ""On the ""Chain of Thought"" Delusions",https://twitter.com/rao2z/status/1760133260385177784
License to Call: Introducing Transformers Agents 2.0,https://huggingface.co/blog/agents
2405.05904 - Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?,https://arxiv.org/abs/2405.05904
2405.03548 - MAmmoTH2: Scaling Instructions from the Web,https://arxiv.org/abs/2405.03548
"2405.01769 - A Survey on Large Language Models for Critical Societal Domains: Finance, Healthcare, and Law",https://arxiv.org/abs/2405.01769
2404.17605 - Autonomous LLM-driven research from data to human-verifiable research papers,https://arxiv.org/abs/2404.17605
data-to-paper: AI-driven scientific research | GitHub,https://github.com/Technion-Kishony-lab/data-to-paper?tab=readme-ov-file
"2402.01817 - LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",https://arxiv.org/abs/2402.01817
"2405.01116 - ""In-Context Learning"" or: How I learned to stop worrying and love ""Applied Information Retrieval""",https://arxiv.org/abs/2405.01116
2405.04674 - Towards Accurate and Efficient Document Analytics with Large Language Models,https://arxiv.org/abs/2405.04674
2405.07863 - RLHF Workflow: From Reward Modeling to Online RLHF,https://arxiv.org/abs/2405.07863
[2405.06624] Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems,https://arxiv.org/abs/2405.06624
2405.06394 - Memory Mosaics,https://arxiv.org/abs/2405.06394
reworkd/tarsier: Vision utilities for web interaction agents üëÄ,https://github.com/reworkd/tarsier
Show HN: Tarsier ‚Äì Vision utilities for web interaction agents,https://news.ycombinator.com/item?id=40369319
LLMs are not suitable for brainstorming ‚Äì piaoyang,https://piaoyang0.wordpress.com/2024/05/15/llms-are-not-suitable-for-brainstorming/
Can You Run It? LLM version - a Hugging Face Space by Vokturz,https://huggingface.co/spaces/Vokturz/can-it-run-llm
Transformer Inference Arithmetic | kipply's blog,https://kipp.ly/transformer-inference-arithmetic/
Viking 7B: The first open LLM for the Nordic languages,https://www.silo.ai//blog/viking-7b-the-first-open-llm-for-the-nordic-languages
Scaling the pre-training of large language models of >100B parameters to thousands of AMD MI250X GPUs on LUMI - LUMI,https://lumi-supercomputer.eu/scaling-the-pre-training-of-large-language-models-of-100b-parameters-to-thousands-of-amd-mi250x-gpus-on-lumi/
2405.09220 - ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models,https://arxiv.org/abs/2405.09220
2405.08944 - Challenges in Deploying Long-Context Transformers: A Theoretical Peak Performance Analysis,https://arxiv.org/abs/2405.08944
"2405.08760 - Is the Pope Catholic? Yes, the Pope is Catholic. Generative Evaluation of Intent Resolution in LLMs",https://arxiv.org/abs/2405.08760
Better LLM Prompting using the Panel-of-Experts,https://sourcery.ai/blog/panel-of-experts/
Don't tell me what (not) to do!,https://sourcery.ai/blog/dont-tell-me-what-not-to-do/
Unit Testing Code with a Mind of Its Own,https://sourcery.ai/blog/unit-testing-llm-output/
Systematically Improving Your RAG - jxnl.co,https://jxnl.co/writing/2024/05/22/systematically-improving-your-rag/
High Agency Pydantic > VC Backed Frameworks ‚Äî with Jason Liu of Instructor,https://www.latent.space/p/instructor
instructor-ai/instructor: structured outputs for llms,https://github.com/instructor-ai/instructor
"microsoft/Phi-3CookBook: This is a Phi-3 book for getting started with Phi-3. Phi-3, a family of open AI models developed by Microsoft. Phi-3 models are the most capable and cost-effective small language models (SLMs) available, outperforming models of the same size and next size up across a variety of language, reasoning, coding, and math benchmarks.",https://github.com/microsoft/Phi-3CookBook
HN discussion,https://news.ycombinator.com/item?id=40431680
"Jerry Liu on X: ""The future of RAG depends on incorporating deep visual understanding in the indexing/retrieval process, beyond just naive text parsing ü•Ω In this cookbook, we use GPT-4o to process a slide deck; not only do we parse the text, but we also translate charts/figures into tables that https://t.co/xN6DUk33Gp"" / X",https://x.com/jerryjliu0/status/1793077217632940304
"Jo Kristian Bergum on LinkedIn: Using this weird trick, we can normalize raw BM25 scores into a range‚Ä¶ | 18 comments",https://www.linkedin.com/posts/jo-bergum_using-this-weird-trick-we-can-normalize-activity-7199018812648656897-d65v
2405.12250 - Your Transformer is Secretly Linear,https://arxiv.org/abs/2405.12250
"[D] AI Agents: too early, too expensive, too unreliable : r/MachineLearning",https://www.reddit.com/r/MachineLearning/comments/1cy1kn9/d_ai_agents_too_early_too_expensive_too_unreliable/
"Financial Statement Analysis with Large Language Models by Alex Kim, Maximilian Muhn, Valeri V. Nikolaev :: SSRN",https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4835311
California Senate Passes SB 1047 - by Dean W. Ball,https://www.hyperdimensional.co/p/california-senate-passes-sb-1047
"Evidently 0.4.25: An open-source tool to evaluate, test and monitor your LLM-powered apps",https://www.evidentlyai.com/blog/open-source-llm-evaluation
What We've Learned From A Year of Building with LLMs,https://eugeneyan.com/writing/llm-lessons/
HN discussion,https://news.ycombinator.com/item?id=40508390
A Hybrid information retriever with DuckDB | Architecture & Performance,https://aetperf.github.io/2024/05/30/A-Hybrid-information-retriever-with-DuckDB.html
2402.01030 - Executable Code Actions Elicit Better LLM Agents,https://arxiv.org/abs/2402.01030
2405.14741 - Bagging Improves Generalization Exponentially,https://arxiv.org/abs/2405.14741
2405.18414 - Don't Forget to Connect! Improving RAG with Graph-based Reranking,https://arxiv.org/abs/2405.18414
2403.01643 - You Need to Pay Better Attention,https://arxiv.org/abs/2403.01643
2405.18669 - Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities,https://arxiv.org/abs/2405.18669
Introducing OpenAI for Nonprofits | OpenAI,https://openai.com/index/introducing-openai-for-nonprofits/
What happened with AI Overviews and next steps,https://blog.google/products/search/ai-overviews-update-may-2024/
What We Learned from a Year of Building with LLMs (Part I),https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/
Prompting Fundamentals and How to Apply them Effectively,https://eugeneyan.com/writing/prompting/
"Indexing all of Wikipedia, on a laptop",https://foojay.io/today/indexing-all-of-wikipedia-on-a-laptop/
Training and Finetuning Embedding Models with Sentence Transformers v3,https://huggingface.co/blog/train-sentence-transformers
Hugging Face on AMD Instinct MI300 GPU,https://huggingface.co/blog/huggingface-amd-mi300
2405.19893 - Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered Thoughts,https://arxiv.org/abs/2405.19893
2405.19874 - Is In-Context Learning Sufficient for Instruction Following in LLMs,https://arxiv.org/abs/2405.19874
2405.19888 - Parrot: Efficient Serving of LLM-based Applications with Semantic Variable,https://arxiv.org/abs/2405.19888
EU Approves Historic AI Act: Pioneering Comprehensive AI Regulation - vcsi.org,https://vcsi.org/eu-ai-act-comprehensive-regulation/
2405.17399 - Transformers Can Do Arithmetic with the Right Embeddings,https://arxiv.org/abs/2405.17399
2405.15071 - Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization,https://arxiv.org/abs/2405.15071
2405.17428 - NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models,https://arxiv.org/abs/2405.17428
2405.15319 - Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training,https://arxiv.org/abs/2405.15319
2405.17247 - An Introduction to Vision-Language Modeling,https://arxiv.org/abs/2405.17247
Sandboxed code interpreter using LlamaIndex with Azure Container Apps dynamic sessions,https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions
"Using this weird trick, we can normalize raw BM25 scores into a range compatible with cosine similarity",https://www.linkedin.com/posts/jo-bergum_using-this-weird-trick-we-can-normalize-activity-7199018812648656897-d65v/
Dense-to-Question and Sparse-to-Answer: Hybrid Retriever System for Industrial Frequently Asked Questions,https://www.mdpi.com/2227-7390/10/8/1335
Mapping the Mind of a Large Language Model,https://www.anthropic.com/research/mapping-mind-language-model
Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet,https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html
2405.14782 - Lessons from the Trenches on Reproducible Evaluation of Language Models,https://arxiv.org/abs/2405.14782
2405.14860 - Not All Language Model Features Are Linear,https://arxiv.org/abs/2405.14860
2405.14205 - Agent Planning with World Knowledge Model,https://arxiv.org/abs/2405.14205
Improving Prompt Consistency with Structured Generations,https://huggingface.co/blog/evaluation-structured-outputs
OverflowAI and the holy grail of search - Stack Overflow,https://stackoverflow.blog/2024/05/17/overflowai-and-the-holy-grail-of-search/
"How to Optimize Chunk Size for RAG in Production? | by Mandar Karhade, MD. PhD. | May, 2024 | Towards AI",https://archive.is/6PZv2
Building Generative AI prompt chaining workflows with human in the loop | AWS Machine Learning Blog,https://aws.amazon.com/blogs/machine-learning/building-generative-ai-prompt-chaining-workflows-with-human-in-the-loop/
2405.10938 - Observational Scaling Laws and the Predictability of Language Model Performance,https://arxiv.org/abs/2405.10938
2405.11582 - SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization,https://arxiv.org/abs/2405.11582
2405.10637 - Layer-Condensed KV Cache for Efficient Inference of Large Language Models,https://arxiv.org/abs/2405.10637
2405.11157 - Towards Modular LLMs by Building and Reusing a Library of LoRAs,https://arxiv.org/abs/2405.11157
2405.12130 - MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning,https://arxiv.org/abs/2405.12130
2405.11273 - Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts,https://arxiv.org/abs/2405.11273
2405.09818 - Chameleon: Mixed-Modal Early-Fusion Foundation Models,https://arxiv.org/abs/2405.09818
"New, harder MMLU-style benchmark",https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro#4-leaderboard
2405.07987 - The Platonic Representation Hypothesis,https://arxiv.org/abs/2405.07987
Thoughts on prompting from Andrew Ng,https://www.deeplearning.ai/the-batch/issue-249/
Claude3,https://twitter.com/AmandaAskell/status/1765207842993434880
2311.16452 - Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine,https://arxiv.org/abs/2311.16452
Viking 7B: The first open LLM for the Nordic languages,https://www.silo.ai/blog/viking-7b-the-first-open-llm-for-the-nordic-languages
2405.09798 - Many-Shot In-Context Learning in Multimodal Foundation Models,https://arxiv.org/abs/2405.09798
2405.09673 - LoRA Learns Less and Forgets Less,https://arxiv.org/abs/2405.09673
knn-router | GitHub,https://github.com/pulzeai-oss/knn-router/blob/main/README.md
2405.04776 - Chain of Thoughtlessness: An Analysis of CoT in Planning,https://arxiv.org/abs/2405.04776
2405.05254 - You Only Cache Once: Decoder-Decoder Architectures for Language Models,https://arxiv.org/abs/2405.05254
2405.00823 - WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting,https://arxiv.org/abs/2405.00823
2403.04327 - ProMoAI: Process Modeling with Generative AI,https://arxiv.org/abs/2403.04327
2405.03207 - A Philosophical Introduction to Language Models - Part II: The Way Forward,https://arxiv.org/abs/2405.03207
2404.19756 - KAN: Kolmogorov-Arnold Networks,https://arxiv.org/abs/2404.19756
Innovation through prompting - by Ethan Mollick,https://www.oneusefulthing.org/p/innovation-through-prompting
"Instructors as Innovators: a Future-focused Approach to New AI Learning Opportunities, With Prompt",https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4802463
2404.18824 - Benchmarking Benchmark Leakage in Large Language Models,https://arxiv.org/abs/2404.18824
2405.01535 - Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models,https://arxiv.org/abs/2405.01535
"hegelai/prompttools: Open-source tools for prompt testing and experimentation, with support for both LLMs (e.g. OpenAI, LLaMA) and vector databases (e.g. Chroma, Weaviate, LanceDB).",https://github.com/hegelai/prompttools
GPT-4o is consistently hallucinating | Hacker News,https://news.ycombinator.com/item?id=40542631
Ethan Mollick on LinkedIn: knowledge work is not just coding,https://www.linkedin.com/feed/update/urn:li:activity:7203874745480531968/
courses/ToolUse/01_tool_use_overview.ipynb at master ¬∑ anthropics/courses,https://github.com/anthropics/courses/blob/master/ToolUse/01_tool_use_overview.ipynb
I. From GPT-4 to AGI: Counting the OOMs - SITUATIONAL AWARENESS,https://situational-awareness.ai/from-gpt-4-to-agi/
Extracting Concepts from GPT-4 | OpenAI,https://openai.com/index/extracting-concepts-from-gpt-4/
sparse-autoencoders.pdf,https://cdn.openai.com/papers/sparse-autoencoders.pdf
openai/sparse_autoencoder,https://github.com/openai/sparse_autoencoder
SAE viewer,https://openaipublic.blob.core.windows.net/sparse-autoencoder/sae-viewer/index.html
2405.19479 - Participation in the age of foundation models,https://arxiv.org/abs/2405.19479
2405.20233 - Grokfast: Accelerated Grokking by Amplifying Slow Gradients,https://arxiv.org/abs/2405.20233
2406.03158 - CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs,https://arxiv.org/abs/2406.03158
NumPy 2.0 migration guide ‚Äî NumPy v2.1.dev0 Manual,https://numpy.org/devdocs/numpy_2_0_migration_guide.html
numpy2-deprecation (NPY201) - Ruff,https://docs.astral.sh/ruff/rules/numpy2-deprecation/
"Will Kurt on LinkedIn: Monday the CEO at .txt said ""It would be great if we could beat GPT-4 by‚Ä¶",https://www.linkedin.com/posts/will-kurt-2591a155_monday-the-ceo-at-txt-said-it-would-be-activity-7204613606854815744-ITWn
Getting started with prompty ‚Äî Prompt flow documentation,https://microsoft.github.io/promptflow/tutorials/prompty-quickstart.html
GitHub - eugeneyan/llm-paper-notes: Notes from the Latent Space paper club. Follow along or start your own!,https://github.com/eugeneyan/llm-paper-notes
"How a single ChatGPT mistake cost us $10,000+ | Asim",https://asim.bearblog.dev/how-a-single-chatgpt-mistake-cost-us-10000/
OpenAI and Apple announce partnership | OpenAI,https://openai.com/index/openai-and-apple-announce-partnership/
"Introducing Apple Intelligence for iPhone, iPad, and Mac - Apple",https://www.apple.com/newsroom/2024/06/introducing-apple-intelligence-for-iphone-ipad-and-mac/
Introducing Apple's On-Device and Server Foundation Models - Apple Machine Learning Research,https://machinelearning.apple.com/research/introducing-apple-foundation-models
2406.01297 - When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs,https://arxiv.org/abs/2406.01297
2406.04313 - Improving Alignment and Robustness with Short Circuiting,https://arxiv.org/abs/2406.04313
2406.01637 - Teams of LLM Agents can Exploit Zero-Day Vulnerabilities,https://arxiv.org/abs/2406.01637
2406.01252 - Towards Scalable Automated Alignment of LLMs: A Survey,https://arxiv.org/abs/2406.01252
2406.04744 - CRAG -- Comprehensive RAG Benchmark,https://arxiv.org/abs/2406.04744
2406.04520 - NATURAL PLAN: Benchmarking LLMs on Natural Language Planning,https://arxiv.org/abs/2406.04520
2406.04823 - BERTs are Generative In-Context Learners,https://arxiv.org/abs/2406.04823
Sober AI is the Norm | Drew Breunig,https://www.dbreunig.com/2024/06/12/sober-ai-is-the-norm.html
"The Challenges of Retrieving and Evaluating Relevant Context for RAG | by Leonie Monigatti | Jun, 2024 | Towards Data Science",https://archive.is/f5ctj
2406.06326 - Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching,https://arxiv.org/abs/2406.06326
2406.04692 - Mixture-of-Agents Enhances Large Language Model Capabilities,https://arxiv.org/abs/2406.04692
2406.06046 - MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models,https://arxiv.org/abs/2406.06046
Lamini-Memory-Tuning/research-paper.pdf at main ¬∑ lamini-ai/Lamini-Memory-Tuning,https://github.com/lamini-ai/Lamini-Memory-Tuning/blob/main/research-paper.pdf
"Introducing Lamini Memory Tuning: 95% LLM Accuracy, 10x Fewer Hallucinations | Lamini - Enterprise LLM Platform",https://www.lamini.ai/blog/lamini-memory-tuning
AI Search: The Bitter-er Lesson,https://yellow-apartment-148.notion.site/AI-Search-The-Bitter-er-Lesson-44c11acd27294f4495c3de778cd09c8d
"Erik Meijer on X: ""We are being robbed ... OpenAI has silently removed mention of GPT-4 32K (https://t.co/ZKb3lPVlcs) and AFAIK they never actually exposed it. Azure is deprecating GPT-4 32K in September (https://t.co/wPe0lMRTib) While GPT-4 Turbo, GPT-4o have 128 input context, they only have 4K"" / X",https://x.com/headinthebox/status/1802404482174820398
Every Way To Get Structured Output From LLMs,https://www.boundaryml.com/blog/structured-output-from-llms
2302.14545 - Modern Bayesian Experimental Design,https://arxiv.org/abs/2302.14545
2406.06485 - Can Language Models Serve as Text-Based World Simulators?,https://arxiv.org/abs/2406.06485
4M: Massively Multimodal Masked Modeling - Apple Machine Learning Research,https://machinelearning.apple.com/research/massively-multimodal
2406.07394 - Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B,https://arxiv.org/abs/2406.07394
"2406.10209 - Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs",https://arxiv.org/abs/2406.10209
Will We Run Out of Data to Train Large Language Models?,https://epochai.org/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data
2406.11741 - Transcendence: Generative Models Can Outperform The Experts That Train Them,https://arxiv.org/abs/2406.11741
I Will Fucking Piledrive You If You Mention AI Again ‚Äî Ludicity,https://ludic.mataroa.blog/blog/i-will-fucking-piledrive-you-if-you-mention-ai-again/
2406.11980 - Prompt Design Matters for Computational Social Science Tasks but in Unpredictable Ways,https://arxiv.org/abs/2406.11980
2406.12824 - From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries,https://arxiv.org/abs/2406.12824
2406.11687 - Tokenization Falling Short: The Curse of Tokenization,https://arxiv.org/abs/2406.11687
2406.11912 - AgileCoder: Dynamic Collaborative Agents for Software Development based on Agile Methodology,https://arxiv.org/abs/2406.11912
2406.11931 - DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence,https://arxiv.org/abs/2406.11931
[2404.06921] GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications,https://arxiv.org/abs/2404.06921
Berkeley Function Calling Leaderboard (aka Berkeley Tool Calling Leaderboard),https://gorilla.cs.berkeley.edu/leaderboard.html
2406.12034 - Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts,https://arxiv.org/abs/2406.12034
2406.11717 - Refusal in Language Models Is Mediated by a Single Direction,https://arxiv.org/abs/2406.11717
Not all 'open source' AI models are actually open: here's a ranking,https://www.nature.com/articles/d41586-024-02012-5
Why we no longer use LangChain for building our AI agents,https://www.octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents
Top 9 Libraries to Accelerate LLM Building - by Avi Chawla,https://www.blog.aiport.tech/p/top-9-libraries-to-accelerate-llm
"Short Musings on AI Engineering and ""Failed AI Projects""",https://www.sh-reya.com/blog/ai-engineering-short/
LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs,https://arxiv.org/abs/2406.15319
Evaluating RAG-Fusion with RAGElo: an Automated Elo-based Framework,https://arxiv.org/abs/2406.14783
Learning to Retrieve Iteratively for In-Context Learning,https://arxiv.org/abs/2406.14739
A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems,https://arxiv.org/abs/2406.14972
Researchers run high-performing large language model on the energy needed to power a lightbulb,https://news.ucsc.edu/2024/06/matmul-free-llm.html
"Open-LLM performances are plateauing, let's make the leaderboard steep again - a Hugging Face Space by open-llm-leaderboard",https://huggingface.co/spaces/open-llm-leaderboard/blog
"Google launches Gemma 2, its next generation of open models",https://blog.google/technology/developers/google-gemma-2/
Finding GPT-4's mistakes with GPT-4 | OpenAI,https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/
"Training a 70B model from scratch: open-source tools, evaluation datasets, and learnings - imbue",https://imbue.com/research/70b-intro/
From bare metal to a 70B model: infrastructure set-up and scripts - imbue,https://imbue.com/research/70b-infrastructure/
Open-sourcing CARBS: a cost-effective hyperparameter optimizer that helps scale small experiments to large language models - imbue,https://imbue.com/research/70b-carbs/
"Ensuring accurate model evaluations: open-sourced, cleaned datasets for models that reason and code - imbue",https://imbue.com/research/70b-evals/
"AI can beat real university students in exams, study suggests - BBC News",https://www.bbc.co.uk/news/articles/cqqqln0eg65o
2406.16203 - LLMs' Classification Performance is Overclaimed,https://arxiv.org/abs/2406.16203
2406.02528 - Scalable MatMul-free Language Modeling,https://arxiv.org/abs/2406.02528
"2406.16678 - Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation",https://arxiv.org/abs/2406.16678
"2406.18495 - WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs",https://arxiv.org/abs/2406.18495
2406.16235 - Preference Tuning For Toxicity Mitigation Generalizes Across Languages,https://arxiv.org/abs/2406.16235
2406.14848 - Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models,https://arxiv.org/abs/2406.14848
2406.13692 - Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation,https://arxiv.org/abs/2406.13692
2406.18676 - Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation,https://arxiv.org/abs/2406.18676
2406.19226 - Simulating Classroom Education with LLM-Empowered Agents,https://arxiv.org/abs/2406.19226
2406.14713 - Risk thresholds for frontier AI,https://arxiv.org/abs/2406.14713
2406.15927 - Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs,https://arxiv.org/abs/2406.15927
2406.16554 - LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training,https://arxiv.org/abs/2406.16554
"2406.13661 - Hitchhiker's guide on Energy-Based Models: a comprehensive review on the relation with other generative models, sampling and statistical physics",https://arxiv.org/abs/2406.13661
2406.16747 - Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers,https://arxiv.org/abs/2406.16747
Beating GPT-4 with Open Source,https://blog.dottxt.co/oss-v-gpt4.html
Introducing AutoGen Studio: A low-code interface for building multi-agent workflows - Microsoft Research,https://www.microsoft.com/en-us/research/blog/introducing-autogen-studio-a-low-code-interface-for-building-multi-agent-workflows/
NumPy releases 2.0,https://numpy.org/devdocs/release/2.0.0-notes.html
Turning the Tables on AI,https://ia.net/topics/turning-the-tables-on-ai
Nemotron-4 340B,https://research.nvidia.com/publication/2024-06_nemotron-4-340b
2024 Gartner Hype Cycle for AI,https://www.linkedin.com/posts/svetlana-sicular-415549_ai-genai-responsibleai-activity-7208519032365305857-NHR8/
2023 Gartner Hype Cycle,https://www.gartner.com/en/articles/what-s-new-in-artificial-intelligence-from-the-2023-gartner-hype-cycle
ask-fini/paramount: Agent accuracy measurements for LLMs,https://github.com/ask-fini/paramount
How to Use AI to Create Role-Play Scenarios for Your Students | Harvard Business Publishing Education,https://hbsp.harvard.edu/inspiring-minds/using-generative-ai-to-create-role-play-scenarios-for-students
2406.08391 - Large Language Models Must Be Taught to Know What They Don't Know,https://arxiv.org/abs/2406.08391
2406.04370 - Large Language Model Confidence Estimation via Black-Box Access,https://arxiv.org/abs/2406.04370
2406.06608 - The Prompt Report: A Systematic Survey of Prompting Techniques,https://arxiv.org/abs/2406.06608
2406.07933 - Large Language Model Unlearning via Embedding-Corrupted Prompts,https://arxiv.org/abs/2406.07933
2406.06443 - LLM Dataset Inference: Did you train on my dataset?,https://arxiv.org/abs/2406.06443
2406.06563 - Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models,https://arxiv.org/abs/2406.06563
2406.07887 - An Empirical Study of Mamba-based Language Models,https://arxiv.org/abs/2406.07887
2406.07522 - Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling,https://arxiv.org/abs/2406.07522
A Picture is Worth 170 Tokens: How Does GPT-4o Encode Images? - OranLooney.com,https://www.oranlooney.com/post/gpt-cnn/
.txt Engineering,https://blog.dottxt.co/
Let's reproduce GPT-2 (124M),https://www.youtube.com/watch?v=l8pRSuU81PU
prompty,https://github.com/microsoft/prompty
2406.00975 - Luna: An Evaluation Foundation Model to Catch Language Model Hallucinations with High Accuracy and Low Cost,https://arxiv.org/abs/2406.00975
How to train a Million Context LLM,https://www.latent.space/p/gradient
Gradient Blog,https://gradient.ai/blog
Computex,https://arxiv.org/abs/2405.21060
State Space Duality (Mamba-2) Part I - The Model | Tri Dao,https://tridao.me/blog/2024/mamba2-part1-model/
2405.20541 - Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models,https://arxiv.org/abs/2405.20541
2406.01574 - MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark,https://arxiv.org/abs/2406.01574
MMLU Pro leaderboard,https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro
2406.02061 - Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models,https://arxiv.org/abs/2406.02061
2405.20624 - Leveraging Large Language Models for Entity Matching,https://arxiv.org/abs/2405.20624
2405.20974 - SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales,https://arxiv.org/abs/2405.20974
2406.02543 - To Believe or Not to Believe Your LLM,https://arxiv.org/abs/2406.02543
"The SMART Principles: Designing Interfaces That LLMs Understand | by Howard Zhou | Jun, 2024 | Medium",https://medium.com/@zhihao.zhou.bupt/the-smart-principles-designing-interfaces-that-llms-understand-aca00630c8c9
"100k H100 Clusters: Power, Network Topology, Ethernet vs InfiniBand, Reliability, Failures, Checkpointing",https://www.semianalysis.com/p/100000-h100-clusters-power-network
2406.20086 - Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs,https://arxiv.org/abs/2406.20086
2406.20052 - Understanding and Mitigating Language Confusion in LLMs,https://arxiv.org/abs/2406.20052
2406.18665 - RouteLLM: Learning to Route LLMs with Preference Data,https://arxiv.org/abs/2406.18665
4M Demo - a Hugging Face Space by EPFL-VILAB,https://huggingface.co/spaces/EPFL-VILAB/4M
Apple Intelligence & Advanced RAG (Practical AI #275),https://changelog.com/practicalai/275
Llm as a judge for retrieval,https://blog.vespa.ai/improving-retrieval-with-llm-as-a-judge/
Stanford AI Index Report 2024,https://aiindex.stanford.edu/report/
"Release v4.42.0: Gemma 2, Tool-use and RAG model support ¬∑ huggingface/transformers",https://github.com/huggingface/transformers/releases/tag/v4.42.0
Chat Templates,https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling
Chat Templates,https://huggingface.co/docs/transformers/main/chat_templating#advanced-retrieval-augmented-generation
AI's $600B Question | Sequoia Capital,https://www.sequoiacap.com/article/ais-600b-question/
Google: AI Potentially Breaking Reality Is a Feature Not a Bug,https://www.404media.co/google-ai-potentially-breaking-reality-is-a-feature-not-a-bug/
"Artificial Intelligence: News, Business, Science | THE DECODER",https://the-decoder.com/
2407.01219 - Searching for Best Practices in Retrieval-Augmented Generation,https://arxiv.org/abs/2407.01219
Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion,https://boyuan.space/diffusion-forcing/
Meta drops AI bombshell: Multi-token prediction models now open for research | VentureBeat,https://venturebeat.com/ai/meta-drops-ai-bombshell-multi-token-prediction-models-now-open-for-research/
"For AI Giants, Smaller Is Sometimes Better | WSJ",https://archive.is/PJHXC
Understanding DSPy with RAG | Kamal Shrestha,https://shresthakamal.com.np/blog/2024/understanding-dspy/
An Extremely Opinionated Annotated List of My Favourite Mechanistic Interpretability Papers v2 ‚Äî AI Alignment Forum,https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite-1
AI Engineer,https://www.youtube.com/@aiDotEngineer/featured
"SylphAI-Inc/LightRAG: The ""PyTorch"" library for LLM applications.",https://github.com/SylphAI-Inc/LightRAG
2407.05563 - LLMBox: A Comprehensive Library for Large Language Models,https://arxiv.org/abs/2407.05563
2407.03234 - Self-Evaluation as a Defense Against Adversarial Attacks on LLMs,https://arxiv.org/abs/2407.03234
2407.03618 - BM25S: Orders of magnitude faster lexical search via eager sparse scoring,https://arxiv.org/abs/2407.03618
2406.17711 - Data curation via joint example selection further accelerates multimodal learning,https://arxiv.org/abs/2406.17711
2407.04153 - Mixture of A Million Experts,https://arxiv.org/abs/2407.04153
2407.05841 - An Empirical Comparison of Vocabulary Expansion and Initialization Approaches for Language Models,https://arxiv.org/abs/2407.05841
2407.05975 - LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages,https://arxiv.org/abs/2407.05975
"GraphRAG Analysis, Part 1: How Indexing Elevates Knowledge Graph Performance in RAG",https://aiencoder.substack.com/p/graphrag-analysis-part-1-how-indexing
"stephenleo/llm-structured-output-benchmarks: Benchmark various LLM Structured Output frameworks: Instructor, Mirascope, Langchain, LlamaIndex, Fructose, Marvin, Outlines, etc on tasks like multi-label classification, named entity recognition, synthetic data generation, etc.",https://github.com/stephenleo/llm-structured-output-benchmarks
Marie Stephen Leo on LinkedIn,https://www.linkedin.com/posts/marie-stephen-leo_generativeai-llm-nlp-activity-7215519900893683712-zVaa
FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision,https://www.together.ai/blog/flashattention-3
Reasoning skills of large language models are often overestimated | MIT News | Massachusetts Institute of Technology,https://news.mit.edu/2024/reasoning-skills-large-language-models-often-overestimated-0711
2407.08223 - Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting,https://arxiv.org/abs/2407.08223
2407.06071 - From Loops to Oops: Fallback Behaviors of Language Models Under Uncertainty,https://arxiv.org/abs/2407.06071
2407.04503 - When LLMs Play the Telephone Game: Cumulative Changes and Attractors in Iterated Cultural Transmissions,https://arxiv.org/abs/2407.04503
2407.07304 - Inference Performance Optimization for Large Language Models on CPUs,https://arxiv.org/abs/2407.07304
2407.08488 - Lynx: An Open Source Hallucination Evaluation Model,https://arxiv.org/abs/2407.08488
Extrinsic Hallucinations in LLMs | Lil'Log,https://lilianweng.github.io/posts/2024-07-07-hallucination/
Benchmarks 201: Why Leaderboards > Arenas >> LLM-as-Judge,https://www.latent.space/p/benchmarks-201
Philipp Schmid on LinkedIn: How can we dynamically improve LLM applications using production data?‚Ä¶ | 10 comments,https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_how-can-we-dynamically-improve-llm-applications-activity-7218500791748231168-PNwv
2407.07972 - Deconstructing What Makes a Good Optimizer for Language Models,https://arxiv.org/abs/2407.07972
2407.06866 - ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context,https://arxiv.org/abs/2407.06866
2407.07612 - Teaching Transformers Causal Reasoning through Axiomatic Training,https://arxiv.org/abs/2407.07612
(PDF) Understanding Transformers via N-Gram Statistics,https://www.researchgate.net/publication/382204056_Understanding_Transformers_via_N-Gram_Statistics
2407.09025 - SpreadsheetLLM: Encoding Spreadsheets for Large Language Models,https://arxiv.org/abs/2407.09025
"2407.10457 - The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism",https://arxiv.org/abs/2407.10457
2407.10671 - Qwen2 Technical Report,https://arxiv.org/abs/2407.10671
2407.10490 - Learning Dynamics of LLM Finetuning,https://arxiv.org/abs/2407.10490
Study finds AI leads to more sameness in creative writing,https://www.emergingtechbrew.com/stories/2024/07/16/science-advances-study-ai-sameness-creative-writing
"mutable.ai on X: ""Leapfrogging traditional vector-based RAG with language maps "" / X",https://x.com/mutableai/status/1813815706783490055
HN discussion,https://news.ycombinator.com/item?id=40998497
Prover-Verifier Games improve legibility of language model outputs | OpenAI,https://openai.com/index/prover-verifier-games-improve-legibility/
2407.12854 - Scaling Retrieval-Based Language Models with a Trillion-Token Datastore,https://arxiv.org/abs/2407.12854
2407.13739 - Scaling Granite Code Models to 128K Context,https://arxiv.org/abs/2407.13739
2406.07057 - Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study,https://arxiv.org/abs/2406.07057
2407.13481 - Attention Overflow: Language Model Input Blur during Long-Context Missing Items Recommendation,https://arxiv.org/abs/2407.13481
2407.13696 - Benchmark Agreement Testing Done Right: A Guide for LLM Benchmark Evaluation,https://arxiv.org/abs/2407.13696
"What happened to BERT & T5? On Transformer Encoders, PrefixLM and Denoising Objectives ‚Äî Yi Tay",https://www.yitay.net/blog/model-architecture-blogpost-encoders-prefixlm-denoising
New compliance and administrative tools for ChatGPT Enterprise | OpenAI,https://openai.com/index/new-tools-for-chatgpt-enterprise/
RAG For a Codebase with 10k Repos,https://www.codium.ai/blog/rag-for-large-scale-code-repos/
"DeepL's next-gen LLM outperforms ChatGPT-4, Google, and Microsoft for translation quality",https://www.deepl.com/en/blog/next-gen-language-model
"When ChatGPT summarises, it actually does nothing of the kind. ‚Äì R&A IT Strategy & Architecture",https://ea.rna.nl/2024/05/27/when-chatgpt-summarises-it-actually-does-nothing-of-the-kind/
relevant comment,https://news.ycombinator.com/item?id=41028335
A Visual Guide to Quantization - by Maarten Grootendorst,https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization
romansky/dom-to-semantic-markdown: DOM to Semantic-Markdown for use in LLMs,https://github.com/romansky/dom-to-semantic-markdown
2407.15017 - Knowledge Mechanisms in Large Language Models: A Survey and Perspective,https://arxiv.org/abs/2407.15017
Study Finds 77% of Employees Report AI Has Increased Their Workload,https://www.forbes.com/sites/bryanrobinson/2024/07/23/employees-report-ai-increased-workload/
marimo | a next-generation Python notebook,https://marimo.io/
Introducing structured extraction with LLM | LlamaIndex posted on the topic,https://www.linkedin.com/posts/llamaindex_our-big-release-today-lets-you-get-structured-activity-7221961614432903170-VIzo
2407.16674 - KAN or MLP: A Fairer Comparison,https://arxiv.org/abs/2407.16674
2407.16318 - PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing,https://arxiv.org/abs/2407.16318
"2407.16216 - A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More",https://arxiv.org/abs/2407.16216
2407.14981 - Open Problems in Technical AI Governance,https://arxiv.org/abs/2407.14981
Multi-modal RAG: Chat with Docs containing Images,https://www.youtube.com/watch?v=Rg35oYuus-w
example notebook | Colab,https://colab.research.google.com/drive/1YkT3EHXjZ5LSPwTiR2D2kroYqqQCl7AT
Building A Generative AI Platform,https://huyenchip.com/2024/07/25/genai-platform.html
Non-Obvious Prompt Engineering Guide,https://www.techsistence.com/p/non-obvious-prompt-engineering-guide
Using Agents to Not Use Agents: How we built our Text-to-SQL Q&A system | Tech notes,https://idinsight.github.io/tech-blog/blog/aam_pseudo_agent/
2407.18961 - MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains,https://arxiv.org/abs/2407.18961
2407.19825 - Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost,https://arxiv.org/abs/2407.19825
Calculating the Cost of a Google Deepmind Paper - 152334H,https://152334h.github.io/blog/scaling-exponents/
Scaling Exponents Across Parameterizations and Optimizers,https://arxiv.org/abs/2407.05872
"An Open Course on LLMs, Led by Practitioners",https://hamel.dev/blog/posts/course/
2407.18553 - REAPER: Reasoning based Retrieval Planning for Complex RAG Systems,https://arxiv.org/abs/2407.18553
2407.17686 - Transformers on Markov Data: Constant Depth Suffices,https://arxiv.org/abs/2407.17686
Introducing Llama 3.1: Our most capable models to date,https://ai.meta.com/blog/meta-llama-3-1/
The Llama 3 Herd of Models | Technical Paper,https://ai.meta.com/research/publications/the-llama-3-herd-of-models/
Large Enough | Mistral AI | Frontier AI in your hands,https://mistral.ai/news/mistral-large-2407/
SearchGPT is a prototype of new AI search features | OpenAI,https://openai.com/index/searchgpt-prototype/
Introducing Rerank 3 Nimble: Faster Reranking for Enterprise Search & Retrieval-Augmented Generation (RAG) Systems,https://cohere.com/blog/rerank-3-nimble
Llamaindex adds structured data extraction functionality,https://docs.llamaindex.ai/en/latest/module_guides/querying/structured_outputs/
2407.16741 - OpenDevin: An Open Platform for AI Software Developers as Generalist Agents,https://arxiv.org/abs/2407.16741
2407.18003 - Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption,https://arxiv.org/abs/2407.18003
2407.16607 - Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?,https://arxiv.org/abs/2407.16607
2407.17535 - LAMBDA: A Large Model Based Data Agent,https://arxiv.org/abs/2407.17535
"Data for A.I. Training Is Disappearing Fast, Study Shows",https://www.nytimes.com/2024/07/19/technology/ai-data-restrictions.html
2406.11794 - DataComp-LM: In search of the next generation of training sets for language models,https://arxiv.org/abs/2406.11794
"2407.13833 - Phi-3 Safety Post-Training: Aligning Language Models with a ""Break-Fix"" Cycle",https://arxiv.org/abs/2407.13833
2407.14057 - LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference,https://arxiv.org/abs/2407.14057
2407.14507 - Internal Consistency and Self-Feedback in Large Language Models: A Survey,https://arxiv.org/abs/2407.14507
GPT-4o mini: advancing cost-efficient intelligence | OpenAI,https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/
Codestral Mamba | Mistral AI | Frontier AI in your hands,https://mistral.ai/news/codestral-mamba/
Mistral NeMo | Mistral AI | Frontier AI in your hands,https://mistral.ai/news/mistral-nemo/
Navigating the LLM Landscape: Uber,https://www.uber.com/blog/genai-gateway/
Claude 3.5 Sonnet system prompt,https://archive.is/N84yT
"vector search]" ‚Äì this work was done specifically for codebase assistance, but I wonder if this would also be useful for our knowledgebase - [Building LLM Applications With Vector Databases",https://neptune.ai/blog/building-llm-applications-with-vector-databases
2407.10718 - Sibyl: Simple yet Effective Agent Framework for Complex Real-world Reasoning,https://arxiv.org/abs/2407.10718
2407.11963 - NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?,https://arxiv.org/abs/2407.11963
2407.10817 - Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation,https://arxiv.org/abs/2407.10817
Prometheus,https://prometheus-eval.github.io/prometheus/
2407.12580 - E5-V: Universal Embeddings with Multimodal Large Language Models,https://arxiv.org/abs/2407.12580
2407.12043 - The Art of Saying No: Contextual Noncompliance in Language Models,https://arxiv.org/abs/2407.12043
"Honey, I shrunk the LLM! A beginner's guide to quantization",https://www.theregister.com/2024/07/14/quantization_llm_feature/
Data Flywheels for LLM Applications,https://www.sh-reya.com/blog/ai-engineering-flywheel/
Can ChatGPT do data science? - Austin Z. Henley,https://austinhenley.com/blog/chatgptdatascience.html
2407.09394 - PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents,https://arxiv.org/abs/2407.09394
2407.09252 - Context Embeddings for Efficient Answer Generation in RAG,https://arxiv.org/abs/2407.09252
2407.08892 - Characterizing Prompt Compression Methods for Long Context Inference,https://arxiv.org/abs/2407.08892
2407.06460 - MUSE: Machine Unlearning Six-Way Evaluation for Language Models,https://arxiv.org/abs/2407.06460
APIGen Pipeline,https://apigen-pipeline.github.io/
2407.02490 - MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention,https://arxiv.org/abs/2407.02490
How Chunk Sizes Affect Semantic Retrieval Results | by Lam Hoang | Artificial Intelligence in Plain English,https://ai.plainenglish.io/investigating-chunk-size-on-semantic-results-b465867d8ca1
AI.Engineer put on a,https://www.youtube.com/@aiDotEngineer/streams
schedule,https://www.ai.engineer/worldsfair/2024/schedule
State of AI report 1H 2024 | Retool Blog | Cache,https://retool.com/blog/state-of-ai-h1-2024
Ragatouille - Learn RAG with Langchain,https://www.sakunaharinda.xyz/ragatouille-book/intro.html
GraphRAG: New tool for complex data discovery now on GitHub - Microsoft Research,https://www.microsoft.com/en-us/research/blog/graphrag-new-tool-for-complex-data-discovery-now-on-github/
Apple's 4M Models - a EPFL-VILAB Collection | Huggingface,https://huggingface.co/collections/EPFL-VILAB/4m-models-660193abe3faf4b4d98a2742
"French AI lab Kyutai unveils Moshi, natural conversational AI similar to GPT-4o's demo",https://the-decoder.com/french-ai-lab-kyutai-unveils-conversational-ai-assistant-moshi-plans-open-source-release/
ChatGPT just (accidentally) shared all of its secret rules | TechRadar,https://www.techradar.com/computing/artificial-intelligence/chatgpt-just-accidentally-shared-all-of-its-secret-rules-heres-what-we-learned
2407.01370 - Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems,https://arxiv.org/abs/2407.01370
2406.19223 - T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings,https://arxiv.org/abs/2406.19223
2406.18518 - APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets,https://arxiv.org/abs/2406.18518
2407.01489 - Agentless: Demystifying LLM-based Software Engineering Agents,https://arxiv.org/abs/2407.01489
paper,https://arxiv.org/abs/2406.13843
Alex Strick van Linschoten - My finetuned models beat OpenAI,https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation.html
HN Discussion,https://news.ycombinator.com/item?id=40843848
Related posts,https://mlops.systems/#category=isafpr
Extracting Fine-Grained Events and Sentiment from Economic News,https://www.researchgate.net/publication/356873749_Extracting_Fine-Grained_Events_and_Sentiment_from_Economic_News
Structured information extraction from scientific text with large language models | Nature Communications,https://www.nature.com/articles/s41467-024-45563-x
Why Your Generative AI Projects Are Failing,https://gradientflow.substack.com/p/why-your-generative-ai-projects-are
Polars Announces v1.0,https://pola.rs/posts/announcing-polars-1/
2407.01502 - AI Agents That Matter,https://arxiv.org/abs/2407.01502
"2406.20094 - Scaling Synthetic Data Creation with 1,000,000,000 Personas",https://arxiv.org/abs/2406.20094
"read|write|like|dislike|...] the text?" or personas interpersonally-related to those generated from text ("a child of, a peer to, ‚Ä¶"). Then, they use the personas to synthesize datasets. - [2406.19502 - Investigating How Large Language Models Leverage Internal Knowledge to Perform Complex Reasoning",https://arxiv.org/abs/2406.19502
2407.00106 - UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI,https://arxiv.org/abs/2407.00106
Kagi LLM Benchmarking Project | Kagi's Docs,https://help.kagi.com/kagi/ai/llm-benchmark.html
[2407.20516] Machine Unlearning in Generative AI: A Survey,https://arxiv.org/abs/2407.20516
[2407.17817] Demystifying Verbatim Memorization in Large Language Models,https://arxiv.org/abs/2407.17817
[2407.19594] Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge,https://arxiv.org/abs/2407.19594
[2408.00118] Gemma 2: Improving Open Language Models at a Practical Size,https://arxiv.org/abs/2408.00118
[2408.00714] SAM 2: Segment Anything in Images and Videos,https://arxiv.org/abs/2408.00714
[2408.00690] Improving Text Embeddings for Smaller Language Models Using Contrastive Fine-tuning,https://arxiv.org/abs/2408.00690
Nvidia's Blackwell Reworked - Shipment Delays & GB200A Reworked Platforms,https://www.semianalysis.com/p/nvidias-blackwell-reworked-shipment
"'You are a helpful mail assistant,' and other Apple Intelligence instructions - The Verge",https://www.theverge.com/2024/8/5/24213861/apple-intelligence-instructions-macos-15-1-sequoia-beta
[2407.21007] The Dual-Edged Sword of Technical Debt: Benefits and Issues Analyzed Through Developer Discussions,https://arxiv.org/abs/2407.21007
[2407.21075] Apple Intelligence Foundation Language Models,https://arxiv.org/abs/2407.21075
[2407.21770] MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts,https://arxiv.org/abs/2407.21770
[2408.01129v1] A Survey of Mamba,https://arxiv.org/abs/2408.01129
[2408.01031] POA: Pre-training Once for Models of All Sizes,https://arxiv.org/abs/2408.01031
[2408.01055v1] LLM as Runtime Error Handler: A Promising Pathway to Adaptive Self-Healing of Software Systems,https://arxiv.org/abs/2408.01055
"[2408.00103] ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget",https://arxiv.org/abs/2408.00103
[2408.01420v1] Mission Impossible: A Statistical Perspective on Jailbreaking LLMs,https://arxiv.org/abs/2408.01420
[2408.02085] Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models,https://arxiv.org/abs/2408.02085
[2408.00764] AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation,https://arxiv.org/abs/2408.00764
[2408.02622] Language Model Can Listen While Speaking,https://arxiv.org/abs/2408.02622
Introducing Structured Outputs in the API | OpenAI,https://openai.com/index/introducing-structured-outputs-in-the-api/
[2408.02373] Operationalizing Contextual Integrity in Privacy-Conscious Assistants,https://arxiv.org/abs/2408.02373
[2408.01050] The Impact of Hyperparameters on Large Language Model Inference Performance: An Evaluation of vLLM and HuggingFace Pipelines,https://arxiv.org/abs/2408.01050
Open_Problems_in_Technical_AI_Governance.pdf,https://cdn.governance.ai/Open_Problems_in_Technical_AI_Governance.pdf
gemma-scope-report.pdf,https://storage.googleapis.com/gemma-scope/gemma-scope-report.pdf
Paper page - Transformer Explainer: Interactive Learning of Text-Generative Models,https://arxiv.org/abs/2408.04619
Explosion-Scratch/apple-intelligence-prompts: System prompts from Apple's new Apple Intelligence on MacOS Sequoia,https://github.com/Explosion-Scratch/apple-intelligence-prompts
Apple Intelligence System Prompts,https://news.ycombinator.com/item?id=41209416
Why I bet on DSPy | Isaac Miller's Blog,https://blog.isaacbmiller.com/posts/dspy
OpenAI Generates More Turmoil,https://spyglass.org/openai-non-non-profit/
[2408.04093] Tree Attention: Topology-aware Decoding for Long-Context Attention on GPU clusters,https://arxiv.org/abs/2408.04093
Chain of Thought (CoT) - Outlines,https://dottxt-ai.github.io/outlines/latest/cookbook/chain_of_thought/
Jina.ai's regex for text segmentation | Han Xiao posted on the topic,https://www.linkedin.com/posts/hxiao87_based-%3F%3F%3F%3F%3F%3F%3F%3F-%3F%3F%3F%3F%3F%3F%3F%3F-activity-7230113200833253376-Y0zk
"CoMM: Collaborative multi-agent, multi-reasoning-path prompting for complex problem solving - Amazon Science",https://www.amazon.science/publications/comm-collaborative-multi-agent-multi-reasoning-path-prompting-for-complex-problem-solving
There's An AI: The Best AI Tools Directory,https://theresanai.com/
"Why Your RAG Doesn't Work. RAG is still promising, but today it's‚Ä¶ | by Christian Griset | Medium",https://medium.com/@cdg2718/why-your-rag-doesnt-work-9755726dd1e9
"Why Your RAG Doesn't Work. RAG is still promising, but today it's‚Ä¶ | by Christian Griset | Mar, 2024 | Medium",https://archive.is/8fC1O
[2407.14679] Compact Language Models via Pruning and Knowledge Distillation,https://arxiv.org/abs/2407.14679
[2408.07852] Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability,https://arxiv.org/abs/2408.07852
[2408.08210] Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models,https://arxiv.org/abs/2408.08210
[2408.08067] RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation,https://arxiv.org/abs/2408.08067
[2405.14486] RefChecker: Reference-based Fine-grained Hallucination Checker and Benchmark for Large Language Models,https://arxiv.org/abs/2405.14486
Phi-3CookBook/md/01.Introduce/Phi3Family.md at c53fa9fda5df6a42476dd8ba5f1ccb446dd1608c ¬∑ microsoft/Phi-3CookBook,https://github.com/microsoft/Phi-3CookBook/blob/c53fa9fda5df6a42476dd8ba5f1ccb446dd1608c/md/01.Introduce/Phi3Family.md
[2408.11796] LLM Pruning and Distillation in Practice: The Minitron Approach,https://arxiv.org/abs/2408.11796
[2408.11745] FocusLLM: Scaling LLM's Context by Parallel Decoding,https://arxiv.org/abs/2408.11745
System Prompts,https://docs.anthropic.com/en/release-notes/system-prompts#july-12th-2024
NirDiamant/RAG_Techniques: This repository showcases various advanced techniques for Retrieval-Augmented Generation (RAG) systems. RAG systems combine information retrieval with generative models to provide accurate and contextually rich responses.,https://github.com/NirDiamant/RAG_Techniques
"Integration with Semantic Kernel, Guidance and Prompt Flow ¬∑ Issue #140 ¬∑ microsoft/autogen",https://github.com/microsoft/autogen/issues/140
"[2408.10548] Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution",https://arxiv.org/abs/2408.10548
This AI Paper by National University of Singapore Introduces A Comprehensive Survey of Language Models for Tabular Data Analysis - MarkTechPost,https://www.marktechpost.com/2024/08/23/this-ai-paper-by-national-university-of-singapore-introduces-a-comprehensive-survey-of-language-models-for-tabular-data-analysis/
[2408.08379] Towards Realistic Synthetic User-Generated Content: A Scaffolding Approach to Generating Online Discussions,https://arxiv.org/abs/2408.08379
[2408.13247] Data Exposure from LLM Apps: An In-depth Investigation of OpenAI's GPTs,https://arxiv.org/abs/2408.13247
[2408.13233] Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time,https://arxiv.org/abs/2408.13233
[2408.14340] Foundation Models for Music: A Survey,https://arxiv.org/abs/2408.14340
[2408.11727] Efficient Detection of Toxic Prompts in Large Language Models,https://arxiv.org/abs/2408.11727
[2408.13467] LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to Small-Scale Local LLMs,https://arxiv.org/abs/2408.13467
"Function calling: The solution for a flexible and efficient RAG system | by Lin Dane | Aug, 2024 | Level Up Coding",https://archive.is/55uUT
Cinnamon/kotaemon: An open-source RAG-based tool for chatting with your documents.,https://github.com/Cinnamon/kotaemon
courses/prompt_engineering_interactive_tutorial at master ¬∑ anthropics/courses,https://github.com/anthropics/courses/tree/master/prompt_engineering_interactive_tutorial
[2408.14805] Platypus: A Generalized Specialist Model for Reading Text in Various Forms,https://arxiv.org/abs/2408.14805
[2408.14906] Writing in the Margins: Better Inference Pattern for Long Context Retrieval,https://arxiv.org/abs/2408.14906
Judge dismisses majority of GitHub Copilot copyright claims,https://www.developer-tech.com/news/judge-dismisses-majority-github-copilot-copyright-claims/
Visualize your RAG Data,https://archive.is/it2T9
Embedding Cluster Summarization | Arize AI,https://docs.arize.com/arize/llm-evaluation-and-annotations/how-does-evaluation-work/troubleshoot-retrieval-with-vector-stores/embedding-cluster-summarization
Retrieval Evaluations | Arize AI,https://docs.arize.com/arize/llm-evaluation-and-annotations/how-does-evaluation-work/troubleshoot-retrieval-with-vector-stores
Startup Contextual AI Uplevels Retrieval-Augmented Generation for Enterprises | NVIDIA Blog,https://blogs.nvidia.com/blog/contextual-ai-retrieval-augmented-generation/
[2408.14717] Text2SQL is Not Enough: Unifying AI and Databases with TAG,https://arxiv.org/abs/2408.14717
[2408.15237] The Mamba in the Llama: Distilling and Accelerating Hybrid Models,https://arxiv.org/abs/2408.15237
[2408.15915] Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models,https://arxiv.org/abs/2408.15915
[2408.15836] Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature,https://arxiv.org/abs/2408.15836
[2408.15204] Can Unconfident LLM Annotations Be Used for Confident Conclusions?,https://arxiv.org/abs/2408.15204
"[2408.16737] Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling",https://arxiv.org/abs/2408.16737
[2408.15232] Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations,https://arxiv.org/abs/2408.15232
Evaluating the Effectiveness of LLM-Evaluators (aka LLM-as-Judge),https://eugeneyan.com/writing/llm-evaluators/
Microsoft releases Phi-3.5 SLMs,https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/4225280
"[2408.10914] To Code, or Not To Code? Exploring Impact of Code in Pre-training",https://arxiv.org/abs/2408.10914
[2408.12599] Controllable Text Generation for Large Language Models: A Survey,https://arxiv.org/abs/2408.12599
[2408.11857] Hermes 3 Technical Report,https://arxiv.org/abs/2408.11857
[2408.12076] ConflictBank: A Benchmark for Evaluating the Influence of Knowledge Conflicts in LLM,https://arxiv.org/abs/2408.12076
[2408.11061] StructuredRAG: JSON Response Formatting with Large Language Models,https://arxiv.org/abs/2408.11061
Introducing Semantic Capability in LinkedIn's Content Search Engine,https://www.linkedin.com/blog/engineering/search/introducing-semantic-capability-in-linkedins-content-search-engine
Prompt Caching (beta) - Anthropic,https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching
[2408.08656] LLMs Are Biased Towards Output Formats! Systematically Evaluating and Mitigating Output Format Bias of LLMs,https://arxiv.org/abs/2408.08656
[2408.08435] Automated Design of Agentic Systems,https://arxiv.org/abs/2408.08435
"Tool Use, Unified",https://huggingface.co/blog/unified-tool-use
Chain of Thought (CoT) using Structured Generation,https://outlines-dev.github.io/outlines/cookbook/chain_of_thought/
"LLM Observability: Fundamentals, Practices, and Tools",https://neptune.ai/blog/llm-observability
Cut the Bull,https://vectara.com/blog/cut-the-bull-detecting-hallucinations-in-large-language-models/
Hughes Hallucination Evaluation Model 2.1 - Vectara,https://vectara.com/blog/hhem-2-1-a-better-hallucination-detection-model/
whitepapers,https://dev.writer.com/home/research
"[2408.03506] 1.5-Pints Technical Report: Pretraining in Days, Not Months -- Your Language Model Thrives on Quality Data",https://arxiv.org/abs/2408.03506
[2311.16119] Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition,https://arxiv.org/abs/2311.16119
"[2408.07055] LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs",https://arxiv.org/abs/2408.07055
Controlled automatic task-specific synthetic data generation for hallucination detection - Amazon Science,https://www.amazon.science/publications/controlled-automatic-task-specific-synthetic-data-generation-for-hallucination-detection
[2408.08274] BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts,https://arxiv.org/abs/2408.08274
[2408.08152] DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search,https://arxiv.org/abs/2408.08152
Advanced RAG Techniques | Weaviate - Vector Database,https://weaviate.io/blog/advanced-rag
"[2408.04682] ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities",https://arxiv.org/abs/2408.04682
[2408.04948] HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction,https://arxiv.org/abs/2408.04948
[2408.06195] Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers,https://arxiv.org/abs/2408.06195
OpenAI: Introducing Structured Outputs in the API | Simon Willison,https://simonwillison.net/2024/Aug/6/openai-structured-outputs/
A deep dive into OpenAI,https://sophiabits.com/blog/openai-structured-outputs-deep-dive
Transformer Explainer,https://poloclub.github.io/transformer-explainer/
[2408.02442] Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models,https://arxiv.org/abs/2408.02442
[2408.03281] StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation,https://arxiv.org/abs/2408.03281
[2408.03811v1] Generative Language Models with Retrieval Augmented Generation for Automated Short Answer Scoring,https://arxiv.org/abs/2408.03811
Gemma Scope: helping the safety community shed light on the inner workings of language models - Google DeepMind,https://deepmind.google/discover/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/
Prompt Design at Character.AI,https://research.character.ai/prompt-design-at-character-ai/
prompt-poet,https://www.rungalileo.io/hallucinationindex
moshi,https://www.moshi.chat/?queue_id=talktomoshi
LLM University,https://cohere.com/llmu
Study Finds Consumers Are Actively Turned Off by Products That Use AI,https://futurism.com/the-byte/study-consumers-turned-off-products-ai
How Does OpenAI Survive?,https://www.wheresyoured.at/to-serve-altman/
"Introducing torchchat: Accelerating Local LLM Inference on Laptop, Desktop and Mobile | PyTorch",https://pytorch.org/blog/torchchat-local-llm-inference/
Optimization Algorithms - Gradient Free Optimizers,https://simonblanke.github.io/gradient-free-optimizers-documentation/1.5/optimizers/
Broccoli AI at its best,https://changelog.com/practicalai/280
Argilla | The tool where experts improve AI models,https://argilla.io/
[2407.19813] Improving Retrieval Augmented Language Model with Self-Reasoning,https://arxiv.org/abs/2407.19813
[2407.21712] Adaptive Retrieval-Augmented Generation for Conversational Systems,https://arxiv.org/abs/2407.21712
[2308.03958] Simple synthetic data reduces sycophancy in large language models,https://arxiv.org/abs/2308.03958
[2407.21530] Data Contamination Report from the 2024 CONDA Shared Task,https://arxiv.org/abs/2407.21530
"GraphRAG Analysis, Part 2: Graph Creation and Retrieval vs Vector Database Retrieval - Blog | MLOps Community",https://home.mlops.community/public/blogs/graphrag-analysis-part-2-graph-creation-and-retrieval-vs-vector-database-retrieval
Optimizing RAG with Azure AI and Arize Phoenix,https://farzzy.hashnode.dev/rag-observability-and-evaluation-with-azure-ai-search-azure-openai-llamaindex-and-arize-phoenix
Cohere Updates the Command R Series,https://cohere.com/blog/command-series-0824
"AI worse than humans at summarising information, trial finds",https://www.crikey.com.au/2024/09/03/ai-worse-summarising-information-humans-government-trial/
Nicholas Carlini,https://nicholas.carlini.com/writing
My benchmark for large language models,https://nicholas.carlini.com/writing/2024/my-benchmark-for-large-language-models.html
"How I Use ""AI""",https://nicholas.carlini.com/writing/2024/how-i-use-ai.html
[2408.15998] Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders,https://arxiv.org/abs/2408.15998
[2408.17344] rerankers: A Lightweight Python Library to Unify Ranking Methods,https://arxiv.org/abs/2408.17344
"AnswerDotAI/rerankers: A lightweight, low-dependency, unified API to use all common reranking and cross-encoder models.",https://github.com/answerdotai/rerankers
Answer.AI - Practical AI R&D,https://www.answer.ai/
small colBERT embedding model,https://www.answer.ai/posts/2024-08-13-small-but-mighty-colbert.html
appear to be working on updating BERT with modern techniques,https://github.com/AnswerDotAI/bert24
"[2408.16725] Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming",https://arxiv.org/abs/2408.16725
mem0ai/mem0: The memory layer for Personalized AI,https://github.com/mem0ai/mem0
Thread by @mattshumer_ on Thread Reader App ‚Äì Thread Reader App,https://threadreaderapp.com/thread/1831767014341538166.html
mattshumer/Reflection-70B ¬∑ Hugging Face,https://huggingface.co/mattshumer/Reflection-70B
[2409.00509] LongRecipe: Recipe for Efficient Long Context Generalization in Large Language Models,https://arxiv.org/abs/2409.00509
[2409.01666] In Defense of RAG in the Era of Long-Context Language Models,https://arxiv.org/abs/2409.01666
[2409.00729] ContextCite: Attributing Model Generation to Context,https://arxiv.org/abs/2409.00729
MadryLab/context-cite,https://github.com/MadryLab/context-cite
[2409.01357] Know When to Fuse: Investigating Non-English Hybrid Retrieval in the Legal Domain,https://arxiv.org/abs/2409.01357
[2409.00608] TinyAgent: Function Calling at the Edge,https://arxiv.org/abs/2409.00608
"The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers by Zheyuan (Kevin) Cui, Mert Demirer, Sonia Jaffe, Leon Musolff, Sida Peng, Tobias Salz :: SSRN",https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4945566
[2405.14831] HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models,https://arxiv.org/abs/2405.14831
Engineer's guide to convincing your Product Manager to prioritize technical debt,https://newsletter.eng-leadership.com/p/engineers-guide-to-convincing-your
claude-prompt-generator/src/metaprompt.txt at main ¬∑ aws-samples/claude-prompt-generator,https://github.com/aws-samples/claude-prompt-generator/blob/main/src/metaprompt.txt
Gabriel Mart√≠n Bl√°zquez shares example data similar to that used for for Reflection 70B,https://www.linkedin.com/posts/gabrielmbmb_yesterday-reflection-70b-was-released-a-activity-7237844854733500417-HiUh
Confirmed: Reflection 70B's official API is a wrapper for Sonnet 3.5,https://news.ycombinator.com/item?id=41484981
How to evaluate performance of LLM Inference Frameworks | Lamini - Enterprise LLM Platform,https://www.lamini.ai/blog/evaluate-performance-llm-inference-frameworks
Bitten by Unicode ‚Äì pyATL,https://pyatl.dev/2024/09/01/bitten-by-unicode/
[2409.03384] Hardware Acceleration of LLMs: A comprehensive survey and comparison,https://arxiv.org/abs/2409.03384
[2409.03215] xLAM: A Family of Large Action Models to Empower AI Agent Systems,https://arxiv.org/abs/2409.03215
[2409.04185] Residual Stream Analysis with Multi-Layer SAEs,https://arxiv.org/abs/2409.04185
[2409.04318] Learning vs Retrieval: The Role of In-Context Examples in Regression with LLMs,https://arxiv.org/abs/2409.04318
[2409.04701] Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models,https://arxiv.org/abs/2409.04701
"Something New: On OpenAI's ""Strawberry"" and Reasoning",https://www.oneusefulthing.org/p/something-new-on-openais-strawberry
"OpenAI's new models ""instrumentally faked alignment""",https://www.transformernews.ai/p/openai-o1-alignment-faking
DataGemma-FullPaper,https://docs.datacommons.org/papers/DataGemma-FullPaper.pdf
DataGemma: AI open models connecting LLMs to Google's Data Commons,https://blog.google/technology/ai/google-datagemma-ai-llm/
Ask HN: What have you built with LLMs?,https://news.ycombinator.com/item?id=41507784
Reader-LM: Small Language Models for Cleaning and Converting HTML to Markdown,https://jina.ai/news/reader-lm-small-language-models-for-cleaning-and-converting-html-to-markdown/?nocache=1
Enforcing JSON Outputs in Commercial LLMs,https://datachain.ai/blog/enforcing-json-outputs-in-commercial-llms
GraphRAG auto-tuning provides rapid adaptation to new domains - Microsoft Research,https://www.microsoft.com/en-us/research/blog/graphrag-auto-tuning-provides-rapid-adaptation-to-new-domains/
Azure-Samples/graphrag-accelerator: One-click deploy of a Knowledge Graph powered RAG (GraphRAG) in Azure,https://github.com/Azure-Samples/graphrag-accelerator
[2306.03872] Deductive Verification of Chain-of-Thought Reasoning,https://arxiv.org/abs/2306.03872
[2409.01704v1] General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model,https://arxiv.org/abs/2409.01704
[2409.06666] LLaMA-Omni: Seamless Speech Interaction with Large Language Models,https://arxiv.org/abs/2409.06666
"[2409.06464] Operational Advice for Dense and Sparse Retrievers: HNSW, Flat, or Inverted Indexes?",https://arxiv.org/abs/2409.06464
Summary of what we have learned during AMA hour with the OpenAI o1 team today by @btibor91(Tibor Blaho),https://twitter-thread.com/t/1834686946846597281
bklieger-groq/g1: g1: Using Llama-3.1 70b on Groq to create o1-like reasoning chains,https://github.com/bklieger-groq/g1
"From API to AGI: Structured Outputs, OpenAI API platform and O1 Q&A ‚Äî with Michelle Pokrass & OpenAI Devrel + Strawberry team",https://www.latent.space/p/openai-api-and-o1
openai-python/chatml.md at v0.28.1 ¬∑ openai/openai-python,https://github.com/openai/openai-python/blob/v0.28.1/chatml.md
How to work with the Chat Markup Language (preview) - Azure OpenAI | Microsoft Learn,https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chat-markup-language
"[2409.05746] LLMs Will Always Hallucinate, and We Need to Live With This",https://arxiv.org/abs/2409.05746
[2409.08642] CPL: Critical Planning Step Learning Boosts LLM Generalization in Reasoning Tasks,https://arxiv.org/abs/2409.08642
Introducing Contextual Retrieval  Anthropic,https://www.anthropic.com/news/contextual-retrieval
HN Discussion,https://news.ycombinator.com/item?id=41598119
defn from Trieve API,https://docs.trieve.ai/api-reference/chunk/create-or-upsert-chunk-or-chunks
Introducing Contextual Retrieval,https://simonwillison.net/2024/Sep/20/introducing-contextual-retrieval/
Prompt caching with Claude,https://simonwillison.net/2024/Aug/14/prompt-caching-with-claude/
Fine-tuning LLMs to 1.58bit: extreme quantization made easy,https://huggingface.co/blog/1_58_llm_extreme_quantization
argilla/FinePersonas-v0.1 ¬∑ Datasets at Hugging Face,https://huggingface.co/datasets/argilla/FinePersonas-v0.1
Advanced RAG Techniques: an Illustrated Overview | by IVAN ILIN | Towards AI,https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6?gi=0d09cfcb91ac
[2409.10173] jina-embeddings-v3: Multilingual Embeddings With Task LoRA,https://arxiv.org/abs/2409.10173
[2409.10594] Kolmogorov-Arnold Transformer,https://arxiv.org/abs/2409.10594
[2409.11055] A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B,https://arxiv.org/abs/2409.11055
[2409.10568] On the limits of agency in agent-based models,https://arxiv.org/abs/2409.10568
[2409.11402] NVLM: Open Frontier-Class Multimodal LLMs,https://arxiv.org/abs/2409.11402
[2409.11136] Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models,https://arxiv.org/abs/2409.11136
[2409.11901] LLMs + Persona-Plug = Personalized LLMs,https://arxiv.org/abs/2409.11901
[2409.12181] A Controlled Study on Long Context Extension and Generalization in LLMs,https://arxiv.org/abs/2409.12181
Some Notes on Adversarial Attacks on LLMs - Cybernetist,https://cybernetist.com/2024/09/23/some-notes-on-adversarial-attacks-on-llms/
"[2409.14160] Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI",https://arxiv.org/abs/2409.14160
[2409.14924] Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely,https://arxiv.org/abs/2409.14924
"Updated production-ready Gemini models, reduced 1.5 Pro pricing, increased rate limits, and more - Google Developers Blog",https://developers.googleblog.com/en/updated-production-ready-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more/
OpenAI rolls out Advanced Voice Mode with more voices and a new look | TechCrunch,https://techcrunch.com/2024/09/24/openai-rolls-out-advanced-voice-mode-with-more-voices-and-a-new-look/
OpenAI to Become For-Profit Company - WSJ,https://www.wsj.com/tech/ai/openai-chief-technology-officer-resigns-7a8b4639
Intel launches Gaudi 3 accelerator for AI: Slower than H100 but also cheaper | Tom's Hardware,https://www.tomshardware.com/tech-industry/artificial-intelligence/intel-launches-gaudi-3-accelerator-for-ai-slower-than-h100-but-also-cheaper
molmo.allenai.org/blog,https://molmo.allenai.org/blog
[2409.16191] HelloBench: Evaluating Long Text Generation Capabilities of Large Language Models,https://arxiv.org/abs/2409.16191
[2409.15360] Reward-Robust RLHF in LLMs,https://arxiv.org/abs/2409.15360
[2409.15700] Making Text Embedders Few-Shot Learners,https://arxiv.org/abs/2409.15700
[2409.17146] Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models,https://arxiv.org/abs/2409.17146
[2409.15127] Boosting Healthcare LLMs Through Retrieved Context,https://arxiv.org/abs/2409.15127
[2409.17115] Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale,https://arxiv.org/abs/2409.17115
[2409.13373] LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench,https://arxiv.org/abs/2409.13373
[2409.15277] A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?,https://arxiv.org/abs/2409.15277
Galileo,https://www.rungalileo.io/
"Mira Murati, CTO",https://x.com/miramurati/status/1839025700009030027
"Barret Zoph, VP Research",https://x.com/barret_zoph/status/1839095143397515452
"Bob McGrew, Chief Research Officer",https://x.com/bobmcgrewai/status/1839099787423134051
Google shares Real-world gen AI use cases from industry leaders,https://blog.google/products/google-cloud/gen-ai-business-use-cases/
Enhance the reliability of your generative AI with new hallucination correction capability,https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/correction-capability-helps-revise-ungrounded-content-and/ba-p/4253281
"Microsoft claims its new tool can correct AI hallucinations, but experts advise caution | TechCrunch",https://techcrunch.com/2024/09/24/microsoft-claims-its-new-tool-can-correct-ai-hallucinations-but-experts-caution-it-has-shortcomings/
Advancing the Accuracy-Efficiency Frontier with Llama-3.1-Nemotron-51B | NVIDIA Technical Blog,https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/
"Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/
Connect 2024: The responsible approach we,https://ai.meta.com/blog/responsible-ai-connect-2024/
Summary of Ilya Sutskevers AI Reading List for John Carmack,https://tensorlabbet.com/2024/09/24/ai-reading-list/
"[2408.12748] SLM Meets LLM: Balancing Latency, Interpretability and Consistency in Hallucination Detection",https://arxiv.org/abs/2408.12748
How we use Lakera Guard to secure our LLMs - Dropbox,https://dropbox.tech/security/how-we-use-lakera-guard-to-secure-our-llms
Scaling: The State of Play in AI - by Ethan Mollick,https://www.oneusefulthing.org/p/scaling-the-state-of-play-in-ai
Google /code - Code Experiments,https://labs.google.com/code/
Table Extraction using LLMs: Unlocking Structured Data from Documents,https://nanonets.com/blog/table-extraction-using-llms-unlocking-structured-data-from-documents/
Customizing LLMs for Enterprise Data Using Domain Adaptation: The Wix Journey,https://www.wix.engineering/post/customizing-llms-for-enterprise-data-using-domain-adaptation-the-wix-journey
openai/MMMLU,https://huggingface.co/datasets/openai/MMMLU
[2409.13449] Minstrel: Structural Prompt Generation with Multi-Agents Coordination for Non-AI Experts,https://arxiv.org/abs/2409.13449
"[2409.12941] Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",https://arxiv.org/abs/2409.12941
[2409.13385] Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey,https://arxiv.org/abs/2409.13385
Eureka: Evaluating and understanding progress in AI - Microsoft Research,https://www.microsoft.com/en-us/research/blog/eureka-evaluating-and-understanding-progress-in-ai/
technical paper,https://www.microsoft.com/en-us/research/uploads/prod/2024/09/Eureka-Evaluating-and-Understanding-Large-Foundation-Models-Sept-13.pdf
Mistral adds free tier and deploys new,https://mistral.ai/news/september-24-release/
‚Äé,https://gemini.google.com/updates
Qwen2.5 family released,https://qwenlm.github.io/blog/qwen2.5/
[2401.02009] Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives,https://arxiv.org/abs/2401.02009
[2409.12917] Training Language Models to Self-Correct via Reinforcement Learning,https://arxiv.org/abs/2409.12917
[2409.12903] Scaling Smart: Accelerating Large Language Model Pre-training with Small Model Initialization,https://arxiv.org/abs/2409.12903
[2409.12136] GRIN: GRadient-INformed MoE,https://arxiv.org/abs/2409.12136
Moshi: a speech-text foundation model for real-time dialogue,https://kyutai.org/Moshi.pdf
"very $$$, high latency",https://news.ycombinator.com/item?id=41524110
"Philipp Schmid, Tech lead @ HuggingFace lists some papers about how o1 might work",https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_here-are-5-papers-you-want-to-read-to-understand-activity-7241017716214571008-eVba
Notes on OpenAI,https://simonwillison.net/2024/Sep/12/openai-o1/
Prompt engineering advice from Anthropic,https://www.linkedin.com/posts/cfregly_5-things-to-know-about-effective-prompt-engineering-activity-7241065050625466370-k3Bk
[2408.07199] Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents,https://arxiv.org/abs/2408.07199
[2305.20050] Let's Verify Step by Step,https://arxiv.org/abs/2305.20050
[2406.12050] Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning,https://arxiv.org/abs/2406.12050
OpenAI releases o1 (previously known as,https://openai.com/o1/
Really interesting graphic on tick-tock process between scaling and optimization | LinkedIn,https://www.linkedin.com/posts/peter-gostev_sometimes-i-hear-something-like-scaling-activity-7236487031629975552-kzWh
GitHub - huggingface/lighteval: LightEval is a lightweight LLM evaluation suite,https://github.com/huggingface/lighteval
GitHub - Mintplex-Labs/anything-llm: The all-in-one Desktop & Docker AI application with full RAG and AI Agent capabilities.,https://github.com/Mintplex-Labs/anything-llm
GitHub - deshraj/claude-memory: Claude Memory: Long-term memory for Claude,https://github.com/deshraj/claude-memory?tab=readme-ov-file
"GitHub - getzep/graphiti: Build and query dynamic, temporally-aware Knowledge Graphs",https://github.com/getzep/graphiti
How to Read Deep Learning Paper as a Software Engineer | YouTube,https://www.youtube.com/watch?v=nL7lAo95D-o
Phind introduces fine-tuned model based on Llama 3.1 405B,https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches
[2409.02326] Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining,https://arxiv.org/abs/2409.02326
"[2409.02850] Oops, I Sampled it Again: Reinterpreting Confidence Intervals in Few-Shot Learning",https://arxiv.org/abs/2409.02850
[2409.03512] From MOOC to MAIC: Reshaping Online Teaching and Learning through LLM-driven Agents,https://arxiv.org/abs/2409.03512
unclecode/crawl4ai: üî•üï∑Ô∏è Crawl4AI: Open-source LLM Friendly Web Crawler & Scrapper,https://github.com/unclecode/crawl4ai
PyTorch Native Architecture Optimization: torchao | PyTorch,https://pytorch.org/blog/pytorch-native-architecture-optimization/
Can AI Developers Be Held Liable for Negligence? - Slashdot,https://yro.slashdot.org/story/24/09/29/0122212/can-ai-developers-be-held-liable-for-negligence
"MIT spinoff Liquid debuts small, efficient non-transformer AI models | VentureBeat",https://venturebeat.com/ai/mit-spinoff-liquid-debuts-non-transformer-ai-models-and-theyre-already-state-of-the-art/
Liquid Foundation Models: Our First Series of Generative AI Models,https://www.liquid.ai/liquid-foundation-models#reimagining-model-architectures
[2409.14683] Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling,https://arxiv.org/abs/2409.14683
[2409.17422] Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction,https://arxiv.org/abs/2409.17422
[2409.02343] NUDGE: Lightweight Non-Parametric Fine-Tuning of Embeddings for Retrieval,https://arxiv.org/abs/2409.02343
[2409.17066] VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models,https://arxiv.org/abs/2409.17066
NeurIPS 2024 (Spotlight) - Text2CAD,https://sadilkhan.github.io/text2cad-project/
[2409.18869] Emu3: Next-Token Prediction is All You Need,https://arxiv.org/abs/2409.18869
[2409.17692] MIO: A Foundation Model on Multimodal Tokens,https://arxiv.org/abs/2409.17692
[2409.18839] MinerU: An Open-Source Solution for Precise Document Content Extraction,https://arxiv.org/abs/2409.18839
[2409.14586] Backtracking Improves Generation Safety,https://arxiv.org/abs/2409.14586
[2406.10279] We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs,https://arxiv.org/abs/2406.10279
"packages during code generation] is at least 5.2% for commercial models and 21.7% for open-source models"" - significant security risk akin to typosquatting (i.e, import `pytotch` instead of `pytorch`) - [[2409.20566] MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning",https://arxiv.org/abs/2409.20566
[2409.19606] Hyper-Connections,https://arxiv.org/abs/2409.19606
[2409.18747] Cottention: Linear Transformers With Cosine Attention,https://arxiv.org/abs/2409.18747
Linear Attention Fundamentals | Hailey Schoelkopf,https://haileyschoelkopf.github.io/blog/2024/linear-attn/
Prefix Linear Attention Can Outspeed Causal Linear Attention | Hailey Schoelkopf,https://haileyschoelkopf.github.io/blog/2024/prefix-lm/
Canvas is a new way to write and code with ChatGPT | OpenAI,https://openai.com/index/introducing-canvas/
"Lewis Tunstall on X: ""Ia nasty footgun with LoRA fine-tuning and chat templates that have special tokens: you must include the embedding layer and LM head in the trainable parameters! https://t.co/mhNO9KY3iq"" / X",https://x.com/_lewtun/status/1840804557800292843
Pytorch cofounder Soumith Chintala's posts are a goldmine,https://soumith.ch/blog.html
How to train a model on 10k H100 GPUs?,https://soumith.ch/blog/2024-10-02-training-10k-scale.md.html
Decisions and Pivots on PyTorch,https://soumith.ch/blog/2022-01-19-pytorch-retro.md.html
Shockingly good super-intelligent summarization prompt : r/LocalLLaMA,https://www.reddit.com/r/LocalLLaMA/comments/1ftjbz3/shockingly_good_superintelligent_summarization/
"Cameron R. Wolfe, Ph.D. on X: ""o1‚Äìand its ability to leverage increased inference time compute for better reasoning‚Äìunlocks new potential for automatic prompt engineering. https://t.co/rDuIP6iPpf"" / X",https://x.com/cwolferesearch/status/1841557739308286424
Doriandarko/o1-engineer: command-line tool designed to assist developers in managing and interacting with their projects efficiently,https://github.com/Doriandarko/o1-engineer
planning prompt,https://github.com/Doriandarko/o1-engineer/blob/4a535182af36d337ac534f11833a94039a802e60/o1-eng.py#L141
OpenAI and Anthropic Revenue Breakdown - by Tanay Jaipuria,https://www.tanayj.com/p/openai-and-anthropic-revenue-breakdown
Retrieval Optimization: From Tokenization to Vector Quantization - DeepLearning.AI,https://learn.deeplearning.ai/courses/retrieval-optimization-from-tokenization-to-vector-quantization/lesson/1/introduction
Update on Reflection-70B,https://glaive.ai/blog/post/reflection-postmortem
HN discussion,https://news.ycombinator.com/item?id=41735665
The Shape of AI | UX Patterns for Artificial Intelligence Design,https://www.shapeof.ai/
Cross Capabilities of LLMs,https://www.llm-cross-capabilities.org/
"[2409.20325] Old Optimizer, New Norm: An Anthology",https://arxiv.org/abs/2409.20325
[2410.00531] TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices,https://arxiv.org/abs/2410.00531
[2410.01215] From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging,https://arxiv.org/abs/2410.01215
[2410.01748] Not All LLM Reasoners Are Created Equal,https://arxiv.org/abs/2410.01748
[2410.02712] LLaVA-Critic: Learning to Evaluate Multimodal Models,https://arxiv.org/abs/2410.02712
[2410.02724] Large Language Models as Markov Chains,https://arxiv.org/abs/2410.02724
Transformers Inference Optimization Toolset | AstraBlog,https://astralord.github.io/posts/transformer-inference-optimization-toolset/
"Can we make any smaller opensource LLM models smarter than human? | by Harish SG | Oct, 2024 | Medium",https://medium.com/@harishhacker3010/can-we-make-any-smaller-opensource-ai-models-smarter-than-human-1ea507e644a0
"Philipp Schmid on LinkedIn: Combining Dynamic Chain of Thoughts, reflection, and verbal reinforcement, existing LLMs like Claude 3.5 Sonnet can be prompted to increase test-time compute and match reasoning strong models like OpenAI o1. | 35 comments",https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_can-anthropic-claude-35-sonnet-outperform-activity-7248611894905778177-md6w
"Ethan Mollick, analyst: 'Students who use AI as a crutch don't learn anything' | Technology | EL PA√çS English",https://english.elpais.com/technology/2024-10-03/ethan-mollick-analyst-students-who-use-ai-as-a-crutch-dont-learn-anything.html
AI is an impediment to learning web development,https://ben.page/jumbocode-ai
[2410.02525] Contextual Document Embeddings,https://arxiv.org/abs/2410.02525
[2410.00907] Addition is All You Need for Energy-efficient Language Models,https://arxiv.org/abs/2410.00907
[2410.02703] Selective Attention Improves Transformer,https://arxiv.org/abs/2410.02703
[2410.05258] Differential Transformer,https://arxiv.org/abs/2410.05258
Erasing Conceptual Knowledge from Language Models,https://elm.baulab.info/
[2410.02707] LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations,https://arxiv.org/abs/2410.02707
[2410.04739] TableRAG: Million-Token Table Understanding with Language Models,https://arxiv.org/abs/2410.04739
TypedDicts are better than you think,https://blog.changs.co.uk/typeddicts-are-better-than-you-think.html
The Ultimate Guide to Error Handling in Python - miguelgrinberg.com,https://blog.miguelgrinberg.com/post/the-ultimate-guide-to-error-handling-in-python
mingrammer/diagrams: :art: Diagram as Code for prototyping cloud system architectures,https://github.com/mingrammer/diagrams
"Humor | In jest, how can I mock a set interval being completed?",https://lemmy.world/pictrs/image/506285c3-6cd0-4c1a-b502-9109dda4c2d9.png
Evaluating Chunking Strategies for Retrieval | Chroma Research,https://research.trychroma.com/evaluating-chunking
Prompt generation - OpenAI API,https://platform.openai.com/docs/guides/prompt-generation
Examples - OpenAI API,https://platform.openai.com/docs/examples
Welcome to State of AI Report 2024,https://www.stateof.ai/
Americans are using AI at fairly high rates. What does this mean for the economy? : Planet Money : NPR,https://www.npr.org/sections/planet-money/2024/10/07/g-s1-26429/americans-using-ai-fairly-high-rates-what-does-this-mean-for-economy
Anecdotal ranking of frontier models | reddit,https://www.reddit.com/r/LocalLLaMA/comments/1fy711x/where_do_you_actually_rank_llama_32_405b_among/
Dylan Patel & Jon (Asianometry) ‚Äì How the Semiconductor Industry Actually Works,https://www.dwarkeshpatel.com/p/dylan-jon
huggingface/evaluation-guidebook,https://github.com/huggingface/evaluation-guidebook
argilla-io/distilabel releases v1.4,https://github.com/argilla-io/distilabel/releases
Chunkr | Open Source Data Ingestion,https://www.chunkr.ai/
lumina-ai-inc/chunkr: Vision model based PDF chunking.,https://github.com/lumina-ai-inc/chunkr
Microsoft.Extensions.AI: Simplifying AI Integration for .NET Partners | Semantic Kernel,https://devblogs.microsoft.com/semantic-kernel/microsoft-extensions-ai-simplifying-ai-integration-for-net-partners/
.NET,https://devblogs.microsoft.com/semantic-kernel/using-json-schema-for-structured-output-in-net-for-openai-models/
Python,https://devblogs.microsoft.com/semantic-kernel/using-json-schema-for-structured-output-in-python-for-openai-models/
Observability in Semantic Kernel | Semantic Kernel,https://devblogs.microsoft.com/semantic-kernel/observability-in-semantic-kernel/
Introducing enterprise multi-agent support in Semantic Kernel | Semantic Kernel,https://devblogs.microsoft.com/semantic-kernel/introducing-agents-in-semantic-kernel/
Micronaire is RAGAS for Semantic Kernel | Semantic Kernel,https://devblogs.microsoft.com/semantic-kernel/microsoft-hackathon-project-micronaire-using-semantic-kernel/
Blog | AutoGen,https://microsoft.github.io/autogen/0.2/blog
Examples | AutoGen,https://microsoft.github.io/autogen/0.2/docs/Examples
Develop a dag flow ‚Äî Prompt flow documentation,https://microsoft.github.io/promptflow/how-to-guides/develop-a-dag-flow/index.html
Launching Long-Term Memory Support in LangGraph,https://blog.langchain.dev/launching-long-term-memory-support-in-langgraph/
A Visual Guide to Mixture of Experts (MoE),https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts
[2410.03617] What Matters for Model Merging at Scale?,https://arxiv.org/abs/2410.03617
[2410.04707] Learning How Hard to Think: Input-Adaptive Allocation of LM Computation,https://arxiv.org/abs/2410.04707
[2410.04422] Hyper-multi-step: The Truth Behind Difficult Long-context Tasks,https://arxiv.org/abs/2410.04422
[2410.04199] LongGenBench: Long-context Generation Benchmark,https://arxiv.org/abs/2410.04199
[2410.04717] Only-IF:Revealing the Decisive Effect of Instruction Diversity on Generalization,https://arxiv.org/abs/2410.04717
[2410.07073] Pixtral 12B,https://arxiv.org/abs/2410.07073
[2410.05993] Aria: An Open Multimodal Native Mixture-of-Experts Model,https://arxiv.org/abs/2410.05993
[2410.05355] Falcon Mamba: The First Competitive Attention-free 7B Language Model,https://arxiv.org/abs/2410.05355
[2410.02465] Response Tuning: Aligning Large Language Models without Instruction,https://arxiv.org/abs/2410.02465
[2410.07137] Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates,https://arxiv.org/abs/2410.07137
[2410.07176] Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models,https://arxiv.org/abs/2410.07176
[2407.16833] Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach,https://arxiv.org/abs/2407.16833
[2410.07869] Benchmarking Agentic Workflow Generation,https://arxiv.org/abs/2410.07869
[2410.08115] Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System,https://arxiv.org/abs/2410.08115
[2410.08164] Agent S: An Open Agentic Framework that Uses Computers Like a Human,https://arxiv.org/abs/2410.08164
Agent S blog,https://www.simular.ai/agent-s
Large language models reduce public knowledge sharing on online Q&A platforms | PNAS Nexus | Oxford Academic,https://academic.oup.com/pnasnexus/article/3/9/pgae400/7754871?login=false
swarm,https://github.com/openai/swarm
OpenAI Cookbook,https://cookbook.openai.com/examples/orchestrating_agents
HN discussion,https://news.ycombinator.com/item?id=41815173
langroid/langroid: Harness LLMs with Multi-Agent Programming,https://github.com/langroid/langroid
High Dimensional Space,https://people.eecs.berkeley.edu/~jrs/highd/
Prompt Design,https://arvid.xyz/posts/prompt-design/
anysphere/priompt: Prompt design using JSX.,https://github.com/anysphere/priompt?tab=readme-ov-file
[R-263] Roadmap - v0.2 ¬∑ Issue #1009 ¬∑ explodinggradients/ragas,https://github.com/explodinggradients/ragas/issues/1009
Adobe Max 2024: All the major announcements around design and AI - The Verge,https://www.theverge.com/2024/10/14/24269859/adobe-max-2024-major-announcements-stream
LLMs-from-scratch/ch05/08_memory_efficient_weight_loading/memory-efficient-state-dict.ipynb at main ¬∑ rasbt/LLMs-from-scratch,https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/08_memory_efficient_weight_loading/memory-efficient-state-dict.ipynb
Signals and Threads | The Uncertain Art of Accelerating ML Models,https://signalsandthreads.com/the-uncertain-art-of-accelerating-ml-models/
LLMs don't do formal reasoning - and that is a HUGE problem,https://garymarcus.substack.com/p/llms-dont-do-formal-reasoning-and
[2410.05229] GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models,https://arxiv.org/abs/2410.05229
[2410.06634] Tree of Problems: Improving structured problem solving with compositionality,https://arxiv.org/abs/2410.06634
[2410.07985] Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models,https://arxiv.org/abs/2410.07985
[2410.09671] OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models,https://arxiv.org/abs/2410.09671
[2410.10630] Thinking LLMs: General Instruction Following with Thought Generation,https://arxiv.org/abs/2410.10630
[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory,https://arxiv.org/abs/2410.10813
[2410.04444] G√∂del Agent: A Self-Referential Agent Framework for Recursive Self-Improvement,https://arxiv.org/abs/2410.04444
[2410.10762] AFlow: Automating Agentic Workflow Generation,https://arxiv.org/abs/2410.10762
[2410.08801] A Methodology for Evaluating RAG Systems: A Case Study On Configuration Dependency Validation,https://arxiv.org/abs/2410.08801
[2410.08815] StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization,https://arxiv.org/abs/2410.08815
[2410.09584] Toward General Instruction-Following Alignment for Retrieval-Augmented Generation,https://arxiv.org/abs/2410.09584
[2407.01449] ColPali: Efficient Document Retrieval with Vision Language Models,https://arxiv.org/abs/2407.01449
"Un Ministral, des Ministraux | Mistral AI | Frontier AI in your hands",https://mistral.ai/news/ministraux/
Multimodal RAG: Contextual Retrieval with Azure AI Search and LlamaIndex,https://farzzy.hashnode.dev/contextual-retrieval-for-multimodal-rag-with-azure-ai-search-azure-openai-and-arize-phoenix-with-llamaindex
LlamaIndex + Nile: Build Multi-Tenant RAG Applications with Ease,https://www.thenile.dev/blog/nile_llamaindex
this GPU box,https://www.reddit.com/r/LocalLLaMA/comments/1g4w2vs/6u_threadripper_4xrtx4090_build/
[2410.04840] Strong Model Collapse,https://arxiv.org/abs/2410.04840
[2406.15786] What Matters in Transformers? Not All Attention is Needed,https://arxiv.org/abs/2406.15786
[2410.10814] Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free,https://arxiv.org/abs/2410.10814
[2410.13848] Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation,https://arxiv.org/abs/2410.13848
[2410.11163] Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence,https://arxiv.org/abs/2410.11163
[2410.11842] MoH: Multi-Head Attention as Mixture-of-Head Attention,https://arxiv.org/abs/2410.11842
W&B Weave,https://weave-docs.wandb.ai/
IBM releases Granite 3.0,https://www.ibm.com/new/ibm-granite-3-0-open-state-of-the-art-enterprise-models
"NuExtract 1.5 - Multilingual, Infinite context, still small, and better than GPT-4o! - NuMind",https://numind.ai/blog/nuextract-1-5---multilingual-infinite-context-still-small-and-better-than-gpt-4o
SouthBridgeAI/diagen,https://github.com/SouthBridgeAI/diagen
"Hrishi (@hrishioa): ""Released diagen yesterday, but how does it work? 1. Generate @terrastruct d2 diagrams with the model of your choice. Sonnet seems best, o1 seems needlessly expensive, gemini-flash is insane if you do a few rounds of visual reflection. What's visual reflection? üëá"" | nitter.poast.org",https://nitter.poast.org/hrishioa/status/1846941743364952258
Meta AI's hidden prompt : r/LocalLLaMA,https://www.reddit.com/r/LocalLLaMA/comments/1g5np9i/meta_ais_hidden_prompt/
Research Paper Report Generating Agent | run-llama/llamacloud-demo,https://github.com/run-llama/llamacloud-demo/blob/main/examples/report_generation/research_paper_report_generation.ipynb
"theJayTea/WritingTools: Apple Intelligence Writing Tools, for Windows",https://github.com/theJayTea/WritingTools
prompts,https://github.com/theJayTea/WritingTools/blob/6291634c8cfdb475d1aa971be01ca0cb406c2a5b/Windows/WritingToolApp.py#L230-L267
[2410.13883] Transformers Utilization in Chart Understanding: A Review of Recent Advances & Future Trends,https://arxiv.org/abs/2410.13883
NuExtract: A Foundation Model for Structured Extraction - NuMind,https://numind.ai/blog/nuextract-a-foundation-model-for-structured-extraction
[2410.12189] DocETL: Agentic Query Rewriting and Evaluation for Complex Document Processing,https://arxiv.org/abs/2410.12189
github/documentation,https://ucbepic.github.io/docetl/
[2410.14059] UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models,https://arxiv.org/abs/2410.14059
[2410.01131v1] nGPT: Normalized Transformer with Representation Learning on the Hypersphere,https://arxiv.org/abs/2410.01131
[2410.12784] JudgeBench: A Benchmark for Evaluating LLM-based Judges,https://arxiv.org/abs/2410.12784
[2410.10934] Agent-as-a-Judge: Evaluate Agents with Agents,https://arxiv.org/abs/2410.10934
[2410.16256] CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution,https://arxiv.org/abs/2410.16256
[2410.14677] Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts,https://arxiv.org/abs/2410.14677
"[2410.16144] 1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs",https://arxiv.org/abs/2410.16144
microsoft/BitNet: Official inference framework for 1-bit LLMs,https://github.com/microsoft/BitNet
"microsoft/VPTQ: VPTQ, A Flexible and Extreme low-bit quantization algorithm",https://github.com/microsoft/VPTQ
[2410.14940] Baichuan Alignment Technical Report,https://arxiv.org/abs/2410.14940
[2410.15735] AutoTrain: No-code training for state-of-the-art models,https://arxiv.org/abs/2410.15735
[2410.14745] SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation,https://arxiv.org/abs/2410.14745
[2410.12788] Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception,https://arxiv.org/abs/2410.12788
"[2408.13296] The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities",https://arxiv.org/abs/2408.13296
Introducing Multimodal Embed 3: Powering AI Search,https://cohere.com/blog/multimodal-embed-3
You Should Probably Pay Attention to Tokenizers - Cybernetist,https://cybernetist.com/2024/10/21/you-should-probably-pay-attention-to-tokenizers/
Google open-sourced its watermarking tool for AI-generated text - The Verge,https://www.theverge.com/2024/10/23/24277873/google-artificial-intelligence-synthid-watermarking-open-source
Stanford CS229 I Machine Learning I Building Large Language Models (LLMs),https://www.youtube.com/watch?v=9vM4p9NN0Ts
[2410.09713] Agentic Information Retrieval,https://arxiv.org/abs/2410.09713
[2410.15490] Dynamic Intelligence Assessment: Benchmarking LLMs on the Road to AGI with a Focus on Model Confidence,https://arxiv.org/abs/2410.15490
[2410.15267] When Machine Unlearning Meets Retrieval-Augmented Generation (RAG): Keep Secret or Forget Knowledge?,https://arxiv.org/abs/2410.15267
[2410.18071] TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts,https://arxiv.org/abs/2410.18071
[2410.09918v1] Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces,https://arxiv.org/abs/2410.09918
"Simplifying, stabilizing, and scaling continuous-time consistency models | OpenAI",https://openai.com/index/simplifying-stabilizing-and-scaling-continuous-time-consistency-models/
[2410.16251] Can Knowledge Editing Really Correct Hallucinations?,https://arxiv.org/abs/2410.16251
[2410.18745] Why Does the Effective Context Length of LLMs Fall Short?,https://arxiv.org/abs/2410.18745
INTELLECT-1 | Prime Intellect,https://app.primeintellect.ai/intelligence
Embeddings are underrated,https://technicalwriting.dev/data/embeddings.html
llama-recipes/recipes/quickstart/NotebookLlama at main ¬∑ meta-llama/llama-recipes,https://github.com/meta-llama/llama-recipes/tree/main/recipes/quickstart/NotebookLlama
"[2410.21169] Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction",https://arxiv.org/abs/2410.21169
RAG arxiv references | aishwaryanr/awesome-generative-ai-guide,https://github.com/aishwaryanr/awesome-generative-ai-guide/blob/main/research_updates/rag_research_table.md
[2410.19572] ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems,https://arxiv.org/abs/2410.19572
Evaluating feature steering: A case study in mitigating social biases  Anthropic,https://www.anthropic.com/research/evaluating-feature-steering
[2410.20526] Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders,https://arxiv.org/abs/2410.20526
[2410.16270] Reflection-Bench: probing AI intelligence with reflection,https://arxiv.org/abs/2410.16270
[2410.21276] GPT-4o System Card,https://arxiv.org/abs/2410.21276
[2410.15608] Moonshine: Speech Recognition for Live Transcription and Voice Commands,https://arxiv.org/abs/2410.15608
[2410.20011] A Survey of Small Language Models,https://arxiv.org/abs/2410.20011
[2410.20636] Language Models And A Second Opinion Use Case: The Pocket Professional,https://arxiv.org/abs/2410.20636
OmniParser,https://microsoft.github.io/OmniParser/
"Gateways, Guardrails, and GenAI Models | by Manish Dewan | Expedia Group Technology | Oct, 2024 | Medium",https://medium.com/expedia-group-tech/gateways-guardrails-and-genai-models-aa606379164d
The Infrastructure Behind AI Search in Figma | Figma Blog,https://www.figma.com/blog/the-infrastructure-behind-ai-search-in-figma/
The three components of the unstructured data stack | Felicis,https://www.felicis.com/insight/unstructured-data-stack
LLM-assisted vector similarity search,https://engineering.grab.com/llm-assisted-vector-similarity-search
[2410.16540] A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration,https://arxiv.org/abs/2410.16540
[2410.13787] Looking Inward: Language Models Can Learn About Themselves by Introspection,https://arxiv.org/abs/2410.13787
"Anthropic introduces an updated Claude 3.5 Sonnet, and Claude 3.5 Haiku",https://www.anthropic.com/news/3-5-models-and-computer-use
microsoft/semantic-kernel - Guided Conversation demo |Github,https://github.com/microsoft/semantic-kernel/blob/main/python/samples/demos/guided_conversations/README.md
Open Source and In-House: How Uber Optimizes LLM Training | Uber Blog,https://www.uber.com/blog/open-source-and-in-house-how-uber-optimizes-llm-training/
Perplexity releases Internal Knowledge Search and Spaces,https://www.perplexity.ai/hub/blog/introducing-internal-knowledge-search-and-spaces
"[2410.12787] The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio",https://arxiv.org/abs/2410.12787
[2410.12409] Revealing the Barriers of Language Agents in Planning,https://arxiv.org/abs/2410.12409
[2410.12405] ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs,https://arxiv.org/abs/2410.12405
Invisible text that AI chatbots understand and humans can,https://arstechnica.com/security/2024/10/ai-chatbots-can-read-and-write-invisible-text-creating-an-ideal-covert-channel/
Introducing the Message Batches API,https://www.anthropic.com/news/message-batches-api
"""Do Anything Now"": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",https://jailbreak-llms.xinyueshen.me/
LLM Pricing - a Hugging Face Space by philschmid,https://huggingface.co/spaces/philschmid/llm-pricing
How We Built AI Tutoring Tools - Khan Academy Blog,https://blog.khanacademy.org/how-we-built-ai-tutoring-tools/
How to Create Effective AI Prompts for Your Learning Journey - Khan Academy Blog,https://blog.khanacademy.org/how-to-create-effective-ai-prompts-khanmigo-kl/
Khan Academy's Approach to Prompt Engineering for Khanmigo,https://blog.khanacademy.org/khan-academys-7-step-approach-to-prompt-engineering-for-khanmigo/
Why We,https://blog.khanacademy.org/why-were-deeply-invested-in-making-ai-better-at-math-tutoring-and-what-weve-been-up-to-lately/
jxm/cde-small-v1,https://huggingface.co/jxm/cde-small-v1
FACT SHEET: Office of Management and Budget Issues Guidance to Advance the Responsible Acquisition of AI in Government | OMB | The White House,https://www.whitehouse.gov/omb/briefing-room/2024/10/03/fact-sheet-omb-issues-guidance-to-advance-the-responsible-acquisition-of-ai-in-government/
Ad hoc tools for gathering prompt context - Austin Z. Henley,https://austinhenley.com/blog/promptcontext.html
ChatPRD,https://www.chatprd.ai/blog
Reforge,https://www.reforge.com/blog/category/product
also ensure the LORA adapter can] learn the embeddings associated with these special tokens. Papers - [[2410.01201] Were RNNs All We Needed?,https://arxiv.org/abs/2410.01201
[2410.01044] RATIONALYST: Pre-training Process-Supervision for Improving Reasoning,https://arxiv.org/abs/2410.01044
[2410.02660] How to Train Long-Context Language Models (Effectively),https://arxiv.org/abs/2410.02660
[2410.02338] How Much Can RAG Help the Reasoning of LLM?,https://arxiv.org/abs/2410.02338
Introducing the Realtime API | OpenAI,https://openai.com/index/introducing-the-realtime-api/
Prompt Caching in the API | OpenAI,https://openai.com/index/api-prompt-caching/
vision,https://openai.com/index/introducing-vision-to-the-fine-tuning-api/
model distillation,https://openai.com/index/api-model-distillation/
Upgrading the Moderation API with our new multimodal moderation model | OpenAI,https://openai.com/index/upgrading-the-moderation-api-with-our-new-multimodal-moderation-model/
OpenAI DevDay 2024 live blog | Simon Willison,https://simonwillison.net/2024/Oct/1/openai-devday-2024-live-blog/
"Greg Kamradt on X: ""How structured outputs work under the hood (via breakout at OpenAI DevDay) https://t.co/pMhuzYwaLr"" / X",https://x.com/GregKamradt/status/1841187546912735248
Leaked OpenAI prompt-refinement,https://gist.github.com/philschmid/3a0ecc9e45763716f4dd9c36b6445fca
Why NotebookLM is blowing everyone,https://www.turingpost.com/p/fod69
Do Vector Databases Lose Accuracy at Scale?,https://www.eyelevel.ai/post/do-vector-databases-lose-accuracy-at-scale
"GitHub - opendatalab/MinerU: A one-stop, open-source, high-quality data extraction tool, supports PDF/webpage/e-book extraction",https://github.com/opendatalab/MinerU
Introducing ChatGPT search | OpenAI,https://openai.com/index/introducing-chatgpt-search/
"Exclusive: OpenAI builds first chip with Broadcom and TSMC, scales back foundry ambition | Reuters",https://www.reuters.com/technology/artificial-intelligence/openai-builds-first-chip-with-broadcom-tsmc-scales-back-foundry-ambition-2024-10-29/
"Bringing developer choice to Copilot with Anthropic's Claude 3.5 Sonnet, Google's Gemini 1.5 Pro, and OpenAI's o1-preview - The GitHub Blog",https://github.blog/news-insights/product-news/bringing-developer-choice-to-copilot/
Claude 3.5 Sonnet on GitHub Copilot  Anthropic,https://www.anthropic.com/news/github-copilot
Raising the bar on SWE-bench Verified with Claude 3.5 Sonnet  Anthropic,https://www.anthropic.com/research/swe-bench-sonnet
Vector Databases Are the Wrong Abstraction,https://www.timescale.com/blog/vector-databases-are-the-wrong-abstraction/
Creating a LLM-as-a-Judge That Drives Business Results ‚Äì Hamel's Blog,https://hamel.dev/blog/posts/llm-judge/
"AlignEval: Building an App to Make Evals Easy, Fun, and Automated",https://eugeneyan.com/writing/aligneval/
Laminar,https://www.lmnr.ai/
[2410.21333] Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse,https://arxiv.org/abs/2410.21333
Introducing SimpleQA | OpenAI,https://openai.com/index/introducing-simpleqa/
[2410.22071] Distinguishing Ignorance from Error in LLM Hallucinations,https://arxiv.org/abs/2410.22071
[2410.22884] Stealing User Prompts from Mixture of Experts,https://arxiv.org/abs/2410.22884
[2410.21943] Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications,https://arxiv.org/abs/2410.21943
[2410.24032] Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks,https://arxiv.org/abs/2410.24032
Mem0 - The Memory layer for your AI apps,https://mem0.ai/
Building successful multi-tenant RAG applications,https://www.thenile.dev/blog/multi-tenant-rag
Highlighting Text in Links with Text Fragments,https://calebhearth.com/text-fragments
"Arvid Kahl on X: ""I want to run AI agents to scrape specific URLs and do some ... What's the framework that allows for this kind of cutting-edge stuff? Is there an AI agent project that we should be using?"" / X",https://x.com/arvidkahl/status/1853264865520976347
bespokelabs/Bespoke-MiniCheck-7B ¬∑ Hugging Face,https://huggingface.co/bespokelabs/Bespoke-MiniCheck-7B
Managing Chat History for Large Language Models (LLMs) | Semantic Kernel,https://devblogs.microsoft.com/semantic-kernel/managing-chat-history-for-large-language-models-llms/
Autoblocks: GenAI product workspace,https://www.autoblocks.ai/
"Understanding Multimodal LLMs - by Sebastian Raschka, PhD",https://magazine.sebastianraschka.com/p/understanding-multimodal-llms
Predicted Outputs - OpenAI API,https://platform.openai.com/docs/guides/latency-optimization#use-predicted-outputs
"OpenAI Developers on X: ""Introducing Predicted Outputs‚Äîdramatically decrease latency for gpt-4o and gpt-4o-mini by providing a reference string. https://t.co/n6mqjQwQV1 Speed up: - Updating a blog post in a doc - Iterating on prior responses - Rewriting code in an existing file, like @exponent_run here: https://t.co/c9O3YtHH7N"" / X",https://x.com/OpenAIDevs/status/1853564730872607229
Simon Willison's notes,https://simonwillison.net/2024/Nov/4/predicted-outputs/
Claude 3.5 Haiku  Anthropic,https://www.anthropic.com/claude/haiku
Token counting (beta) - Anthropic,https://docs.anthropic.com/en/docs/build-with-claude/token-counting
"wh on X investigates further https://t.co/mA8qyuUALb"" / X",https://x.com/nrehiew_/status/1852701616287125624
Introducing the analysis tool in Claude.ai  Anthropic,https://www.anthropic.com/news/analysis-tool
Simon Willison's notes,https://simonwillison.net/2024/Oct/24/claude-analysis-tool/
API,https://docs.anthropic.com/en/docs/build-with-claude/pdf-support
Chat (feature preview),https://twitter.com/AnthropicAI/status/1852393688451653849
Armand Ruiz on LinkedIn: Disclosing the full list of datasets used to train IBM LLMs Granite‚Ä¶ | 39 comments,https://www.linkedin.com/posts/armand-ruiz_disclosing-the-full-list-of-datasets-used-activity-7259535100927725569-0rmS
Introducing the First AMD 1B Language Models: AMD OLMo,https://www.amd.com/en/developer/resources/technical-articles/introducing-the-first-amd-1b-language-model.html
Brute-forcing the LLM guardrails. exploring and pushing the limits of AI | by Daniel Kharitonov | Medium,https://medium.com/@volkot/brute-forcing-the-llm-guardrails-e02fcd9bc9a4
Home | D2 Documentation,https://d2lang.com/
From Zero to PPO: Understanding the Path to Helpful AI Models,https://www.adaptive-ml.com/post/from-zero-to-ppo
What Every Developer Should Know About GPU Computing,https://blog.codingconfessions.com/p/gpu-computing
Navigating Fair Housing Guardrails in LLMs - Zillow Tech Hub,https://www.zillow.com/tech/navigating-fair-housing-guardrails-in-llms/
"An AI polling startup polls bots, predicts Harris will win | Semafor",https://www.semafor.com/article/11/04/2024/an-ai-polling-startup-polls-bots-predicts-harris-will-win
[2411.00027] Personalization of Large Language Models: A Survey,https://arxiv.org/abs/2411.00027
[2410.22370] Survey of User Interface Design and Interaction Techniques in Generative AI Applications,https://arxiv.org/abs/2410.22370
[2411.00412] Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation,https://arxiv.org/abs/2411.00412
[2411.01747] DynaSaur: Large Language Agents Beyond Predefined Actions,https://arxiv.org/abs/2411.01747
"[2410.16315] Why AI Is WEIRD and Should Not Be This Way: Towards AI For Everyone, With Everyone, By Everyone",https://arxiv.org/abs/2410.16315
[2411.00860] Survey of Cultural Awareness in Language Models: Text and Beyond,https://arxiv.org/abs/2411.00860
[2311.08593] Summarization-Based Document IDs for Generative Retrieval with Language Models,https://arxiv.org/abs/2311.08593
[2410.16454] Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge,https://arxiv.org/abs/2410.16454
"[2411.02355] ""Give Me BF16 or Give Me Death""? Accuracy-Performance Trade-Offs in LLM Quantization",https://arxiv.org/abs/2411.02355
[2411.02335] Sparsing Law: Towards Large Language Models with Greater Activation Sparsity,https://arxiv.org/abs/2411.02335
[2411.00918] LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models,https://arxiv.org/abs/2411.00918
[2411.02265] Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent,https://arxiv.org/abs/2411.02265
[2410.24159] GPT or BERT: why not both?,https://arxiv.org/abs/2410.24159
Mapping the landscape of gen-AI product user experience (Interconnected),https://interconnected.org/home/2024/07/19/ai-landscape
The Dual LLM pattern for building AI assistants that can resist prompt injection,https://simonwillison.net/2023/Apr/25/dual-llm-pattern/
Microsoft tests AI-powered editing in Notepad - The Verge,https://www.theverge.com/2024/11/6/24289707/microsoft-notepad-ai-text-editing-rewrite
Meta,https://techcrunch.com/2024/11/04/meta-says-its-making-its-llama-models-available-for-us-national-security-applications/
Anthropic,https://techcrunch.com/2024/11/07/anthropic-teams-up-with-palantir-and-aws-to-sell-its-ai-to-defense-customers/
"Rohan Paul on X: ""First dedicated transformer ASIC (Application-Specific Integrated Circuit) just dropped. Sohu is an ASIC, a custom chip with hardware specifically optimized for transformer calculations, making AI models run 10x faster than GPUs https://t.co/YLgwwOlSvV"" / X",https://x.com/rohanpaul_ai/status/1854326252674384129
[2411.02959] HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems,https://arxiv.org/abs/2411.02959
[2411.03562] Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level,https://arxiv.org/abs/2411.03562
[2411.05000] Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?,https://arxiv.org/abs/2411.05000
[2411.02830] Mixtures of In-Context Learners,https://arxiv.org/abs/2411.02830
[2411.04996] Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models,https://arxiv.org/abs/2411.04996
[2411.04905] OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models,https://arxiv.org/abs/2411.04905
OpenCoder: Top-Tier Open Code Large Language Models,https://opencoder-llm.github.io/
[2411.04965] BitNet a4.8: 4-bit Activations for 1-bit LLMs,https://arxiv.org/abs/2411.04965
[2410.21228] LoRA vs Full Fine-tuning: An Illusion of Equivalence,https://arxiv.org/abs/2410.21228
CONFIRMED: LLMs have indeed reached a point of diminishing returns,https://garymarcus.substack.com/p/confirmed-llms-have-indeed-reached
"OpenAI's new ""Orion"" model reportedly shows [only] small gains over GPT-4",https://the-decoder.com/openais-new-orion-model-reportedly-shows-small-gains-over-gpt-4/
OpenAI Scored a Legal Win Over Progressive Publishers‚Äîbut the Fight's Not Finished | WIRED,https://www.wired.com/story/opena-alternet-raw-story-copyright-lawsuit-dmca-standing/
RawStory vs OpenAI case was dismissed by the judge,https://www.courtlistener.com/docket/68290709/117/raw-story-media-inc-v-openai-inc/
Ethan He on LinkedIn: Cosmos Tokenizer - a nvidia Collection,https://www.linkedin.com/posts/ethanhe42_cosmos-tokenizer-a-nvidia-collection-activity-7260409186126094337-oCto
State-of-the-Art Multimodal Generative AI Model Development with NVIDIA NeMo | NVIDIA Technical Blog,https://developer.nvidia.com/blog/state-of-the-art-multimodal-generative-ai-model-development-with-nvidia-nemo/
Building a multi-agent concierge system from scratch,https://www.youtube.com/watch?v=wuuO04j4jPc&themeRefresh=1
LlamaIndex on LinkedIn: Powerpoint generation is a top agent use case among enterprises üî• This‚Ä¶,https://www.linkedin.com/posts/llamaindex_powerpoint-generation-is-a-top-agent-use-activity-7261423410881146880-TCTg
The AI Services Revolution: Why VCs Are Betting Big on Agentic AI ‚Äì VC Cafe,https://www.vccafe.com/2024/11/09/the-ai-services-revolution-why-vcs-are-betting-big-on-agentic-ai/
AI Agents Directory | Discover Top AI Agents,https://alltheaitools.com/ai-agents
Intro to Arch | Arch Docs v0.1,https://docs.archgw.com/get_started/intro_to_arch.html
Dust - Build custom AI assistants to speed up your work,https://dust.tt/
"Agents @ Work: Dust.tt - Latent Space: The AI Engineer Podcast ‚Äî Practitioners talking LLMs, CodeGen, Agents, Multimodality, AI UX, GPU Infra and al - Apple Podcasts",https://podcasts.apple.com/us/podcast/agents-work-dust-tt/id1674008350?i=1000676544467
bhavnicksm/chonkie: ü¶õ CHONK your texts with Chonkie ‚ú® - The no-nonsense RAG chunking library,https://github.com/bhavnicksm/chonkie
Everything I've learned so far about running local LLMs,https://nullprogram.com/blog/2024/11/10/
microsoft/TinyTroupe: LLM-powered multiagent persona simulation for imagination enhancement and business insights.,https://github.com/microsoft/TinyTroupe
Binary vector embeddings are so cool | Evan Schwartz,https://emschwartz.me/binary-vector-embeddings-are-so-cool/
LiteLLM - Getting Started | liteLLM,https://docs.litellm.ai/
[2402.05201] The Effect of Sampling Temperature on Problem Solving in Large Language Models,https://arxiv.org/abs/2402.05201
[2411.04986] The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities,https://arxiv.org/abs/2411.04986
[2411.02571] MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs,https://arxiv.org/abs/2411.02571
[2411.04425] DELIFT: Data Efficient Language model Instruction Fine Tuning,https://arxiv.org/abs/2411.04425
[2411.06839] LLM-Neo: Parameter Efficient Knowledge Distillation for Large Language Models,https://arxiv.org/abs/2411.06839
[2411.04872] FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI,https://arxiv.org/abs/2411.04872
Test-Time Training Project Website,https://yueatsprograms.github.io/ttt/home.html
The Surprising Effectiveness of Test-Time Training for Abstract Reasoning,https://ekinakyurek.github.io/papers/ttt.pdf
[2410.08020] Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs,https://arxiv.org/abs/2410.08020
Aman's AI Journal - Primers - Agents,https://aman.ai/primers/ai/agents/
The Future of Programming: Copilots vs. Agents (Part I),https://archive.is/zHwxz
"Cloud providers deploy custom AI accelerators, nibble away at Nvidia's AI dominance",https://www.ankursnewsletter.com/p/google-tpus-vs-aws-trainium-and-inferentia
"The Beginner's Guide to Visual Prompt Injections: Invisibility Cloaks, Cannibalistic Adverts, and Robot Women | Lakera ‚Äì Protecting AI teams that disrupt the world.",https://www.lakera.ai/blog/visual-prompt-injections
Prompt Injecting Your Way To Shell: OpenAI's Containerized | 0din.ai,https://0din.ai/blog/prompt-injecting-your-way-to-shell-openai-s-containerized-chatgpt-environment
Releasing the largest multilingual open pretraining dataset,https://huggingface.co/blog/Pclanglais/two-trillion-tokens-open
Google's Learn About experiment,https://learning.google.com/experiments/learn-about/signup
Knowledge-as-a-service: The future of community business models - Stack Overflow,https://stackoverflow.blog/2024/09/30/knowledge-as-a-service-the-future-of-community-business-models/
"OK, NVIDIA NV-emb-2 embeddings are really, really good‚Ä¶",https://www.linkedin.com/posts/tunguz_ok-nvidia-nv-emb-2-embeddings-are-really-activity-7262862383885213696-MWVv
nvidia/NV-Embed-v2 ¬∑ Hugging Face,https://huggingface.co/nvidia/NV-Embed-v2
Design UX to encourage human feedback,https://applied-llms.org/#design-your-ux-for-human-in-the-loop
Nexusflow on LinkedIn: Introducing Athene-V2: An Open Model Suite Comparable to GPT-4o across‚Ä¶,https://www.linkedin.com/posts/nexusflow-ai_introducing-athene-v2-an-open-model-suite-activity-7262856019490349056-aKba
Improve your prompts in the developer console  Anthropic,https://www.anthropic.com/news/prompt-improver
We can all be AI engineers ‚Äì and we can do it with open source models,https://blog.helix.ml/p/we-can-all-be-ai-engineers
AI Spec,https://aispec.org/
[2411.07133] Stronger Models are NOT Stronger Teachers for Instruction Tuning,https://arxiv.org/abs/2411.07133
[2411.07763] Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows,https://arxiv.org/abs/2411.07763
"[2411.06722] Synthesize, Partition, then Adapt: Eliciting Diverse Samples from Foundation Models",https://arxiv.org/abs/2411.06722
[2411.07494] Rapid Response: Mitigating LLM Jailbreaks with a Few Examples,https://arxiv.org/abs/2411.07494
[2411.08257] GPTree: Towards Explainable Decision-Making via LLM-powered Decision Trees,https://arxiv.org/abs/2411.08257
pyro-ppl/numpyro: Probabilistic programming with NumPy powered by JAX for autograd and JIT compilation to GPU/TPU/CPU.,https://github.com/pyro-ppl/numpyro
AI Agents stack landscape,https://www.letta.com/blog/ai-agents-stack
ZeroEntropy-AI/llama-chunk,https://github.com/ZeroEntropy-AI/llama-chunk
Agents @ Work: Lindy.ai - Latent Space,https://www.latent.space/p/lindy
Lindy.ai,https://www.lindy.ai/
"need to] prune the memories."" - @17, almost everything is structured output, though some things that allow standard text generation (email drafting, etc) - @18/19, agentic hierarchy -- ""How do you think about what's a Lindy versus what's a sub-function of a Lindy? Like, what's the hierarchy?"" ""I think the line is a little arbitrary. It's kind of like when you code, like when do you start to create a new class versus when do you overload your current class. I think of it in terms of like jobs to be done and I think of it in terms of who is the Lindy serving."" For some specialized tasks or if the Lindy is serving a different audience, I create a separate Lindy for it. ""And you can call a Lindy from within another Lindy. That's right. You can kind of chain them together."" - On setting up a user flywheel: - @16, ""people want to brag about the complexity of their Lindys. So this would be like a 65 point Lindy, right? 65 point -- Complexity counting. Like how many nodes, how many things, how many conditions, right? ... let people brag. Let people be super users. Give them a score."" - @24, ""you have an academy [for tutorials]... I also see some other people doing it for you for free.... I think that's the flywheel that you built the platform where creators see value in allying themselves to you. And so then, you know, your incentive is to make them successful so that they can make other people successful and then it just drives more and more engagement. Like it's earned media. Like you don't have to do anything."" ""Yeah... community is everything."" - @30, ""The raw capabilities [of the connectors/non-AI infra] for sure are a big limit. It is actually shocking the extent to which the model is no longer the limit. It was the limit a year ago. It was too expensive. The context window was too small. It's kind of insane that we started building this when the context windows were like 4,000 tokens. Like today, our system prompt is more than 4,000 tokens."" - @31, on human-in-the-loop and real-time ICL - ""So you can turn on a toggle on any step of your Lindy workflow to be like, ask me for confirmation before you actually execute this step. So it's like, hey, I receive an email, you send a reply, ask me for confirmation before actually sending it. And so today you see the email that's about to get sent and you can either approve, deny, or change it and then approve. And we are making it so that when you make a change, we are then saving this change that you're making or embedding it in the vector database. And then we are retrieving these examples for future tasks and injecting them into the context window. So that's the kind of capability that makes a huge difference for users."" - @32, on prompt caching: ""For prompt caching to work, you need the front prefix portion to be stable."" ""Yes, but we have this append-only ledger paradigm. So every node keeps appending to that ledger and every filled node inherits all the context built up by all the previous nodes. And so we can just decide, like, hey, every X thousand nodes, we trigger prompt caching again.... But basically, because we keep appending to the prompt, the prompt caching works pretty well."" - post ~38, sprawling discussion on tech & politics, - [NVIDIA/garak: the LLM vulnerability scanner",https://github.com/NVIDIA/garak
On reducing RAG hallucinations by allowing "I don't know",https://www.linkedin.com/posts/daanyaalchaudry_i-managed-to-almost-completely-eliminate-activity-7262040782428106754-BxN0
"Department of Homeland Security releases ""Roles and Responsibilities Framework for Artificial Intelligence in Critical Infrastructure"" report",https://www.dhs.gov/sites/default/files/2024-11/24_1114_dhs_ai-roles-and-responsibilities-framework-508.pdf
AI Replacing Entry-Level Jobs Could Break the Career Ladder - Bloomberg,https://www.bloomberg.com/news/articles/2024-11-15/ai-replacing-entry-level-jobs-could-break-the-career-ladder
Mistral has entered the chat | Mistral AI | Frontier AI in your hands,https://mistral.ai/news/mistral-chat/
"circlemind-ai/fast-graphrag: RAG that intelligently adapts to your use case, data, and queries",https://github.com/circlemind-ai/fast-graphrag/tree/main
HN Discussion: FastGraphRAG ‚Äì Better RAG using good old PageRank,https://news.ycombinator.com/item?id=42174829
[2411.05285] A Taxonomy of AgentOps for Enabling Observability of Foundation Model based Agents,https://arxiv.org/abs/2411.05285
"[2411.11504] Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering",https://arxiv.org/abs/2411.11504
[2411.09213] Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering,https://arxiv.org/abs/2411.09213
[2411.11767] Drowning in Documents: Consequences of Scaling Reranker Inference,https://arxiv.org/abs/2411.11767
[2411.07641] Top-$nœÉ$: Not All Logits Are You Need,https://arxiv.org/abs/2411.07641
LangChain State of AI Agents Report,https://www.langchain.com/stateofaiagents#barriers-and-challenges
xpander.ai | Build Better AI Agents,https://xpander.ai/
xpander.ai Agent Graph System makes AI agents 4X more reliable | VentureBeat,https://venturebeat.com/ai/xpander-ais-agent-graph-system-makes-ai-agents-more-reliable-by-giving-them-info-step-by-step/
Powering the next generation of AI development with AWS  Anthropic,https://www.anthropic.com/news/anthropic-amazon-trainium
DocumindHQ/documind: Open-source platform for extracting structured data from documents using AI.,https://github.com/DocumindHQ/documind
DataExpert-io/data-engineer-handbook: This is a repo with links to everything you'd ever want to learn about data engineering,https://github.com/DataExpert-io/data-engineer-handbook
Understanding the BM25 full text search algorithm | Evan Schwartz,https://emschwartz.me/understanding-the-bm25-full-text-search-algorithm/
Judge Arena: Benchmarking LLMs as Evaluators,https://huggingface.co/blog/arena-atla
"DeepSeek on X: ""üöÄ DeepSeek-R1-Lite-Preview is now live",https://x.com/deepseek_ai/status/1859200141355536422
Agent Memory : r/LocalLLaMA,https://www.reddit.com/r/LocalLLaMA/comments/1gvhpjj/agent_memory/
Introducing Observers: AI Observability with Hugging Face datasets through a lightweight SDK,https://huggingface.co/blog/davidberenstein1957/observers-a-lightweight-sdk-for-ai-observability
"PaulPauls/llama3_interpretability_sae: A complete end-to-end pipeline for LLM interpretability with sparse autoencoders (SAEs) using Llama 3.2, written in pure PyTorch and fully reproducible.",https://github.com/PaulPauls/llama3_interpretability_sae
"OK, I can partly explain the LLM chess weirdness now",https://dynomight.net/more-chess/
Are language models good at making predictions?,https://dynomight.net/predictions/
2024: The State of Generative AI in the Enterprise - Menlo Ventures,https://menlovc.com/2024-the-state-of-generative-ai-in-the-enterprise/
T√ºlu 3: The next era in open post-training,https://www.interconnects.ai/p/tulu-3
A statistical approach to model evaluations  Anthropic,https://www.anthropic.com/research/statistical-approach-to-model-evals
openbmb/VisRAG-Ret ¬∑ Hugging Face,https://huggingface.co/openbmb/VisRAG-Ret
[2411.11296] Steering Language Model Refusal with Sparse Autoencoders,https://arxiv.org/abs/2411.11296
[2411.09003] Refusal in LLMs is an Affine Function,https://arxiv.org/abs/2411.09003
[2411.12372] RedPajama: an Open Dataset for Training Large Language Models,https://arxiv.org/abs/2411.12372
"[2411.08165] Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion",https://arxiv.org/abs/2411.08165
[2310.11324] Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting,https://arxiv.org/abs/2310.11324
[2411.10958] SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration,https://arxiv.org/abs/2411.10958
[2409.08107] WhisperNER: Unified Open Named Entity and Speech Recognition,https://arxiv.org/abs/2409.08107
aiOla unveils open source AI transcription that masks sensitive info | VentureBeat,https://venturebeat.com/ai/aiola-unveils-open-source-ai-audio-transcription-model-that-obscures-sensitive-info-in-realtime/
CSnakes,https://tonybaloney.github.io/CSnakes/
Unlocking the Power of Memory: Announcing General Availability of Semantic Kernel's Memory Packages | Semantic Kernel,https://devblogs.microsoft.com/semantic-kernel/unlocking-the-power-of-memory-announcing-general-availability-of-semantic-kernels-memory-packages/
Leveraging Microsoft 365 Agents SDK with Semantic Kernel for Enhanced Multichannel AI | Semantic Kernel,https://devblogs.microsoft.com/semantic-kernel/leveraging-microsoft-365-agents-sdk-with-semantic-kernel-for-enhanced-multichannel-ai/
App Intents | Apple Developer Documentation,https://developer.apple.com/documentation/appintents
Making actions and content discoverable and widely available | Apple Developer Documentation,https://developer.apple.com/documentation/appintents/making-actions-and-content-discoverable-and-widely-available
Jina CLIP v2: Multilingual Multimodal Embeddings for Text and Images,https://jina.ai/news/jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images/
getomni-ai/zerox: PDF to Markdown with vision models,https://github.com/getomni-ai/zerox
XGrammar | Home,https://xgrammar.mlc.ai/
NVIDIA/logits-processor-zoo: A collection of LogitsProcessors to customize and enhance LLM behavior for specific tasks.,https://github.com/NVIDIA/logits-processor-zoo
Introducing the Model Context Protocol  Anthropic,https://www.anthropic.com/news/model-context-protocol
Introducing Model Context Protocol (MCP) - ChatGPT for teams | Glama,https://glama.ai/blog/2024-11-25-model-context-protocol-quickstart
Advancing red teaming with people and AI | OpenAI,https://openai.com/index/advancing-red-teaming-with-people-and-ai/
openais-approach-to-external-red-teaming.pdf,https://cdn.openai.com/papers/openais-approach-to-external-red-teaming.pdf
diverse-and-effective-red-teaming.pdf,https://cdn.openai.com/papers/diverse-and-effective-red-teaming.pdf
[2311.17049] MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training,https://arxiv.org/abs/2311.17049
apple/coreml-mobileclip ¬∑ Hugging Face,https://huggingface.co/apple/coreml-mobileclip
[2411.02093] Do Advanced Language Models Eliminate the Need for Prompt Engineering in Software Engineering?,https://arxiv.org/abs/2411.02093
[2411.12946] A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection,https://arxiv.org/abs/2411.12946
[2411.13543] BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games,https://arxiv.org/abs/2411.13543
[2411.12580] Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models,https://arxiv.org/abs/2411.12580
[2411.16594] From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge,https://arxiv.org/abs/2411.16594
llm-as-a-judge/Awesome-LLM-as-a-judge,https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge
Building LLM-driven agents | sean goedecke,https://www.seangoedecke.com/llm-driven-agents/
Illogical Logic: Why Agents Are Stupid & What We Can Do About It // Dan Jeffries // Agents in Prod,https://www.youtube.com/watch?v=TbnIA0Er5jA
[2305.18323] ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,https://arxiv.org/abs/2305.18323
"Introducing Microsoft Copilot actions, new agents, and tools to empower IT| Microsoft 365 Blog",https://www.microsoft.com/en-us/microsoft-365/blog/2024/11/19/introducing-copilot-actions-new-agents-and-tools-to-empower-it-teams/
The Shift from Models to Compound AI Systems,https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/
Memory in Agent Systems - by Aurimas Grici,https://www.newsletter.swirlai.com/p/memory-in-agent-systems
OCR at the edge: recommendations,https://docs.google.com/document/d/1gEavG5BXXDBbZ_TrtSc8o6lRANpbytAa82Wg1vvJXbg/mobilebasic
Building AI Systems on Postgres: An Inside Look at pgai Vectorizer,https://www.aiengineeringpodcast.com/episodepage/building-ai-systems-on-postgres-an-inside-look-at-pgai-vectorizer
provides] a system that works out of the box to automatically create and update embeddings as the underlying source data changes. - Observability - [Observability in LLMOps pipeline - Different Levels of Scale,https://www.newsletter.swirlai.com/p/observability-in-llmops-pipeline
Judge Arena Leaderboard,https://huggingface.co/spaces/AtlaAI/judge-arena
Ai2 blog | T,https://allenai.org/blog/tulu-3-technical
tulu-3-report.pdf,https://allenai.org/papers/tulu-3-report.pdf
[2411.14402] Multimodal Autoregressive Pre-training of Large Vision Encoders,https://arxiv.org/abs/2411.14402
State of AI Report 2024 // Nathan Benaich // Agents in Production - Video | MLOps Community,https://home.mlops.community/public/videos/state-of-ai-report-2024-nathan-benaich-agents-in-production
Presentation,https://docs.google.com/presentation/d/1y_oP8JH24uTING55RlkXHsRxf3fUt5unnoxd6iUX6XU/edit#slide=id.g313a92a5244_0_997
Illogical Logic: Why Agents Are Stupid & What We Can Do About It // Dan Jeffries // Agents in Production - Video | MLOps Community,https://home.mlops.community/public/videos/illogical-logic-why-agents-are-stupid-and-what-we-can-do-about-it-dan-jeffries-agents-in-production
LLMs to agents: The Beauty & Perils of Investing in GenAI // VC Panel // Agents in Production - Video | MLOps Community,https://home.mlops.community/public/videos/llms-to-agents-the-beauty-and-perils-of-investing-in-genai-vc-panel-agents-in-production-2024-11-15
GitHub - jmatthiesen/dotnet-ai-resources: A collection of resources available to .NET developers working with AI.,https://github.com/jmatthiesen/dotnet-ai-resources
PyTorch Deprecation of Conda Nightly Builds - release/packaging - PyTorch Developer Mailing List,https://dev-discuss.pytorch.org/t/pytorch-deprecation-of-conda-nightly-builds/2590
astral-sh/uv: A Python package and project manager,https://github.com/astral-sh/uv
OpenAI,https://platform.openai.com/docs/guides/prompt-generation#meta-prompts
GitHub - r1chardj0n3s/parse: Parse strings using a specification based on the Python format() syntax.,https://github.com/r1chardj0n3s/parse
PleIAs/OCRonos-Vintage,https://huggingface.co/PleIAs/OCRonos-Vintage
Nexusflow.ai | Blog :: Introducing Athene-V2: Advancing Beyond the Limits of Scaling with Targeted Post-training,https://nexusflow.ai/blogs/athene-v2
Gemini is now accessible from the OpenAI python library - Google Developers Blog,https://developers.googleblog.com/en/gemini-is-now-accessible-from-the-openai-library/
SWE Kit: Open-Source Headless IDE for Coding Agents with State-of-the-Art Performance - Composio,https://composio.dev/swe-kit/
LLM overkill is real: I analyzed 12 benchmarks to find the right-sized model for each use case,https://www.reddit.com/r/LocalLLaMA/comments/1glscfk/llm_overkill_is_real_i_analyzed_12_benchmarks_to/
LLM Selector,https://llmselector.vercel.app/
Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks - Microsoft Research,https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/
report pdf,https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Magentic-One.pdf
[2402.02805] Graph-enhanced Large Language Models in Asynchronous Plan Reasoning,https://arxiv.org/abs/2402.02805
DS4SD/docling: Get your docs ready for gen AI,https://github.com/DS4SD/docling
[2408.09869] Docling Technical Report,https://arxiv.org/abs/2408.09869
[2410.07959] COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking Suite for the EU Artificial Intelligence Act,https://arxiv.org/abs/2410.07959
together-cookbook/Open_Contextual_RAG.ipynb at main,https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb
"GitHub - NirDiamant/GenAI_Agents: This repository provides tutorials and implementations for various Generative AI Agent techniques, from basic to advanced. It serves as a comprehensive guide for building intelligent, interactive AI systems.",https://github.com/NirDiamant/GenAI_Agents
Document understanding is very very hard: an illustration : r/LocalLLaMA,https://www.reddit.com/r/LocalLLaMA/comments/1gekd53/document_understanding_is_very_very_hard_an/?rdt=65404
The AI Risk Repository,https://airisk.mit.edu/
[2410.20088] RARe: Retrieval Augmented Retrieval with In-Context Examples,https://arxiv.org/abs/2410.20088
Rules of Machine Learning: | Google for Developers,https://developers.google.com/machine-learning/guides/rules-of-ml
How to Run a Weekly Paper Club (and Build a Learning Community),https://eugeneyan.com/writing/paper-club/
The National Academies Press | Artificial Intelligence and the Future of Work,https://nap.nationalacademies.org/resource/27644/interactive/
Building LLMs is probably not going be a brilliant business,https://calpaterson.com/porter.html
"Ai2 on X: ""Meet OLMo 2, the best fully open language model to date, including a family of 7B and 13B models trained up to 5T tokens. OLMo 2 outperforms other fully open models and competes with open-weight models like Llama 3.1 8B ‚Äî As always, we released our data, code, recipes and more üéÅ https://t.co/YQ7z8W9lE6"" / X",https://x.com/allen_ai/status/1861511421064028646
"Multimodal parsing for RAG: Azure OpenAI GPT-4o, LlamaParse and Azure AI Search",https://techcommunity.microsoft.com/blog/Azure-AI-Services-blog/multimodal-parsing-for-rag-azure-openai-gpt-4o-llamaparse-and-azure-ai-search/4330399
dynamic_section_retrieval.ipynb | llama_parse,https://github.com/run-llama/llama_parse/blob/main/examples/advanced_rag/dynamic_section_retrieval.ipynb
Fine-tune Embedding models for Retrieval Augmented Generation (RAG),https://www.philschmid.de/fine-tune-embedding-model-for-rag
Choosing the Right AI Agent Framework: LangGraph vs CrewAI vs OpenAI Swarm,https://www.relari.ai/blog/ai-agent-framework-comparison-langgraph-crewai-openai-swarm
How to choose the right Agent Framework,https://www.linkedin.com/posts/yizhang0123_aiagent-llm-langgraph-activity-7266936795265712128-thgP
"QwenLM/Qwen-Agent: Agent framework and applications built upon Qwen>=2.0, featuring Function Calling, Code Interpreter, RAG, and Chrome extension.",https://github.com/QwenLM/Qwen-Agent/tree/main
QwQ: Reflect Deeply on the Boundaries of the Unknown | Qwen,https://qwenlm.github.io/blog/qwq-32b-preview/
Conversational Game Theory ‚Äì Collective Intelligence Engine for Ai and Humans,https://aikiwiki.com/
vidore/colsmolvlm-alpha ¬∑ Hugging Face,https://huggingface.co/vidore/colsmolvlm-alpha
MinishLab/vicinity: Lightweight Nearest Neighbors with Flexible Backends,https://github.com/MinishLab/vicinity
Shadow Workspace: Iterating on Code in the Background,https://www.cursor.com/blog/shadow-workspace
The Dark Art of Chunking,https://blog.trustgraph.ai/p/dark-art-of-chunking
Connect Data Silos | TrustGraph,https://trustgraph.ai/
trustgraph-ai/trustgraph: Connect Data Silos with Explainable AI‚ö°üöÄ,https://github.com/trustgraph-ai/trustgraph
The 2024 State of RAG,https://www.youtube.com/watch?v=dxXf2zSAdo0
"andrewyng/aisuite: Simple, unified interface to multiple Generative AI providers",https://github.com/andrewyng/aisuite
AgentOS,https://ag2.ai/
ag2ai/ag2: AG2 (formerly AutoGen): The Open-Source AgentOS. Join us at: https://discord.gg/pAbnFJrkgZ,https://github.com/ag2ai/ag2
lmnr-ai/flow: A lightweight task engine for building AI agents that prioritizes simplicity and flexibility.,https://github.com/lmnr-ai/flow
PydanticAI,https://ai.pydantic.dev/
jiter/crates/jiter-python at main ¬∑ pydantic/jiter,https://github.com/pydantic/jiter/tree/main/crates/jiter-python
[2411.16133] Context Awareness Gate For Retrieval Augmented Generation,https://arxiv.org/abs/2411.16133
[2411.10541] Does Prompt Formatting Have Any Impact on LLM Performance?,https://arxiv.org/abs/2411.10541
[2411.17501] Inference Scaling $scriptsizemathtt{F}$Laws: The Limits of LLM Resampling with Imperfect Verifiers,https://arxiv.org/abs/2411.17501
[2411.05059] FineTuneBench: How well do commercial fine-tuning APIs infuse knowledge into LLMs?,https://arxiv.org/abs/2411.05059
[2411.07127] Benchmarking LLMs' Judgments with No Gold Standard,https://arxiv.org/abs/2411.07127
yx-lu/Benchmarking-LLMs--Judgments-with-No-Gold-Standard,https://github.com/yx-lu/Benchmarking-LLMs--Judgments-with-No-Gold-Standard
[2411.14497] Star-Agents: Automatic Data Optimization with LLM Agents for Instruction Tuning,https://arxiv.org/abs/2411.14497
[2411.05504] LBPE: Long-token-first Tokenization to Improve Large Language Models,https://arxiv.org/abs/2411.05504
[2410.22832] HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language Models,https://arxiv.org/abs/2410.22832
[2410.19494] Graph Linearization Methods for Reasoning on Graphs with Large Language Models,https://arxiv.org/abs/2410.19494
[2411.17713] Llama Guard 3-1B-INT4: Compact and Efficient Safeguard for Human-AI Conversations,https://arxiv.org/abs/2411.17713
[2411.18478] Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS,https://arxiv.org/abs/2411.18478
[2412.00154] o1-Coder: an o1 Replication for Coding,https://arxiv.org/abs/2412.00154
[2412.01152v1] INTELLECT-1 Technical Report,https://arxiv.org/abs/2412.01152
ü§ó Open-source AI: year in review 2024,https://huggingface-open-source-ai-year-in-review-2024.static.hf.space/index.html?day=3
Introducing Amazon Nova: Frontier intelligence and industry leading price performance | AWS News Blog,https://aws.amazon.com/de/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/#aws-page-content-main
OpenAI o1 System Card | OpenAI,https://openai.com/index/openai-o1-system-card/
They Said It Couldn't Be Done,https://huggingface.co/blog/Pclanglais/common-models
Getting Started - Turftopic,https://x-tabdeveloping.github.io/turftopic/
ZenML - LLMOps Database,https://www.zenml.io/llmops-database
"Codium-ai/AlphaCodium: Official implementation for the paper: ""Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering""""",https://github.com/Codium-ai/AlphaCodium/tree/main
"AI Engineering: Building Applications with Foundation Models: Huyen, Chip: 9781098166304: Amazon.com: Books",https://www.amazon.com/AI-Engineering-Building-Applications-Foundation/dp/1098166302?=&language=en_US
AILuminate - MLCommons,https://mlcommons.org/ailuminate/
A New Benchmark for the Risks of AI | WIRED,https://www.wired.com/story/benchmark-for-ai-risks/
"gusye1234/nano-graphrag: A simple, easy-to-hack GraphRAG implementation",https://github.com/gusye1234/nano-graphrag
Let's build an app for evaluating LLMs,https://blog.mozilla.ai/lets-build-an-app-for-evaluating-llms/
CohereForAI/Global-MMLU ¬∑ Datasets at Hugging Face,https://huggingface.co/datasets/CohereForAI/Global-MMLU
[2412.02592] OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation,https://arxiv.org/abs/2412.02592
VikParuchuri/marker: Convert PDF to markdown + JSON quickly with high accuracy,https://github.com/VikParuchuri/marker
[2412.03555] PaliGemma 2: A Family of Versatile VLMs for Transfer,https://arxiv.org/abs/2412.03555
CLIP to SigLIP: Vision-Language Models with Contrastive Learning,https://blog.ritwikraha.dev/choosing-between-siglip-and-clip-for-language-image-pretraining
[2404.06910] Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation,https://arxiv.org/abs/2404.06910
[2412.03205] U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills in LLMs,https://arxiv.org/abs/2412.03205
Llama 3.3 70B,https://www.llama.com/
Countless.dev | AI Model Pricing Comparison,https://countless.dev/
Multi-Turn Chat Evals ‚Äì Hamel's Blog,https://hamel.dev/notes/llm/officehours/evalmultiturn.html
"mindee/doctr: docTR (Document Text Recognition) - a seamless, high-performing & accessible library for OCR-related tasks powered by Deep Learning.",https://github.com/mindee/doctr
"15 Times to use AI, and 5 Not to - by Ethan Mollick",https://www.oneusefulthing.org/p/15-times-to-use-ai-and-5-not-to
I-JEPA now available on huggingface,https://huggingface.co/docs/transformers/main/en/model_doc/ijepa
Niels Rogge on LinkedIn,https://www.linkedin.com/posts/niels-rogge-a3b7a3127_genai-activity-7271871534296612865-2QXM
[2412.03679] Evaluating Language Models as Synthetic Data Generators,https://arxiv.org/abs/2412.03679
[2412.04003] Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement,https://arxiv.org/abs/2412.04003
[2412.01169] OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows,https://arxiv.org/abs/2412.01169
[2412.04468] NVILA: Efficient Frontier Visual Language Models,https://arxiv.org/abs/2412.04468
[2412.04862] EXAONE 3.5: Series of Large Language Models for Real-world Use Cases,https://arxiv.org/abs/2412.04862
A Path Towards Autonomous Machine Intelligence | OpenReview,https://openreview.net/forum?id=BZ5a1r-kVsf
[2412.06769] Training Large Language Models to Reason in a Continuous Latent Space,https://arxiv.org/abs/2412.06769
Google introduces Gemini 2.0: A new AI model for the agentic era,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/
Selecting a model for semantic search at Dropbox scale - Dropbox,https://dropbox.tech/machine-learning/selecting-model-semantic-search-dropbox-ai
"Scaling Laws ‚Äì O1 Pro Architecture, Reasoning Training Infrastructure, Orion and Claude 3.5 Opus "Failures" ‚Äì SemiAnalysis",https://semianalysis.com/2024/12/11/scaling-laws-o1-pro-architecture-reasoning-training-infrastructure-orion-and-claude-3-5-opus-failures/
Taming LLMs,https://www.souzatharsis.com/tamingLLMs/markdown/toc.html
Transformer Shortage Crisis: Can New Engineering Solve It? - IEEE Spectrum,https://spectrum.ieee.org/transformer-shortage
Open-source Observability for LLMs with OpenTelemetry,https://www.traceloop.com/openllmetry
MLflow Tracing for LLM Observability,https://mlflow.org/docs/latest/llms/tracing/index.html
Enterprise-Level LLMOps: W&B Traces,https://wandb.ai/site/traces/
LLM Tracing | DeepEval - The Open-Source LLM Evaluation Framework,https://docs.confident-ai.com/docs/confident-ai-tracing
What is LLM Tracing? Tools & Challenges,https://www.deepchecks.com/glossary/llm-tracing/
Quickstart: LLM Tracing | Arize Docs,https://docs.arize.com/arize/llm-tracing/quickstart-llm
LLM Observability & Application Tracing (open source) - Langfuse,https://langfuse.com/docs/tracing
[2412.07626] OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations,https://arxiv.org/abs/2412.07626
[2412.07334] Frame Representation Hypothesis: Multi-Token LLM Interpretability and Concept-Guided Text Generation,https://arxiv.org/abs/2412.07334
[2412.07724] Granite Guardian,https://arxiv.org/abs/2412.07724
Clio: Privacy-preserving insights into real-world AI use  Anthropic Blog,https://www.anthropic.com/research/clio
Clio-Privacy-Preserving-Insights-into-Real-World-AI-Use.pdf,https://assets.anthropic.com/m/7e1ab885d1b24176/original/Clio-Privacy-Preserving-Insights-into-Real-World-AI-Use.pdf
[2412.08905] Phi-4 Technical Report,https://arxiv.org/abs/2412.08905
[2311.06237] Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming,https://arxiv.org/abs/2311.06237
[2412.09596] InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions,https://arxiv.org/abs/2412.09596
[2412.08635] Multimodal Latent Language Modeling with Next-Token Diffusion,https://arxiv.org/abs/2412.08635
[2412.09569] JuStRank: Benchmarking LLM Judges for System Ranking,https://arxiv.org/abs/2412.09569
JuStRank Leaderboard,https://huggingface.co/spaces/ibm/JuStRank
[2412.09460] The Impact of Copyrighted Material on Large Language Models: A Norwegian Perspective,https://arxiv.org/abs/2412.09460
Louis Hunt on LinkedIn: I recently left the brilliant team at Liquid AI where I was CFO & VP BD to‚Ä¶ | 115 comments,https://www.linkedin.com/posts/louiswhunt_i-recently-left-the-brilliant-team-at-liquid-activity-7273794644964950016-_sJO
Ask HN: Examples of Agentic LLM Systems in Production?,https://news.ycombinator.com/item?id=42431361
Tips for Python Developers - A tour of C# | Microsoft Learn,https://learn.microsoft.com/en-us/dotnet/csharp/tour-of-csharp/tips-for-python-developers
A Sneak Peek at CSnakes: Embed Python code in .NET Apps | Microsoft Learn,https://learn.microsoft.com/en-us/shows/on-dotnet/a-sneak-peek-at-csnakes-embed-python-code-in-dotnet-apps
NirDiamant,https://github.com/NirDiamant
"NirDiamant/Prompt_Engineering: This repository offers a comprehensive collection of tutorials and implementations for Prompt Engineering techniques, ranging from fundamental concepts to advanced strategies. It serves as an essential resource for mastering the art of effectively communicating with and leveraging large language models in AI applications.",https://github.com/NirDiamant/Prompt_Engineering
NirDiamant/Controllable-RAG-Agent: This repository provides an advanced Retrieval-Augmented Generation (RAG) solution for complex question answering. It uses sophisticated graph based algorithm to handle the tasks.,https://github.com/NirDiamant/Controllable-RAG-Agent
12 Days of OpenAI - Release Updates | OpenAI Help Center,https://help.openai.com/en/articles/10271060-12-days-of-openai-release-updates#h_d989b42229
[2412.09094v1] Filter-then-Generate: Large Language Models with Structure-Text Adapter for Knowledge Graph Completion,https://arxiv.org/abs/2412.09094
Byte Latent Transformer: Patches Scale Better Than Tokens | Research - AI at Meta,https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/
[2412.10360] Apollo: An Exploration of Video Understanding in Large Multimodal Models,https://arxiv.org/abs/2412.10360
Beyond Fairness in Computer Vision: A Holistic Approach to Mitigating Harms and Fostering Community-Rooted Computer Vision Research,https://cdn.sanity.io/files/wc2kmxvk/revamp/79776912203edccc44f84d26abed846b9b23cb06.pdf
[2412.11919] RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation,https://arxiv.org/abs/2412.11919
[2412.11231] Smaller Language Models Are Better Instruction Evolvers,https://arxiv.org/abs/2412.11231
[2412.10047] Large Action Models: From Inception to Implementation,https://arxiv.org/abs/2412.10047
Phidata - How engineers build AI agents,https://www.phidata.com/
Model licenses on HuggingFace,https://aiworld.eu/embed/api/model/license/treemap
Building effective agents  Anthropic,https://www.anthropic.com/research/building-effective-agents
Blog/articles/multi_agents.md at main ¬∑ YukoOshima/Blog,https://github.com/YukoOshima/Blog/blob/main/articles/multi_agents.md
Artificial-Analysis-AI-Review-2024-Highlights.pdf,https://artificialanalysis.ai/downloads/ai-review/2024/Artificial-Analysis-AI-Review-2024-Highlights.pdf
"Nomic Blog: Data Maps, Part 4: Why Are Web Browsers The Best Data Browsers?",https://www.nomic.ai/blog/posts/why-are-web-browsers-the-best-data-browsers
"Finally, a Replacement for BERT: Introducing ModernBERT",https://huggingface.co/blog/modernbert
[2412.13435] Lightweight Safety Classification Using Pruned Language Models,https://arxiv.org/abs/2412.13435
FACTS Grounding: A new benchmark for evaluating the factuality of large language models - Google DeepMind,https://deepmind.google/discover/blog/facts-grounding-a-new-benchmark-for-evaluating-the-factuality-of-large-language-models/
Bryan Perozzi on LinkedIn: Graph Reasoning in Large Language Models | 18 comments,https://www.linkedin.com/posts/bryanperozzi_graph-reasoning-in-large-language-models-activity-7272638482664013824-boMF
A Gentle Introduction to Graph Neural Networks,https://distill.pub/2021/gnn-intro/
This is where the data to build AI comes from | MIT Technology Review,https://www.technologyreview.com/2024/12/18/1108796/this-is-where-the-data-to-build-ai-comes-from/
Multimodal_Data_Provenance.pdf,https://www.dataprovenance.org/Multimodal_Data_Provenance.pdf
Alignment faking in large language models  Anthropic,https://www.anthropic.com/research/alignment-faking
[2412.12276] Emergence of Abstractions: Concept Encoding and Decoding Mechanism for In-Context Learning in Transformers,https://arxiv.org/abs/2412.12276
[2412.12004] The Open Source Advantage in Large Language Models (LLMs),https://arxiv.org/abs/2412.12004
[2412.13091] LMUnit: Fine-grained Evaluation with Natural Language Unit Tests,https://arxiv.org/abs/2412.13091
[2412.14161] TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks,https://arxiv.org/abs/2412.14161
[2412.13194] Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation Model Internet Agents,https://arxiv.org/abs/2412.13194
[2412.15115] Qwen2.5 Technical Report,https://arxiv.org/abs/2412.15115
[2412.14475] MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval,https://arxiv.org/abs/2412.14475
[2412.15204] LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks,https://arxiv.org/abs/2412.15204
LongBench v2,https://longbench2.github.io/
[2412.14689] How to Synthesize Text Data without Model Collapse?,https://arxiv.org/abs/2412.14689
OpenAI teases `o3`,https://openai.com/12-days/
OpenAI o3 Breakthrough High Score on ARC-AGI-Pub,https://arcprize.org/blog/oai-o3-pub-breakthrough
o1: A Technical Primer ‚Äî LessWrong,https://www.lesswrong.com/posts/byNYzsfFmb2TpYFPW/o1-a-technical-primer
AI Copyright Case Tracker | Wired,https://www.wired.com/story/ai-copyright-case-tracker/
NeurIPS 2024: main themes and takeaways,https://www.amplifypartners.com/blog-posts/neurips-2024-main-themes-and-takeaways
Moving to GraphRAG 1.0 - Streamlining ergonomics for developers and users - Microsoft Research,https://www.microsoft.com/en-us/research/blog/moving-to-graphrag-1-0-streamlining-ergonomics-for-developers-and-users/
Building Confidence: A Case Study in How to Create Confidence Scores for GenAI Applications - Spotify Engineering : Spotify Engineering,https://engineering.atspotify.com/2024/12/building-confidence-a-case-study-in-how-to-create-confidence-scores-for-genai-applications/
You could have designed state of the art positional encoding,https://huggingface.co/blog/designing-positional-encoding
MI300X vs H100 vs H200 Benchmark Part 1: Training ‚Äì CUDA Moat Still Alive ‚Äì SemiAnalysis,https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/
GitHub - bytedance/monolith: ByteDance's Recommendation System,https://github.com/bytedance/monolith
lightonai/pylate: Late Interaction Models Training & Retrieval,https://github.com/lightonai/pylate
AI Watermarking 101: Tools and Techniques,https://huggingface.co/blog/watermarking
Mapping the latent space of Llama 3.3 70B - Goodfire Papers,https://www.goodfire.ai/papers/mapping-latent-spaces-llama/
Introduction | Cosmograph,https://cosmograph.app/docs/cosmograph/Introduction
Ryan Siegler on LinkedIn: My RAG app returning "5" as the answer to "What is the best thing to do in‚Ä¶ | 30 comments,https://www.linkedin.com/posts/ryan-siegler-816207102_my-rag-app-returning-5-as-the-answer-to-activity-7274796484275879937-4Bmb
Hamel H. on LinkedIn: Open Office Hours ‚Äì Hamel's Blog,https://www.linkedin.com/posts/hamelhusain_open-office-hours-hamels-blog-activity-7276991427727777793-VjSn
Open Office Hours ‚Äì Hamel's Blog,https://hamel.dev/notes/llm/officehours/
[2405.18369] PromptWizard: Task-Aware Prompt Optimization Framework,https://arxiv.org/abs/2405.18369
[2412.17558] A Survey of Query Optimization in Large Language Models,https://arxiv.org/abs/2412.17558
2412.15529) XRAG: eXamining the Core -- Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation,https://arxiv.org/abs/2412.15529
Jury,https://github.com/obss/jury
UpTrain,https://github.com/uptrain-ai/uptrain
[2412.16926] Revisiting In-Context Learning with Long Context Language Models,https://arxiv.org/abs/2412.16926
[2412.16429] LearnLM: Improving Gemini for Learning,https://arxiv.org/abs/2412.16429
[2412.15605] Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks,https://arxiv.org/abs/2412.15605
[2412.17747] Deliberation in Latent Space via Differentiable Cache Augmentation,https://arxiv.org/abs/2412.17747
[2412.15176] Rethinking Uncertainty Estimation in Natural Language Generation,https://arxiv.org/abs/2412.15176
[2412.15035] LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps,https://arxiv.org/abs/2412.15035
[2412.19437] DeepSeek-V3 Technical Report,https://arxiv.org/abs/2412.19437
"Nils Reimers on X: ""@andersonbcdefg Training on the MTEB training splits and evaluating on MTEB for the leaderboard submission was never intended. Embedding models need to be evaluated out of domain, anything else doesn't make sense (see BEIR paper). It was a big mistake to publish these training splits for MTEB."" / X",https://x.com/Nils_Reimers/status/1870812625505849849
The MTEB benchmark is dead | Hacker News,https://news.ycombinator.com/item?id=42504379
Fine-tune classifier with ModernBERT in 2025,https://www.philschmid.de/fine-tune-modern-bert-in-2025
Agents Category | Interconnects | Nathan Lambert,https://www.interconnects.ai/t/agents
Why OpenAI's Structure Must Evolve To Advance Our Mission | OpenAI,https://openai.com/index/why-our-structure-must-evolve-to-advance-our-mission/
Why Perfect Clustering Algorithms Don't Exist,https://blog.codingconfessions.com/p/the-cap-theorem-of-clustering
Building AI Products‚ÄîPart I: Back-end Architecture,https://philcalcado.com/2024/12/14/building-ai-products-part-i.html
AI Product Management,https://www.deeplearning.ai/the-batch/issue-279/
OpenSPG/KAG: KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs. It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.,https://github.com/OpenSPG/KAG
Can LLMs Accurately Recall the Bible,https://benkaiser.dev/can-llms-accurately-recall-the-bible/
[2412.18547] Token-Budget-Aware LLM Reasoning,https://arxiv.org/abs/2412.18547
[2412.17483] A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression,https://arxiv.org/abs/2412.17483
[2412.18319] Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search,https://arxiv.org/abs/2412.18319
Practices for Governing Agentic AI Systems | OpenAI,https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf
"Top AI Stories of 2024! Agents Rise, Prices Fall, Models Shrink, and more...",https://www.deeplearning.ai/the-batch/issue-281/
Latent Space: The AI Engineer Podcast,https://www.latent.space/podcast
Looking back on AI in 2024,https://frontierai.substack.com/p/looking-back-on-ai-in-2024
LangChain State of AI 2024 Report,https://blog.langchain.dev/langchain-state-of-ai-2024/
The Year in LlamaIndex: 2024 ‚Äî LlamaIndex - Build Knowledge Assistants over your Enterprise Data,https://www.llamaindex.ai/blog/the-year-in-llamaindex-2024
Nvidia in 2024: year in review | TechRadar,https://www.techradar.com/computing/gpu/nvidia-in-2024-year-in-review
What We've Learned From A Year of Building with LLMs ‚Äì Applied LLMs,https://applied-llms.org/
A Global Capability Center leader's guide for driving Generative AI adoption,https://www2.deloitte.com/content/dam/Deloitte/in/Documents/strategy/in-strategy-a-gcc-leaders-guide-for-driving-gen-ai-adoption-single-page-web-version-v5-noexp.pdf
"üåÅ#80: What's in 2025? From Elad Gil, FrancÃßois Chollet, Maxime Labonne, swyx and others",https://www.turingpost.com/p/fod80
"Clem Delangue ü§ó: ""Six predictions for AI in 2025 (and a review of how my 2024 predictions turned out):"" ‚Äî Bluesky",https://bsky.app/profile/clem.hf.co/post/3lcdceiv7vk2h
12 AI predictions for 2025 | CIO,https://www.cio.com/article/3630070/12-ai-predictions-for-2025.html
"From AI agents to enterprise budgets, 20 VCs share their predictions on enterprise tech in 2025 | TechCrunch",https://techcrunch.com/2024/12/30/from-ai-agents-to-enterprise-budgets-20-vcs-share-their-predictions-on-enterprise-tech-in-2025/
20 Predictions For AI in 2025 - by Alberto Romero,https://www.thealgorithmicbridge.com/p/20-predictions-for-ai-in-2025
"[2412.13663] Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference",https://arxiv.org/abs/2412.13663
[2310.04560] Talk like a Graph: Encoding Graphs for Large Language Models,https://arxiv.org/abs/2310.04560
[2406.09170] Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning,https://arxiv.org/abs/2406.09170
[2405.18512] Understanding Transformer Reasoning Capabilities via Graph Algorithms,https://arxiv.org/abs/2405.18512
Scaling test-time compute - a Hugging Face Space by HuggingFaceH4,https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute
